<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Kevin Chan's blog - 《Deep Learning》读书笔记</title>
    <link rel="icon" href="https://applenob.github.io/static/favicon.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="https://applenob.github.io/static/favicon.ico" type="image/x-icon" />
    <meta name="description" content="">
    <meta name="author" content="Kevin Chan">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="https://applenob.github.io/theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="https://applenob.github.io/theme/bootstrap.min.css" rel="stylesheet">
    <link href="https://applenob.github.io/theme/bootstrap.min.responsive.css" rel="stylesheet">
    <link href="https://applenob.github.io/theme/local.css" rel="stylesheet">
    <link href="https://applenob.github.io/theme/pygments.css" rel="stylesheet">

    <!-- So Firefox can bookmark->"abo this site" -->
        <link href="https://applenob.github.io/feeds/all.atom.xml" rel="alternate" title="Kevin Chan's blog" type="application/atom+xml">

</head>

<body>

<div class="navbar">
    <div class="navbar-inner">
    <div class="container">

         <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
         </a>

        <a class="brand" href="https://applenob.github.io">Kevin Chan's blog</a>

        <div class="nav-collapse">
        <ul class="nav">
            
        </ul>
        </div>
        
    </div>
    </div>
</div>

<div class="container">
    <div class="content">
    <div class="row">

        <div class="span9">
    <div class='article'>
        <div class="content-title">
            <h1>《Deep Learning》读书笔记</h1>


by <a class="url fn" href="https://applenob.github.io/author/chan.html">Chan</a>
 


        </div>
	
        <div><style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style>
<style type="text/css">
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="《Deep-Learning》">《Deep Learning》<a class="anchor-link" href="#《Deep-Learning》">¶</a></h1><p></p>
<h2 id="目录">目录<a class="anchor-link" href="#目录">¶</a></h2><ul>
<li><a href="https://danteliujie.github.io/deep_learning#0.书本介绍">0.书本介绍</a></li>
<li><a href="https://danteliujie.github.io/deep_learning#1.-Introduction">1. Introduction</a></li>
<li><a href="https://danteliujie.github.io/deep_learning#2.-Linear-Algebra">2. Linear Algebra</a></li>
<li><a href="https://danteliujie.github.io/deep_learning#3.-Probability-and-Information-Theory">3. Probability and Information Theory</a></li>
<li><a href="https://danteliujie.github.io/deep_learning#4.-Numerical-Computation">4. Numerical Computation</a></li>
<li><a href="https://danteliujie.github.io/deep_learning#5.-Machine-Learning-Basics">5. Machine Learning Basics</a></li>
<li><a href="https://danteliujie.github.io/deep_learning#6.-Deep-Feedforward-Networks">6. Deep Feedforward Networks</a></li>
<li><a href="https://danteliujie.github.io/deep_learning#7.-Regularization-for-Deep-Learning">7. Regularization for Deep Learning</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">8. Optimization for Training Deep Models</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">9. Convolutional Networks</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">10. Sequence Modeling: Recurrent and Recursive Nets</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">11. Practical Methodology</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">12. Application</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">13. Linear Factor Models</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">14. Autoencoders</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">15. Representation Learning</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">16. Structured Probabilistic Models for Deep Learning</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">17. Monte Carlo Methods</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">18. Confronting the Partition Function</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">19. Approximate Inference</a></li>
<li><a href="https://danteliujie.github.io/deep_learning">20. Deep Generative Models</a></li>
</ul>
<hr />
<hr />
<h2 id="0.书本介绍">0.书本介绍<a class="anchor-link" href="#0.书本介绍">¶</a></h2><p><a href="https://book.douban.com/subject/26883982/">Deep Learning</a></p>
<p><a href="http://www.deeplearningbook.org/">原版阅读网站</a></p>
<p>作者:  <strong>Ian Goodfellow / Yoshua Bengio / Aaron Courville </strong></p>
<p>内容简介：</p>
<pre><code>"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject." -- Elon Musk, co-chair of OpenAI; co-founder and CEO of Tesla and SpaceX
Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning.
The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models.
Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.</code></pre>
<p>这本书由GAN的发明人Ian Goodfellow主写，系统地介绍了深度学习的基础知识和后续发展，是一本值得反复读的好书。</p>
<p>这里我做的笔记是基于本书的框架，但内容不限于来自本书，最终目的是加深对知识本身的理解。</p>
<p>笔记也会不断更新，只要在今后在工作中碰到其中的问题需要进一步研究学习，就会继续丰富此笔记的内容。</p>
<hr />
<hr />
<h2 id="1.-Introduction">1. Introduction<a class="anchor-link" href="#1.-Introduction">¶</a></h2><p>什么是<strong>machine learning</strong>?</p>
<p>在原始的AI系统中，定义不同的case使用不同的解决方法，这称为“hard code”。进一步的AI系统需要一种去获取知识的能力，也就是从原始数据中发现模式（“Pattern”），这种能力就是<strong>machine learning</strong>。</p>
<p>但是，一般的machine learning算法严重依赖于数据的<strong>表示(representation)</strong>，表示中包含的每份信息又称为<strong>feature</strong>。</p>
<p>这又引发了一个新的问题，对于很多task，我们不知道应该提取什么样的特征（只能经验主义）。</p>
<p>于是又有了<strong>representation learning</strong>，即使用machine learning不光光是学习reprsentation到output的映射（mapping），还要学习data到representation的映射。</p>
<p>典型的表示学习算法是<strong>autoencoder</strong>。encoder函数是将输入数据映射成表示;decoder函数将表示映射回原始数据的格式。</p>
<p>representation learning的难点：表示是多种多样的，一种表示学习算法很难覆盖多种层次和不同类型的表示。</p>
<p><strong>Deep Learning</strong>：使用多层次的结构，用简单的表示来获取高层的表示。这样，解决了上面的问题（一种方法）。</p>
<p><img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/dl.png" /></p>
<hr />
<h1 id="2.-Linear-Algebra">2. Linear Algebra<a class="anchor-link" href="#2.-Linear-Algebra">¶</a></h1><p><strong>基础概念</strong>：</p>
<ul>
<li>Scalars: 一个数；</li>
<li>Vctors: 一列数；</li>
<li>Matrices: 二位数组的数，每个元素由两个下标确定；</li>
<li>Tensors: 多维数组的数。</li>
</ul>
<p><strong>转置（transpose）</strong>：$(A^T)_{i,j}=A_{j,i}$</p>
<p><strong>矩阵乘法</strong>: $C=AB$， $C_{i,j}=\sum_kA_{i,k}B_{k,j}$</p>
<p><strong>元素乘法(element product; Hardamard product)</strong>：$A \bigodot B$</p>
<p><strong>点乘(dot product)</strong>: 向量<strong>x</strong>，<strong>y</strong>的点乘：  $x^Ty$</p>
<p><strong>单位矩阵(identic matrix)</strong>: $I_n$， 斜对角的元素值是1,其他地方都是0</p>
<p><strong>逆矩阵（inverse matrix）</strong>:</p>
<ul>
<li>$A^{-1}$, $A^{-1}A=I_n$</li>
<li>方程Ax=b，如果A可逆，则$x=A^{-1}b$</li>
</ul>
<p><strong>线性组合（linear combination）</strong>：</p>
<ul>
<li>将矩阵$A$看作是不同的列向量的组合$[d1,d2,...,dn]$，每个列向量代表一个方向，$x$可以代表在每个方向上移动的距离，那么$Ax=b$可以理解成原点如何在$A$指定的各个方向上移动，最后到达$b$点。</li>
<li>$Ax$即为线性组合，组合的对象是各个列向量，方式是$x$的元素。</li>
</ul>
<p><strong>生成空间（span）</strong>：对所有的$x$，生成的点$Ax$的集合，即为$A$的生成空间。</p>
<p><strong>范数（Norm）</strong>：</p>
<ul>
<li>用来衡量vector的尺寸。</li>
<li>$L^p$ norm:$||x||_p = \left ( \sum_i{|x_i|^p} \right )^{\frac{1}{p}}$</li>
</ul>
<p><strong>Frobenius-norm</strong>:</p>
<ul>
<li>用来衡量matrix的尺寸。</li>
<li>类似于$L_2$ norm:$||A||_F=\sqrt{\sum_{i,j}{A_{i,j}^2}}$</li>
</ul>
<p><strong>对角阵（diagnal matrix）</strong>：除了对角线上的元素不为0,其他元素都为0。可以表示为$diag(v)$。</p>
<p><strong>对称阵（symmetric matrix）</strong>：$A=A^T$</p>
<p><strong>单位向量（unit vector）</strong>：$||x||_2 = 1$</p>
<p><strong>正交（orthogonal）</strong>：如果$x^Ty=0$，则向量$x$和向量$y$彼此正交。</p>
<p><strong>正交归一化矩阵（orthonormal matrix）</strong>：</p>
<ul>
<li>每行都相互正交并且都是单位向量。</li>
<li>$A^TA=AA^T=I$，有$A^{-1}=A^T$。</li>
</ul>
<p><strong>特征分解</strong>：</p>
<ul>
<li>特征向量$v$和特征值$λ$，满足：$Av = λv$，方阵A可以这样分解：$A=Vdiag(λ)V^{-1}$，其中,$V=[v^{(1)},...,v^{(n)}]$,$λ=[λ_1, ..., λ_n]^T$。</li>
<li>特别的，如果A是一个实对称阵，那么$A=Q∧Q^T$</li>
</ul>
<p><strong>正定（positive definite）</strong>：一个矩阵的所有特征值都是正的，则称这个矩阵正定。</p>
<p><strong>奇异值分解（singular value decomposition）</strong>：</p>
<ul>
<li>$A = UDV^T$</li>
<li>其中$A$是$m×n$矩阵。</li>
<li>$U$是$m×m$正交矩阵，$U$的列向量称为左奇异向量，是$AA^T$的特征向量。</li>
<li>$D$是$m×n$对角矩阵，对角线上的元素称为<strong>奇异值</strong>。</li>
<li>$V$是正交$n×n$矩阵，$V$的列向量称为右奇异向量，是$A^TA$的特征向量。</li>
</ul>
<p><strong>伪逆（Moore-Penrose Pseudoinverse）</strong>：</p>
<ul>
<li>$A^+ = VD^+U^T$</li>
<li>其中，$D^+$是由D的每个对角线元素取倒数（reciprocal）获得。</li>
</ul>
<p><strong>迹（Trace）</strong>：$Tr(A) = \sum_i A_{i,i} $，即<strong>对角线元素之和</strong>。</p>
<p><strong>行列式（Determinant）</strong>：</p>
<ul>
<li>$det(A)$，是一个将一个matrix映射到一个实数的function。</li>
<li>行列式的值等于矩阵的所有特征值的乘积。</li>
</ul>
<p><strong>奇异矩阵（singular matrix）</strong>：</p>
<ul>
<li>前提是方阵</li>
<li>如果A(n×n)为奇异矩阵（singular matrix）<=> A的秩$Rank(A)<n$&lt; li>
<li>如果A(n×n)为非奇异矩阵（nonsingular matrix）<=> A满秩，$Rank(A)=n$。</li>
</ul>
<hr />
<h1 id="3.-Probability-and-Information-Theory">3. Probability and Information Theory<a class="anchor-link" href="#3.-Probability-and-Information-Theory">¶</a></h1><h2 id="概率论部分">概率论部分<a class="anchor-link" href="#概率论部分">¶</a></h2><p><strong>频率学派概率（frequentist probability）</strong>：认为概率和事件发生的频率相关。</p>
<p><strong>贝叶斯学派概率（Bayesian probability）</strong>：认为概率是的对某件事发生的确定程度，可以理解成是确信的程度（degree of belief）。</p>
<p><strong>随机变量（random variable）</strong>：一个可能随机获取不同值的变量。</p>
<p><strong>概率质量函数（probability mass function，PMF）</strong>：用来描述离散随机变量的概率分布。表示为P(x)，是状态到概率的映射。</p>
<p><strong>概率密度函数（probability density function，PDF）</strong>：用来描述连续随机变量的概率分布，p(x)。</p>
<p><strong>条件概率（conditional probability）</strong>：$P(y=y|x=x) = \frac{P(y=y, x=x)}{P(x=x)}$</p>
<p><strong>条件概率的链式法则（chain rule of conditional probability）</strong>：$P(x^{(1)}, ..., x^{(n)}) = P(x^{(1)})\prod^n_{i=2}P(x^{(i)}|x^{(1)}, ..., x^{(i-1)})$</p>
<p><strong>独立（independence）</strong>：$\forall x \in x, y ∈ y, p(x=x, y=y) = p(x=x)p(y=y)$</p>
<p><strong>条件独立（conditional independence）</strong>：$\forall x ∈ x, y ∈ y, z ∈ z,p(x=x, y=y | z=z) = p(x=x | z=z)p(y=y | z=z)$</p>
<p><strong>期望（expectation）</strong>：</p>
<ul>
<li>期望针对某个函数$f(x)$，关于概率分布$P(x)$的平均值。</li>
<li>对离散随机变量：$E_{x \sim P}[f(x)] = \sum_xP(x)f(x)$</li>
<li>对连续随机变量：$E_{x \sim p}[f(x)] = \int P(x)f(x)dx$</li>
<li>期望是线性的：$E_x[αf(x)+βg(x)] = αE_x[f(x)]+βE_x[f(x)]$</li>
</ul>
<p><strong>方差（variance）</strong>：</p>
<ul>
<li>用来衡量从随机变量$x$的分布函数$f(x)$中采样出来的一系列值和期望的偏差。</li>
<li>$Var(x) = E[(f(x)-E[f(x)])^2]$，方差开平方即为标准差（standard deviation）。</li>
</ul>
<p><strong>协方差（covariance）</strong>：</p>
<ul>
<li>用于衡量两组值之间的线性相关程度。</li>
<li>$Cov(f(x), g(y)) = E[(f(x)-E[f(x)])(g(y)-E[g(y)])]$。</li>
<li>独立比协方差为0更强，因为独立还排除了非线性的相关。</li>
</ul>
<p><strong>贝努力分布（Bernoulli Distribution）</strong>：随机变量只有两种可能的分布，只有一个参数：$Φ$，即$x=1$的概率。</p>
<p><strong>多项式分布（Multinoulli Distribution）</strong>随机变量有$k$种可能的分布，参数是一个长度为$k-1$的向量$p$。</p>
<p><strong>高斯分布（Gaussian Distribution）</strong></p>
<ul>
<li>即正态分布（normal distribution）</li>
<li>$\textit{N}(x;μ,σ^2) = \sqrt{\frac{1}{2πσ^2}}exp(-\frac{1}{2σ^2}(x-μ)^2)$。</li>
<li>中心极限定理（central limit theorem）认为，大量的独立随机变量的和近似于一个高斯分布，这一点可以大量使用在应用中，我们可以认为噪声是属于正态分布的。</li>
<li><img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/gauss.PNG" /></li>
</ul>
<p><strong>多元正态分布（multivariate normal distribution）</strong>：</p>
<ul>
<li>给定协方差矩阵$\mathbf{Σ}$（正定对称），$\textit{N}(x;μ,Σ) = \sqrt{\frac{1}{(2π)^ndet(Σ)}}exp(-\frac{1}{2}(x-μ)^TΣ^{-1}(x-μ))$</li>
<li><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/MultivariateNormal.png/450px-MultivariateNormal.png" /></li>
</ul>
<p><strong>指数分布（exponential distribution）</strong>：</p>
<ul>
<li>在深度学习的有研究中，经常会用到在x=0点获得最高的概率的分布，$p(x; λ) = λ\mathbf{1}_{x≥0}exp(-λx)$，或者：$f(x) = \left\{\begin{matrix}λexp(-λx) \;\;\;\; x≥0 \\0 \;\;\;\; else \end{matrix}\right.$，其中λ > 0是分布的一个参数，常被称为率参数（rate parameter）。</li>
<li><img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/exp_dis.png" /></li>
</ul>
<p><strong>拉普拉斯分布（Laplace Distribution）</strong>：</p>
<ul>
<li>另一个可以在一个点获得比较高的概率的分布。</li>
<li>$Laplace(x ;μ,γ) = \frac{1}{2γ}exp(-\frac{|x-μ|}{γ})$</li>
<li><img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/laplace.jpg" /></li>
</ul>
<p><strong>迪拉克分布（Dirac Distribution）</strong>：</p>
<ul>
<li>$p(x) = δ(x-μ)$，这是一个泛函数。迪拉克分布经常被用于组成经验分布（empirical distribution）</li>
<li>$p(x) = \frac{1}{m}\sum_{i=1}^m{δ(x-x^{(i)})}$</li>
<li><img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/dirac.png" /></li>
</ul>
<p><strong>逻辑斯蒂函数（logistic function）</strong>：</p>
<ul>
<li>$σ(x) = \frac{1}{1+exp(-x)}$，常用来生成贝努力分布的Φ参数。</li>
<li><img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/logistic.png" /></li>
</ul>
<p><strong>softplus function</strong>:</p>
<ul>
<li>$ζ(x) = log(1+exp(x))$，是“取正”函数的“soft”版</li>
<li>$x^+ = max(0, x)$</li>
<li><img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/softplus.png" /></li>
</ul>
<p><strong>贝叶斯公式（Bayes' Rule）</strong>：$P(x|y) = \frac{P(x)P(y|x)}{P(y)}$</p>
<h2 id="信息论部分">信息论部分<a class="anchor-link" href="#信息论部分">¶</a></h2><p><strong>信息论背后的直觉： 学习一件不太可能的事件比学习一件比较可能的事件更有信息量。</strong></p>
<p><strong>信息（information）需要满足的三个条件</strong>：</p>
<ul>
<li>1.比较可能发生的事件的信息量要少；</li>
<li>2.比较不可能发生的事件的信息量要大；</li>
<li>3.独立发生的事件之间的信息量应该是可以叠加的。</li>
</ul>
<p><strong>自信息（Self-Information）：</strong>对事件$x=x$，$I(x) = -logP(x)$，满足上面三个条件，单位是nats（底为e）</p>
<p><strong>香农熵（Shannon Entropy）</strong>：</p>
<ul>
<li>自信息只包含一个事件的信息。</li>
<li><strong>对于整个概率分布$p(x)$</strong>，不确定性可以这样衡量：$E_{x\sim P}[I(x)] = -E_{x\sim P}[logP(x)]$。</li>
<li>也可以表示成$H(P)$。</li>
</ul>
<p><strong>多个随机变量</strong>：</p>
<p><strong>1.联合熵（Joint Entropy）</strong>：$H(X,Y)=-∑_{x,y}p(x,y)log(p(x,y))$，同时考虑多个事件的条件下（即考虑联合分布概率）的熵。</p>
<p><strong>2.条件熵（Conditional Entropy）</strong>：$H(X|Y)=-∑_yp(y)∑_xp(x|y)log(p(x|y))$，某件事情已经发生的情况下，另外一件事情的熵。</p>
<p><strong>3.互信息（Mutual Information）</strong>：$I(X,Y)=H(X)+H(Y)−H(X,Y)$，两个事件的信息<strong>相交</strong>的部分。</p>
<p><strong>4.信息变差（Variation of information）</strong>：$V(X,Y)=H(X,Y)−I(X,Y)$，两个事件的信息<strong>不相交</strong>的部分。</p>
<p><strong>KL散度（Kullback-Leibler Divergence）</strong>：</p>
<ul>
<li>衡量两个分布$P(x)$和$Q(x)$之间的差距。</li>
<li>$D_{KL}(P||Q)=E_{x\sim P}[log \frac{P(x)}{Q(x)}]=E_{x\sim P}[logP(x)-logQ(x)]$，注意$D_{KL}(P||Q)≠D_{KL}(Q||P)$</li>
</ul>
<p><strong>交叉熵（Cross Entropy）</strong>：</p>
<ul>
<li>$H(P,Q)=H(P)+D_{KL}(P||Q)=-E_{x\sim P}[logQ(x)]$</li>
<li>假设$P$是真实分布，$Q$是模型分布，那么最小化交叉熵$H(P,Q)$可以让模型分布逼近真实分布。</li>
</ul>
<p><strong>注</strong>：信息论部分很好的参考资料，<a href="http://colah.github.io/posts/2015-09-Visual-Information/">colah的博客</a>。</p>
<h2 id="图模型（Structured-Probabilistic-Models）">图模型（Structured Probabilistic Models）<a class="anchor-link" href="#图模型（Structured-Probabilistic-Models）">¶</a></h2><p><strong>有向图模型（Directed Model）</strong>：</p>
<ul>
<li>$p(x) = \prod_i p(x_i | Pa_g(x_i))$， 其中$Pa_g(x_i)$是$x_i$的父节点。</li>
<li><img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/dg.png" /></li>
<li>$P(a,b,c,d,e)=p(a)p(b|a)p(c|a,b)p(d|b)p(e|c)$</li>
</ul>
<p><strong>无向图模型（Undirected Model）</strong>：</p>
<ul>
<li>所有节点都彼此联通的集合称作“团”（Clique）。</li>
<li>$p(x)=\frac{1}{Z}\prod_i{Φ^{(i)}(C^{(i)})}$，其中，Φ称作facor，每个factor和一个团（clique）相对应。</li>
<li><img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/udg.png" /></li>
<li>$p(a,b,c,d,e) = \frac{1}{Z}Φ^{(1)}(a,b,c)Φ^{(2)}(b,d)Φ^{(3)}(c,e)$</li>
</ul>
<h1 id="4.-Numerical-Computation">4. Numerical Computation<a class="anchor-link" href="#4.-Numerical-Computation">¶</a></h1><p><strong>数值优化（Numerical Computation）</strong>：通常指代那些在解决数学问题时，不使用从符号表达式中直接推导出解析解，而是使用迭代更新的方式获取答案的算法。</p>
<p><strong>上溢和下溢（overflow/underflow）</strong>：数据太小或者太大，在计算机内存中无法表示。</p>
<p><strong>优化问题（optimization problem）</strong>：优化目标：最小化函数：损失函数（loss function）/ 错误函数（error function）通常上标*表示最优解。$x^*=argmin f(x)$</p>
<p><strong>临界点（critical point）</strong>：$f'(x)=0$的点称为临界点，一般临界点取得极大值或者极小值，否则为鞍点（saddle point）。</p>
<p><strong>梯度下降（gradient descent）</strong>：$x' = x-ε\triangledown _xf(x)$，其中，ε是学习率。</p>
<p><strong>Jacobian 矩阵（Jacobian matrix）</strong>：</p>
<ul>
<li>如果我们有一个函数f:$\mathbb{R}^m \rightarrow \mathbb{R}^n$</li>
<li>那么Jacobian矩阵即为：$J_{i,j} = \frac {\partial}{\partial x_j}f(x)_i$。</li>
</ul>
<p><strong>Hessian 矩阵（Hessian matrix）</strong>：</p>
<ul>
<li>$H(f)(x)_{i,j} = \frac {\partial ^2}{\partial x_i \partial x_j}f(x)$。</li>
<li>可以知道，Hessian矩阵是对称阵。</li>
</ul>
<p><strong>牛顿法（Newton's method）</strong>：</p>
<ul>
<li>将函数用二阶的泰勒公式近似：$f(x)≈f(x^{(0)})+(x-x^{(0)})^T\triangledown_xf(x^{(0)})+\frac{1}{2}(x-x^{(0)})^TH(f)(x^{(0)})(x-x^{(0)})$，求解临界点$x^* = x^{(0)}-H(f)(x^{(0)})^{-1}\triangledown_xf(x^{(0)})$。</li>
<li>梯度下降称为“一阶优化算法”；牛顿法称为“二阶优化算法”。</li>
</ul>
<h3 id="拉格朗日对偶性">拉格朗日对偶性<a class="anchor-link" href="#拉格朗日对偶性">¶</a></h3><p><strong>原始问题</strong>：</p>
<ul>
<li>$f(x),c_i(x),h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题：</li>
<li>$\underset{x\in R^n}{min}f(x)\\s.t.\;\;c_i(x)\leq 0\;\;i=1,...,k\\h_j(x)=0\;\;j=1,...,l$</li>
<li>即有$k$个不等式约束：$c_i(x)$和$l$个等式约束：$h_j(x)$。</li>
</ul>
<p><strong>拉格朗日函数</strong>：</p>
<ul>
<li>$L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k\alpha_i c_i(x)+\sum_{j=1}^l\beta_jh_j(x)$</li>
<li>$\alpha_i$和$\beta_j$，称为<strong>拉格朗日乘子</strong>，$\alpha_i\geq 0$。</li>
</ul>
<p><strong>拉格朗日函数的极大极小问题</strong>：</p>
<ul>
<li>令$\theta_P(x) = \underset{\alpha,\beta}{max}\;L(x,\alpha,\beta)$</li>
<li>如果存在$x$违反了原始问题的约束条件，则$\theta_P(x)=\infty$，当$x$不违反原始问题的约束条件，则$\theta_P(x)=f(x)$</li>
<li>因此：$\underset{x}{min}\theta_P(x)=\underset{x}{min}\underset{\alpha,\beta}{max}L(x,\alpha,\beta)$等价于原问题。</li>
</ul>
<p><strong>对偶问题</strong>：</p>
<ul>
<li>$\underset{\alpha,\beta}{max}\underset{x}{min}\;L(x,\alpha,\beta)$</li>
<li>定理：如果函数$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是仿射函数，则$x^*,\alpha^*,\beta^*$是同时是原始问题和对偶问题的解的必要条件是满足KKT条件。</li>
</ul>
<p><strong>KKT条件</strong>：</p>
<ul>
<li>1.拉格朗日函数对$x$，$λ$，$α$求偏导都为0：<ul>
<li>$\triangledown_xL(x^*,\alpha^*,\beta^*)=0$</li>
<li>$\triangledown_{\alpha}L(x^*,\alpha^*,\beta^*)=0$</li>
<li>$\triangledown_{\beta}L(x^*,\alpha^*,\beta^*)=0$</li>
</ul>
</li>
<li>2.对于不等式约束，$\alpha_i^*c_i(x^*)=0\;\;\;i=1,...,k$（对偶互补条件）。</li>
</ul>
<h1 id="5.-Machine-Learning-Basics">5. Machine Learning Basics<a class="anchor-link" href="#5.-Machine-Learning-Basics">¶</a></h1><p><strong>机器学习定义</strong>：一个计算机程序，如果它能做到在任务T中的性能P随着经验E可以提高，那就可以称它是关于某类任务T和性能衡量P，从经验E中学习。</p>
<p><strong>机器学习任务（$T$）类别</strong>：分类（classification）/缺失输入数据的分类（classification with missing data）/回归（regression）/转录（transciption）/机器翻译（machine translation）/结构化输出（structured output）/异常检测（anomaly detection）/合成和采样（synthesis and smapling）/缺失值填补（imputation of missing data）/去噪（denoising）/密度估计（density estimation）</p>
<p><strong>机器学习的性能（$P$）</strong>：$P$因为$T$的不同而不同。对于像分类/缺失输入数据的分类/转录，使用准确率（accuracy）来衡量性能；而对于密度估计，通常输出模型在一些样本上概率对数的平均值。</p>
<p><strong>机器学习的经验（$E$）</strong>：根据经验的不同，分为监督学习和无监督学习。监督学习：学习$p(x)$；无监督学习：学习$p(y|x)$。通常来说，无监督学习通常指代从不需要人工标注数据中提取信息。</p>
<p><strong>泛化（generalization）</strong>：在先前未观测到的输入上表现良好的能力被称为泛化。</p>
<p><strong>欠拟合（underfitting）和过拟合（overfitting）</strong>：机器学习的性能取决于两点因素：1.使训练误差更小；2.使训练误差和测试误差的差距更小。分别对应欠拟合的改善和过拟合的改善。</p>
<p><strong>模型的容量（capacity）</strong>：模型的容量是指其拟合各种函数的能力。</p>
<p><strong>VC维（Vapnik-Chervonenkis dimension）</strong>：VC维用来度量二分类器的容量。假设存在$m$个不同的$x$点的训练集，分类器可以任意地标记该$m$个不同的$x$点，VC维即$m$的最大可能值。<a href="http://www.flickering.cn/machine_learning/2015/04/vc%E7%BB%B4%E7%9A%84%E6%9D%A5%E9%BE%99%E5%8E%BB%E8%84%89/">详细解释</a></p>
<p><strong>奥卡姆剃刀（Occam's razor）</strong>：在同样能够解释已知观测现象的假设中，我们应该挑选”最简单”的那一个。</p>
<p><strong>没有免费的午餐定理（no free lunch theorem）</strong>：所有分类算法在分类没有见过的点的时候，他们的错误率的期望是一样的。这个定理告诉我们，必须要针对特定的任务去设计机器学习算法。</p>
<p><strong>正则化（Regularization）</strong>：正则化是指我们针对减少泛化误差而不是训练误差，在一个机器学习算法上做的任何改动。</p>
<p><strong>超参数（Hyperparameters）</strong>：超参数的值不能通过学习算法本身学习出来。</p>
<p><strong>验证集（Validation Sets）</strong>：验证集用来调超参数。</p>
<h3 id="统计学的一些基本概念：估计（Estimators）/偏差（Bias）/方差（Variance）">统计学的一些基本概念：估计（Estimators）/偏差（Bias）/方差（Variance）<a class="anchor-link" href="#统计学的一些基本概念：估计（Estimators）/偏差（Bias）/方差（Variance）">¶</a></h3><p><strong>点估计（Point Estimation）</strong>：</p>
<p>试图为某些参数提供一个“最优”的预测。</p>
<p>将参数$θ$的点估计记为$\hat θ$，令$\{x^{(1)}, . . . , x^{(m)}\}$是$m$个独立同分布(i.i.d.)的数据点。点估计是这些数据的任意函数：$\hat θ=g(x^{(1)}, . . . , x^{(m)})$</p>
<p><strong>估计的偏差（Bias）</strong>：</p>
<p>估计的偏差被定义为:$bias(\hat θ_m) = E(\hat θ_m) - θ$，即，估计的期望和真实值的差。</p>
<p>如果$bias(\hat θ_m)=0$,那么估计量$θ_m$被称为是无偏估计。</p>
<p><strong>估计的方差（Variance）</strong>：</p>
<p>就是一个方差$Var(\hat θ)$。</p>
<p>$Var(X) = E[(X - \mu)^2]$，其中$μ=E[X]$。</p>
<p><strong>均方误差（mean squared error， MSE）</strong>：</p>
<p>$MSE = E[(\hat θ_m - θ)^2] = Bias(\hat θ_m)^2 + Var(\hat θ_m)$</p>
<h3 id="最大似然估计（Maximum-Likelihood-Estimation,-MLE）">最大似然估计（Maximum Likelihood Estimation, MLE）<a class="anchor-link" href="#最大似然估计（Maximum-Likelihood-Estimation,-MLE）">¶</a></h3><p>参数θ的最大似然估计：$θ_{ML} = \underset{θ}{argmax}\;p_{model}(\mathbb{X};θ) = \underset{θ}{argmax}\;\prod_{i=1}^mp_{model}(x^{(i)};θ)$，其中$\mathbb{X}=\{x^{(1)}, . . . , x^{(m)}\}$。</p>
<p>似然函数：$p_{model}(\mathbb{X};θ)$是一族由$θ$确定在相同空间上的概率分布。可以看到，<strong>这里$θ$并不是一个随机变量，而仅仅是一个参数。</strong></p>
<p>对数形式：$θ_{ML} = \underset{θ}{argmax}\sum_{i=1}^mlogp_{model}(x^{(i)};θ)$。</p>
<p>是一种点估计方法。</p>
<h3 id="贝叶斯统计（Bayesian-Statistics）">贝叶斯统计（Bayesian Statistics）<a class="anchor-link" href="#贝叶斯统计（Bayesian-Statistics）">¶</a></h3><p>最大似然估计是频率学派的观点，认为参数θ是固定的，但是未知；贝叶斯统计观点认为，数据集是直接观察得到的，因此数据集不是随机的，但是<strong>参数θ是一个随机变量</strong>。</p>
<p>在观察到数据前,我们将$θ$的已知知识表示成<strong>先验概率分布(prior probability distribution)</strong>$p(θ)$。</p>
<p>$p(θ|x^{(1)},x^{(2)},...,x^{(m)}) = \frac{p(x^{(1)},x^{(2)},...,x^{(m)}|θ)p(θ)}{p(x^{(1)},x^{(2)},...,x^{(m)})}$</p>
<h3 id="最大后验(Maximum-A-Posteriori,-MAP)估计">最大后验(Maximum A Posteriori, MAP)估计<a class="anchor-link" href="#最大后验(Maximum-A-Posteriori,-MAP)估计">¶</a></h3><p>$θ_{MAP}=\underset{θ}{argmax}\;p(θ∣x)=\underset{θ}{argmax}\;logp(x∣θ)+logp(θ)$</p>
<p>我们可以认出上式右边的$logp(x|θ)$对应着标准的对数似然项，$logp(θ)$对应着先验分布。</p>
<p>正如全贝叶斯推断,MAP 贝叶斯推断的优势是能够利用来自先验的信息,这些信息无法从训练数据中获得。该附加信息有助于减少最大后验点估计的方差，然而,这个优点的代价是增加了偏差。</p>
<p>依然是一种点估计方法。</p>
<p><strong>机器学习算法的常见组成部分</strong>：一个数据集（dataset）+一个损失函数（cost function）+一个优化过程（optimization procedure）+一个模型（model）</p>
<p><strong>维数灾难（the Curse of Dimensionality）</strong>：当数据的维数很高时，很多机器学习问题变得相当困难。 这种现象被称为维数灾难。</p>
<h3 id="流形（manifold）学习">流形（manifold）学习<a class="anchor-link" href="#流形（manifold）学习">¶</a></h3><p>流形指连接在一起的区域。 数学上，它是指一组点，且每个点都有其邻域。 给定一个任意的点，其流形局部看起来像是欧几里得空间。</p>
<p>日常生活中，我们将地球视为二维平面，但实际上它是三维空间中的球状流形。</p>
<p>流形学习算法通过一个假设来克服这个障碍，该假设认为$R_n$中大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿着流形的方向或仅发生在我们切换到另一流形时。</p>
<p>我们认为在人工智能的一些场景中，如涉及到处理图像、声音或者文本时，流形假设至少是近似对的。</p>
<p>关于流形学习有一个很好的<a href="http://www.cad.zju.edu.cn/reports/%C1%F7%D0%CE%D1%A7%CF%B0.pdf">中文ppt</a>，可以作为参考材料。</p>
<h2 id="6.-Deep-Feedforward-Networks">6. Deep Feedforward Networks<a class="anchor-link" href="#6.-Deep-Feedforward-Networks">¶</a></h2><p><strong>深度前馈网络（Deep Feedforward Networks）</strong>：也被称为前馈神经网络（feedforward neural networks），或者多层感知机（multi-layer perceptrons， MLPs）是典型的深度学习模型。前馈网络的目标是去近似一个函数$f^*$。模型之所以称为前馈，是因为信息只向前流动，没有反馈的连接。</p>
<p><strong>基于梯度的学习（Gradient Based Learning）</strong>：神经网络模型和线性模型最大的区别在于神经网络的非线性使得损失函数<strong>不再是凸函数</strong>。这意味着神经网络的训练通常使用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值；而不是像用于训练线性回归模型的线性方程求解器，或者用于训练逻辑回归或SVM的<strong>凸优化算法</strong>那样保证全局收敛。 凸优化从任何一种初始参数出发都会收敛（理论上如此——在实践中也很鲁棒但可能会遇到数值问题）。 用于非凸损失函数的随机梯度下降没有这种收敛性保证，并且对参数的初始值很敏感。 对于前馈神经网络，将所有的权重值初始化为小随机数是很重要的。 偏置可以初始化为零或者小的正值。</p>
<p><strong>输出单元：</strong></p>
<p><strong>1.用于高斯输出分布的线性单元（Linear Output Units）</strong>：$\hat y = W^Th+b$，通常用来预测条件高斯分布：$p(y|x)=N(y;\hat y, I)$</p>
<p><strong>2.用于Bernoulli输出分布的sigmoid单元（Sigmoid Output Units）</strong>：二分类任务，可以通过这个输出单元解决。$\hat y = σ(w^Th+b)$，其中，σ是sigmoid函数。</p>
<p><strong>3.用于 Multinoulli输出分布的softmax单元（Softmax Output Units）</strong>：$z=W^th+b$，而$softmax(z)_i=\frac{exp(z_i)}{\sum_jexp(z_j)}$，如果说argmax函数返回的是一个onehot的向量，那么softmax可以理解成soft版的argmax函数。</p>
<p><strong>隐藏单元：</strong></p>
<p><strong>1.修正线性单元（Rectified Linear Units，ReLU）</strong>：使用激活函数$g(z)=max\{0,z\}$，有$h=g(W^Tx+b)$。通常b的初始值选一个小正值，如0.1。这样relu起初很可能是被激活的。relu的一个缺点是它不能在激活值是0的时候，进行基于梯度的学习。因此又产生了各种变体。</p>
<p><strong>1.1.maxout单元：整流线性单元的一种扩展</strong>：$g(z)_i=\underset {j∈\mathbb{G}(i)}{max}z_j$，其中，$\mathbb{G}(i)$是第i组的输入索引集$\{(i−1)k+1,…,ik\}$。</p>
<p><strong>2.logistic sigmoid与双曲正切函数（Hyperbolic Tangent）单元</strong>：使用logistic sigmoid：$g(z)=σ(z)$；使用双曲正弦函数：$g(z)=tanh(z)$，其中, $tanh(z)=2σ(2z)-1$。 但是，在这两个函数的两端都很容易饱和，所以不鼓励用在隐藏单元中，一定要用可以优先选择双曲正弦函数。</p>
<p><strong>通用近似性质（Universal Approximation Properties）</strong>：一个前馈神经网络如果具有线性输出层和至少一层具有激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。 虽然具有单层的前馈网络足以表示任何函数，但是网络层可能大得不可实现，并且可能无法正确地学习和泛化。 在很多情况下，使用更深的模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误差。</p>
<p><strong>MLP的深度（Depth）</strong>：具有d个输入、深度为l、每个隐藏层具有n个单元的深度整流网络可以描述的线性区域的数量是$O(\begin{pmatrix}
n\\
d
\end{pmatrix}^{d(l−1)}n^d)$,意味着，这是深度l的指数级。</p>
<p><strong>后向传播算法（Back-Propagation）</strong>：后向传播算法将偏差（cost）在网络中从后往前传播，用来计算关于cost的梯度。后向传播算法本身不是学习算法，而是学习算法，像SGD，使用后向传播算法来计算梯度。对于bp的生动理解，可以参考<a href="https://zhihu.com/question/27239198/answer/89853077">知乎的这个回答</a>，“同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值”；“BP算法就是主动还款。e把所欠之钱还给c，d。c，d收到钱，乐呵地把钱转发给了a，b，皆大欢喜”。
<img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/fp.png" />
<img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/bp.png" /></p>
<p><strong>计算图（Computational Graphs）</strong>：节点代表变量（variable）；引入操作（operation）的概念，操作是一个或者多个变量的函数，如果一个变量y是由一个对于变量x的操作得来的，那么就可以画一条有向边，从x指向y；</p>
<p><strong>微积分中的链式法则（Chain Rule）</strong>：假设y=g(x)，z=f(g(x))=f(y)，那么有$\frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}$；进一步，如果$x∈R^m，y∈R^n，g：R^m \rightarrow R^n，f：R^n \rightarrow R$，有$\frac{\partial z}{\partial x_i}=\sum_j \frac{\partial z}{\partial y_j}\frac{\partial y_j}{\partial x_i}$，可以写成向量形式：$\triangledown _xz(\frac{\partial y}{\partial x})^T\triangledown _yz$，其中，$\frac{\partial y}{\partial x}$是n×m的g的Jacobian矩阵。</p>
<p><strong>不同框架的bp实现</strong>：1."symbol-to-number"：以计算图和数值作为图的输入，返回一系列<strong>微分的数值</strong>，作为输入的梯度。比如<strong>Torch</strong>，<strong>Caffe</strong>。2."symbol-to-symbol"：以计算图作为输入，开辟额外的图，来保存<strong>微分计算的符号表示</strong>。这种方法可以在学习算法中多次使用，并且可以用来计算更高阶的微分。比如<strong>Tensorflow</strong>，<strong>Theano</strong>。</p>
<h2 id="7.-Regularization-for-Deep-Learning">7. Regularization for Deep Learning<a class="anchor-link" href="#7.-Regularization-for-Deep-Learning">¶</a></h2><p><strong>正则化（Regularization）</strong>：对学习算法的修改——旨在减少泛化误差而不是训练误差。</p>
<p><strong>参数范数惩罚（Parameter Norm Penalties）</strong>：通过对目标函数J添加一个<strong>参数范数惩罚Ω(θ)</strong>，来限制模型的学习能力。 我们将正则化后的目标函数记为$\tilde{J}$：$\tilde{J}(θ;X,y)=J(θ;X,y)+αΩ(θ)$，其中α∈[0,∞)是权衡范数惩罚项Ω和标准目标函数 J(X;θ)相对贡献的超参数。说明，在神经网络中我们通常只对每一层仿射变换的<strong>权重w</strong>做惩罚而不对偏置做正则惩罚。典型的参数范数惩罚有$L^2$参数正则和$L^1$参数正则。</p>
<p><strong>$L^2$参数正则（$L^2$ Parameter Regularization）</strong>：也就是<strong>权重衰减（weight decay）</strong>，顾名思义，权重会有所减小；罚项为$Ω(θ)=\frac{1}{2}||w||_2^2$，也称为<strong>岭回归（ridge regression）</strong>或Tikhonov正则。 在训练过程中，只有在显著减小目标函数方向上的参数会保留得相对完好；在无助于目标函数减小的方向（对应Hessian矩阵较小的特征值）上改变参数不会显著增加梯度。 这种不重要方向对应的分量会在训练过程中因正则化而衰减掉。</p>
<p><strong>$L^1$参数正则（$L^1$ Parameter Regularization）</strong>：罚项为$Ω(θ)=||w||_1=\sum_i|w_i|$，相比于$L^2$正则，$L^1$正则产生更多的<strong>稀疏性结果</strong>，此处稀疏性指的是最优值中的一些参数为0。这一性质也使得$L^1$正则在<strong>特征选择</strong>机制中被广泛使用。</p>
<p><strong>作为约束的范数惩罚</strong>：我们可以把参数范数惩罚看作对权重强加的约束。 如果Ω是$L^2$范数，那么权重就是被约束在一个$L^2$球中。 如果Ω是$L^1$范数，那么权重就是被约束在一个$L^1$范数限制的区域中。</p>
<p><strong>数据集增强（Dataset Augmentation）</strong>：实际上，让机器学习模型泛化得更好的最好办法是使用更多的数据进行训练。 但在实践中，我们拥有的数据量是很有限的。 解决这个问题的一种方法是创建假数据并添加到训练集中。这种方法用在分类任务来说是很简单的； 但这种方法对于其他许多任务来说并不那么容易，比如密度估计问题。 数据集增强在目标识别（objective recognition）和语音识别（speech recognition）上被证实比较有效。通常情况下，人工设计的数据集增强方案可以大大减少机器学习技术的泛化误差。</p>
<p><strong>噪声鲁棒性（Noise Robustness）</strong>：噪声可以直接注入到输入数据，作为数据集增强，向输入添加方差极小的噪声等价于对权重施加范数惩罚；也可以向隐藏单元添加噪声，这种罚项更加强大；可以直接向权重w注入噪声，经常用于rnn，它推动模型进入对权重小的变化相对不敏感的区域，找到的点不只是极小点，还是由平坦区域所包围的最小点；还可以向输出目标注入噪声，比如label smoothing。</p>
<p><strong>半监督学习（Semi-Supervised Learning）</strong>：在深度学习的背景下，半监督学习通常指的是学习一个表示：h=f(x)。 学习表示的目的是使相同类中的样本有类似的表示。</p>
<p><strong>多任务学习（Multi-Task Learning）</strong>：通过合并几个任务中的样例（可以视为对参数施加的软约束）来提高泛化的一种方式。 当模型的一部分在任务之间共享时，模型的这一部分更多地被约束为良好的值（假设共享是合理的），往往能更好地泛化。从深度学习的观点看，底层的先验知识如下：能解释数据变化的因素中，某些因素是跨两个或更多任务共享的。
<img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/multi-task.png" /></p>
<p><strong>提前停止（Early Stopping）</strong>：我们经常观察到，训练误差会随着时间的推移逐渐降低，但验证集的误差会再次上升。这意味着，如果在验证集误差开始上升的时候提前停止训练，这就是“提前终止”策略。可以获得更好的模型 可能是深度学习中最常用的正则化形式。 它的流行主要是因为有效性和简单性，还可以减少训练过程的计算成本。</p>
<p><strong>参数绑定（Parameter Tying）</strong>：A模型和已经有参数的模型B任务相似，可以让A尽可能接近B，设置罚项：$Ω(w^{(A)},w^{(B)})=‖w^{(A)}−w^{(B)}‖^2_2 $</p>
<p><strong>参数共享（Parameter Sharing）</strong>：直接让A模型的参数等于B模型的参数，典型的应用是CNN。</p>
<p><strong>稀疏表示（Sparse Representation）</strong>：惩罚神经网络中的激活单元，稀疏化激活单元，惩罚项：Ω(h)。稀疏表示和$L^1$正则带来的稀疏参数容易混淆，区别：稀疏表示：<img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/sparse-rep.png" />稀疏参数：<img alt="" src="https://raw.githubusercontent.com/applenob/reading_note/master/res/sparse-weight.png" /></p>
<p><strong>Bagging（Bootstrap Aggregating）</strong>：通过结合多个模型来降低泛化误差。这种方法通常又被称为“集成方法”（Ensemble Method）。具体来说，Bagging涉及构造k个不同的数据集，再训练k个模型，集合所有模型的预测结果投票得出最后结果。模型平均是一个减少泛化误差的非常强大可靠的方法。 但在作为科学论文算法的基准时，它通常是不鼓励使用的，因为任何机器学习算法都可以从模型平均中大幅获益</p>
<p><strong>Dropout</strong>：Dropout可以被认为是集成大量深层神经网络的实用Bagging方法。 但是Dropout训练与Bagging训练不太一样。 在Bagging的情况下，所有模型都是独立的。 在Dropout的情况下，所有模型共享参数。 Dropout不仅仅是训练一个Bagging的集成模型， 并且是共享隐藏单元的集成模型。 这意味着无论其他隐藏单元是否在模型中，每个隐藏单元必须都能够表现良好。 隐藏单元必须准备好进行模型之间的交换和互换。 因此Dropout正则化每个隐藏单元不仅是一个很好的特征，更要在许多情况下是良好的特征。 除此之外，计算方便是Dropout的一个优点，Dropout的另一个显著优点是不怎么限制适用的模型或训练过程。 然而，当只有极少的训练样本可用时，Dropout不会很有效。</p>
<p><strong>对抗训练（Adversarial Training）</strong>： 对抗训练通过鼓励网络在训练数据附近的局部区域恒定来限制这一高度敏感的局部线性行为。 这可以被看作是一种明确地向监督神经网络引入局部恒定先验的方法。</p>

<hr />
<hr />
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
</div>
</div>
</div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div>
	
        <hr>

    </div>
        </div>
        
        <div class="span3">

            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Site
                </li>
            
                <li><a href="https://applenob.github.io/archives.html">Archives</a>
                <li><a href="https://applenob.github.io/tags.html">Tags</a>



                <li><a href="https://applenob.github.io/feeds/all.atom.xml" rel="alternate">Atom feed</a></li>

            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Categories
                </li>
                
                <li><a href="https://applenob.github.io/category/algorithm.html">Algorithm</a></li>
                <li><a href="https://applenob.github.io/category/deep-learning.html">Deep Learning</a></li>
                <li><a href="https://applenob.github.io/category/general-ai.html">General AI</a></li>
                <li><a href="https://applenob.github.io/category/machine-learning.html">Machine Learning</a></li>
                <li><a href="https://applenob.github.io/category/nlp.html">NLP</a></li>
                <li><a href="https://applenob.github.io/category/python.html">Python</a></li>
                <li><a href="https://applenob.github.io/category/reinforcement-learning.html">Reinforcement Learning</a></li>
                <li><a href="https://applenob.github.io/category/tensorflow.html">Tensorflow</a></li>
                   
            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Links
                </li>
            
                <li><a href="http://getpelican.com/">Pelican</a></li>
                <li><a href="http://python.org/">Python.org</a></li>
                <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
            </ul>
            </div>


            <div class="social">
            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Social
                </li>
           
                <li><a href="https://github.com/applenob">github</a></li>
                <li><a href="https://twitter.com/wooolven">twitter</a></li>
                <li><a href="http://weibo.com/u/2350205681">微博</a></li>
                <li><a href="https://applenob.github.io/static/resume_junwen.pdf">CV</a></li>
                <li><a href="https://applenob.github.io/static/cv_cn.pdf">个人简历，欢迎骚扰：）</a></li>
            </ul>
            </div>
            </div>

        </div>  
    </div>     </div> 
<footer>
<br />
<p><a href="https://applenob.github.io">Kevin Chan's blog</a> &copy; Kevin Chan 2018</p>
</footer>

</div> <!-- /container -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="https://applenob.github.io/theme/bootstrap-collapse.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-99008972-1', 'auto');
  ga('send', 'pageview');

</script>
 
</body>
</html>