<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>DanteLiujie's blog - ASR Processing</title>
    <link rel="icon" href="https://applenob.github.io/static/favicon.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="https://applenob.github.io/static/favicon.ico" type="image/x-icon" />
    <meta name="description" content="">
    <meta name="author" content="Kevin Chan">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="https://applenob.github.io/theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="https://applenob.github.io/theme/bootstrap.min.css" rel="stylesheet">
    <link href="https://applenob.github.io/theme/bootstrap.min.responsive.css" rel="stylesheet">
    <link href="https://applenob.github.io/theme/local.css" rel="stylesheet">
    <link href="https://applenob.github.io/theme/pygments.css" rel="stylesheet">

    <!-- So Firefox can bookmark->"abo this site" -->
    <link href="https://applenob.github.io/feeds/all.atom.xml" rel="alternate" title="DanteLiujie's blog" type="application/atom+xml">
</head>

<body>

<div class="navbar">
    <div class="navbar-inner">
    <div class="container">

         <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
         </a>

        <a class="brand" href="https://danteliujie.github.io">DanteLiujie's blog</a>

        <div class="nav-collapse">
        <ul class="nav">
            
        </ul>
        </div>
        
    </div>
    </div>
</div>

<div class="container">
    <div class="content">
    <div class="row">

    <div class="span9" style="width:100%">
    <div class='article'>
        <div class="content-title">
            <h1>ASR Processing</h1>
            全部内容来自 <a class="url fn" href="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/">Shichaog1</a> 的gitbook, 修改了一些显示style以获取所需的打印效果, 修正了一些小错误, 主要目的用于个人打印用.
        </div>

        <div>
		<style type="text/css">
			/*!
			*
			* IPython notebook
			*
			*/
			/* CSS font colors for translated ANSI colors. */
			.ansibold {
			  font-weight: bold;
			}
			/* use dark versions for foreground, to improve visibility */
			.ansiblack {
			  color: black;
			}
			.ansired {
			  color: darkred;
			}
			.ansigreen {
			  color: darkgreen;
			}
			.ansiyellow {
			  color: #c4a000;
			}
			.ansiblue {
			  color: darkblue;
			}
			.ansipurple {
			  color: darkviolet;
			}
			.ansicyan {
			  color: steelblue;
			}
			.ansigray {
			  color: gray;
			}
			/* and light for background, for the same reason */
			.ansibgblack {
			  background-color: black;
			}
			.ansibgred {
			  background-color: red;
			}
			.ansibggreen {
			  background-color: green;
			}
			.ansibgyellow {
			  background-color: yellow;
			}
			.ansibgblue {
			  background-color: blue;
			}
			.ansibgpurple {
			  background-color: magenta;
			}
			.ansibgcyan {
			  background-color: cyan;
			}
			.ansibggray {
			  background-color: gray;
			}
			div.cell {
			  /* Old browsers */
			  display: -webkit-box;
			  -webkit-box-orient: vertical;
			  -webkit-box-align: stretch;
			  display: -moz-box;
			  -moz-box-orient: vertical;
			  -moz-box-align: stretch;
			  display: box;
			  box-orient: vertical;
			  box-align: stretch;
			  /* Modern browsers */
			  display: flex;
			  flex-direction: column;
			  align-items: stretch;
			  border-radius: 2px;
			  box-sizing: border-box;
			  -moz-box-sizing: border-box;
			  -webkit-box-sizing: border-box;
			  border-width: 1px;
			  border-style: solid;
			  border-color: transparent;
			  width: 100%;
			  padding: 5px;
			  /* This acts as a spacer between cells, that is outside the border */
			  margin: 0px;
			  outline: none;
			  border-left-width: 1px;
			  padding-left: 5px;
			  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
			}
			div.cell.jupyter-soft-selected {
			  border-left-color: #90CAF9;
			  border-left-color: #E3F2FD;
			  border-left-width: 1px;
			  padding-left: 5px;
			  border-right-color: #E3F2FD;
			  border-right-width: 1px;
			  background: #E3F2FD;
			}
			@media print {
			  div.cell.jupyter-soft-selected {
				border-color: transparent;
			  }
			}
			div.cell.selected {
			  border-color: #ababab;
			  border-left-width: 0px;
			  padding-left: 6px;
			  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
			}
			@media print {
			  div.cell.selected {
				border-color: transparent;
			  }
			}
			div.cell.selected.jupyter-soft-selected {
			  border-left-width: 0;
			  padding-left: 6px;
			  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
			}
			.edit_mode div.cell.selected {
			  border-color: #66BB6A;
			  border-left-width: 0px;
			  padding-left: 6px;
			  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
			}
			@media print {
			  .edit_mode div.cell.selected {
				border-color: transparent;
			  }
			}
			.prompt {
			  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
			  min-width: 14ex;
			  /* This padding is tuned to match the padding on the CodeMirror editor. */
			  padding: 0.4em;
			  margin: 0px;
			  font-family: monospace;
			  text-align: right;
			  /* This has to match that of the the CodeMirror class line-height below */
			  line-height: 1.21429em;
			  /* Don't highlight prompt number selection */
			  -webkit-touch-callout: none;
			  -webkit-user-select: none;
			  -khtml-user-select: none;
			  -moz-user-select: none;
			  -ms-user-select: none;
			  user-select: none;
			  /* Use default cursor */
			  cursor: default;
			}
			@media (max-width: 540px) {
			  .prompt {
				text-align: left;
			  }
			}
			div.inner_cell {
			  min-width: 0;
			  /* Old browsers */
			  display: -webkit-box;
			  -webkit-box-orient: vertical;
			  -webkit-box-align: stretch;
			  display: -moz-box;
			  -moz-box-orient: vertical;
			  -moz-box-align: stretch;
			  display: box;
			  box-orient: vertical;
			  box-align: stretch;
			  /* Modern browsers */
			  display: flex;
			  flex-direction: column;
			  align-items: stretch;
			  /* Old browsers */
			  -webkit-box-flex: 1;
			  -moz-box-flex: 1;
			  box-flex: 1;
			  /* Modern browsers */
			  flex: 1;
			}
			/* input_area and input_prompt must match in top border and margin for alignment */
			div.input_area {
			  border: 1px solid #cfcfcf;
			  border-radius: 2px;
			  background: #f7f7f7;
			  line-height: 1.21429em;
			}
			/* This is needed so that empty prompt areas can collapse to zero height when there
			   is no content in the output_subarea and the prompt. The main purpose of this is
			   to make sure that empty JavaScript output_subareas have no height. */
			div.prompt:empty {
			  padding-top: 0;
			  padding-bottom: 0;
			}
			div.unrecognized_cell {
			  padding: 5px 5px 5px 0px;
			  /* Old browsers */
			  display: -webkit-box;
			  -webkit-box-orient: horizontal;
			  -webkit-box-align: stretch;
			  display: -moz-box;
			  -moz-box-orient: horizontal;
			  -moz-box-align: stretch;
			  display: box;
			  box-orient: horizontal;
			  box-align: stretch;
			  /* Modern browsers */
			  display: flex;
			  flex-direction: row;
			  align-items: stretch;
			}
			div.unrecognized_cell .inner_cell {
			  border-radius: 2px;
			  padding: 5px;
			  font-weight: bold;
			  color: red;
			  border: 1px solid #cfcfcf;
			  background: #eaeaea;
			}
			div.unrecognized_cell .inner_cell a {
			  color: inherit;
			  text-decoration: none;
			}
			div.unrecognized_cell .inner_cell a:hover {
			  color: inherit;
			  text-decoration: none;
			}
			@media (max-width: 540px) {
			  div.unrecognized_cell > div.prompt {
				display: none;
			  }
			}
			div.code_cell {
			  /* avoid page breaking on code cells when printing */
			}
			@media print {
			  div.code_cell {
				page-break-inside: avoid;
			  }
			}
			/* any special styling for code cells that are currently running goes here */
			div.input {
			  page-break-inside: avoid;
			  /* Old browsers */
			  display: -webkit-box;
			  -webkit-box-orient: horizontal;
			  -webkit-box-align: stretch;
			  display: -moz-box;
			  -moz-box-orient: horizontal;
			  -moz-box-align: stretch;
			  display: box;
			  box-orient: horizontal;
			  box-align: stretch;
			  /* Modern browsers */
			  display: flex;
			  flex-direction: row;
			  align-items: stretch;
			}
			@media (max-width: 540px) {
			  div.input {
				/* Old browsers */
				display: -webkit-box;
				-webkit-box-orient: vertical;
				-webkit-box-align: stretch;
				display: -moz-box;
				-moz-box-orient: vertical;
				-moz-box-align: stretch;
				display: box;
				box-orient: vertical;
				box-align: stretch;
				/* Modern browsers */
				display: flex;
				flex-direction: column;
				align-items: stretch;
			  }
			}
			/* input_area and input_prompt must match in top border and margin for alignment */
			div.input_prompt {
			  color: #303F9F;
			  border-top: 1px solid transparent;
			}
			div.input_area > div.highlight {
			  margin: 0.4em;
			  border: none;
			  padding: 0px;
			  background-color: transparent;
			}
			div.input_area > div.highlight > pre {
			  margin: 0px;
			  border: none;
			  padding: 0px;
			  background-color: transparent;
			}
			/* The following gets added to the <head> if it is detected that the user has a
			 * monospace font with inconsistent normal/bold/italic height.  See
			 * notebookmain.js.  Such fonts will have keywords vertically offset with
			 * respect to the rest of the text.  The user should select a better font.
			 * See: https://github.com/ipython/ipython/issues/1503
			 *
			 * .CodeMirror span {
			 *      vertical-align: bottom;
			 * }
			 */
			.CodeMirror {
			  line-height: 1.21429em;
			  /* Changed from 1em to our global default */
			  font-size: 14px;
			  height: auto;
			  /* Changed to auto to autogrow */
			  background: none;
			  /* Changed from white to allow our bg to show through */
			}
			.CodeMirror-scroll {
			  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
			  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
			  overflow-y: hidden;
			  overflow-x: auto;
			}
			.CodeMirror-lines {
			  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
			  /* we have set a different line-height and want this to scale with that. */
			  padding: 0.4em;
			}
			.CodeMirror-linenumber {
			  padding: 0 8px 0 4px;
			}
			.CodeMirror-gutters {
			  border-bottom-left-radius: 2px;
			  border-top-left-radius: 2px;
			}
			.CodeMirror pre {
			  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
			  /* .CodeMirror-lines */
			  padding: 0;
			  border: 0;
			  border-radius: 0;
			}
			/*

			Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
			Adapted from GitHub theme

			*/
			.highlight-base {
			  color: #000;
			}
			.highlight-variable {
			  color: #000;
			}
			.highlight-variable-2 {
			  color: #1a1a1a;
			}
			.highlight-variable-3 {
			  color: #333333;
			}
			.highlight-string {
			  color: #BA2121;
			}
			.highlight-comment {
			  color: #408080;
			  font-style: italic;
			}
			.highlight-number {
			  color: #080;
			}
			.highlight-atom {
			  color: #88F;
			}
			.highlight-keyword {
			  color: #008000;
			  font-weight: bold;
			}
			.highlight-builtin {
			  color: #008000;
			}
			.highlight-error {
			  color: #f00;
			}
			.highlight-operator {
			  color: #AA22FF;
			  font-weight: bold;
			}
			.highlight-meta {
			  color: #AA22FF;
			}
			/* previously not defined, copying from default codemirror */
			.highlight-def {
			  color: #00f;
			}
			.highlight-string-2 {
			  color: #f50;
			}
			.highlight-qualifier {
			  color: #555;
			}
			.highlight-bracket {
			  color: #997;
			}
			.highlight-tag {
			  color: #170;
			}
			.highlight-attribute {
			  color: #00c;
			}
			.highlight-header {
			  color: blue;
			}
			.highlight-quote {
			  color: #090;
			}
			.highlight-link {
			  color: #00c;
			}
			/* apply the same style to codemirror */
			.cm-s-ipython span.cm-keyword {
			  color: #008000;
			  font-weight: bold;
			}
			.cm-s-ipython span.cm-atom {
			  color: #88F;
			}
			.cm-s-ipython span.cm-number {
			  color: #080;
			}
			.cm-s-ipython span.cm-def {
			  color: #00f;
			}
			.cm-s-ipython span.cm-variable {
			  color: #000;
			}
			.cm-s-ipython span.cm-operator {
			  color: #AA22FF;
			  font-weight: bold;
			}
			.cm-s-ipython span.cm-variable-2 {
			  color: #1a1a1a;
			}
			.cm-s-ipython span.cm-variable-3 {
			  color: #333333;
			}
			.cm-s-ipython span.cm-comment {
			  color: #408080;
			  font-style: italic;
			}
			.cm-s-ipython span.cm-string {
			  color: #BA2121;
			}
			.cm-s-ipython span.cm-string-2 {
			  color: #f50;
			}
			.cm-s-ipython span.cm-meta {
			  color: #AA22FF;
			}
			.cm-s-ipython span.cm-qualifier {
			  color: #555;
			}
			.cm-s-ipython span.cm-builtin {
			  color: #008000;
			}
			.cm-s-ipython span.cm-bracket {
			  color: #997;
			}
			.cm-s-ipython span.cm-tag {
			  color: #170;
			}
			.cm-s-ipython span.cm-attribute {
			  color: #00c;
			}
			.cm-s-ipython span.cm-header {
			  color: blue;
			}
			.cm-s-ipython span.cm-quote {
			  color: #090;
			}
			.cm-s-ipython span.cm-link {
			  color: #00c;
			}
			.cm-s-ipython span.cm-error {
			  color: #f00;
			}
			.cm-s-ipython span.cm-tab {
			  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
			  background-position: right;
			  background-repeat: no-repeat;
			}
			div.output_wrapper {
			  /* this position must be relative to enable descendents to be absolute within it */
			  position: relative;
			  /* Old browsers */
			  display: -webkit-box;
			  -webkit-box-orient: vertical;
			  -webkit-box-align: stretch;
			  display: -moz-box;
			  -moz-box-orient: vertical;
			  -moz-box-align: stretch;
			  display: box;
			  box-orient: vertical;
			  box-align: stretch;
			  /* Modern browsers */
			  display: flex;
			  flex-direction: column;
			  align-items: stretch;
			  z-index: 1;
			}
			/* class for the output area when it should be height-limited */
			div.output_scroll {
			  /* ideally, this would be max-height, but FF barfs all over that */
			  height: 24em;
			  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
			  width: 100%;
			  overflow: auto;
			  border-radius: 2px;
			  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
			  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
			  display: block;
			}
			/* output div while it is collapsed */
			div.output_collapsed {
			  margin: 0px;
			  padding: 0px;
			  /* Old browsers */
			  display: -webkit-box;
			  -webkit-box-orient: vertical;
			  -webkit-box-align: stretch;
			  display: -moz-box;
			  -moz-box-orient: vertical;
			  -moz-box-align: stretch;
			  display: box;
			  box-orient: vertical;
			  box-align: stretch;
			  /* Modern browsers */
			  display: flex;
			  flex-direction: column;
			  align-items: stretch;
			}
			div.out_prompt_overlay {
			  height: 100%;
			  padding: 0px 0.4em;
			  position: absolute;
			  border-radius: 2px;
			}
			div.out_prompt_overlay:hover {
			  /* use inner shadow to get border that is computed the same on WebKit/FF */
			  -webkit-box-shadow: inset 0 0 1px #000;
			  box-shadow: inset 0 0 1px #000;
			  background: rgba(240, 240, 240, 0.5);
			}
			div.output_prompt {
			  color: #D84315;
			}
			/* This class is the outer container of all output sections. */
			div.output_area {
			  padding: 0px;
			  page-break-inside: avoid;
			  /* Old browsers */
			  display: -webkit-box;
			  -webkit-box-orient: horizontal;
			  -webkit-box-align: stretch;
			  display: -moz-box;
			  -moz-box-orient: horizontal;
			  -moz-box-align: stretch;
			  display: box;
			  box-orient: horizontal;
			  box-align: stretch;
			  /* Modern browsers */
			  display: flex;
			  flex-direction: row;
			  align-items: stretch;
			}
			div.output_area .MathJax_Display {
			  text-align: left !important;
			}
			div.output_area 
			div.output_area 
			div.output_area img,
			div.output_area svg {
			  max-width: 100%;
			  height: auto;
			}
			div.output_area img.unconfined,
			div.output_area svg.unconfined {
			  max-width: none;
			}
			/* This is needed to protect the pre formating from global settings such
			   as that of bootstrap */
			.output {
			  /* Old browsers */
			  display: -webkit-box;
			  -webkit-box-orient: vertical;
			  -webkit-box-align: stretch;
			  display: -moz-box;
			  -moz-box-orient: vertical;
			  -moz-box-align: stretch;
			  display: box;
			  box-orient: vertical;
			  box-align: stretch;
			  /* Modern browsers */
			  display: flex;
			  flex-direction: column;
			  align-items: stretch;
			}
			@media (max-width: 540px) {
			  div.output_area {
				/* Old browsers */
				display: -webkit-box;
				-webkit-box-orient: vertical;
				-webkit-box-align: stretch;
				display: -moz-box;
				-moz-box-orient: vertical;
				-moz-box-align: stretch;
				display: box;
				box-orient: vertical;
				box-align: stretch;
				/* Modern browsers */
				display: flex;
				flex-direction: column;
				align-items: stretch;
			  }
			}
			div.output_area pre {
			  margin: 0;
			  padding: 0;
			  border: 0;
			  vertical-align: baseline;
			  color: black;
			  background-color: transparent;
			  border-radius: 0;
			}
			/* This class is for the output subarea inside the output_area and after
			   the prompt div. */
			div.output_subarea {
			  overflow-x: auto;
			  padding: 0.4em;
			  /* Old browsers */
			  -webkit-box-flex: 1;
			  -moz-box-flex: 1;
			  box-flex: 1;
			  /* Modern browsers */
			  flex: 1;
			  max-width: calc(100% - 14ex);
			}
			div.output_scroll div.output_subarea {
			  overflow-x: visible;
			}
			/* The rest of the output_* classes are for special styling of the different
			   output types */
			/* all text output has this class: */
			div.output_text {
			  text-align: left;
			  color: #000;
			  /* This has to match that of the the CodeMirror class line-height below */
			  line-height: 1.21429em;
			}
			/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
			div.output_stderr {
			  background: #fdd;
			  /* very light red background for stderr */
			}
			div.output_latex {
			  text-align: left;
			}
			/* Empty output_javascript divs should have no height */
			div.output_javascript:empty {
			  padding: 0;
			}
			.js-error {
			  color: darkred;
			}
			/* raw_input styles */
			div.raw_input_container {
			  line-height: 1.21429em;
			  padding-top: 5px;
			}
			pre.raw_input_prompt {
			  /* nothing needed here. */
			}
			input.raw_input {
			  font-family: monospace;
			  font-size: inherit;
			  color: inherit;
			  width: auto;
			  /* make sure input baseline aligns with prompt */
			  vertical-align: baseline;
			  /* padding + margin = 0.5em between prompt and cursor */
			  padding: 0em 0.25em;
			  margin: 0em 0.25em;
			}
			input.raw_input:focus {
			  box-shadow: none;
			}
			p.p-space {
			  margin-bottom: 10px;
			}
			div.output_unrecognized {
			  padding: 5px;
			  font-weight: bold;
			  color: red;
			}
			div.output_unrecognized a {
			  color: inherit;
			  text-decoration: none;
			}
			div.output_unrecognized a:hover {
			  color: inherit;
			  text-decoration: none;
			}
			.rendered_html {
			  color: #000;
			  /* any extras will just be numbers: */
			}



			.rendered_html :link {
			  text-decoration: underline;
			}
			.rendered_html :visited {
			  text-decoration: underline;
			}






			.rendered_html h1:first-child {
			  margin-top: 0.538em;
			}
			.rendered_html h2:first-child {
			  margin-top: 0.636em;
			}
			.rendered_html h3:first-child {
			  margin-top: 0.777em;
			}
			.rendered_html h4:first-child {
			  margin-top: 1em;
			}
			.rendered_html h5:first-child {
			  margin-top: 1em;
			}
			.rendered_html h6:first-child {
			  margin-top: 1em;
			}








			.rendered_html * + ul {
			  margin-top: 1em;
			}
			.rendered_html * + ol {
			  margin-top: 1em;
			}


			.rendered_html pre,



			.rendered_html tr,
			.rendered_html th,

			.rendered_html td,


			.rendered_html * + table {
			  margin-top: 1em;
			}

			.rendered_html * + p {
			  margin-top: 1em;
			}

			.rendered_html * + img {
			  margin-top: 1em;
			}
			.rendered_html img,

			.rendered_html img.unconfined,

			div.text_cell {
			  /* Old browsers */
			  display: -webkit-box;
			  -webkit-box-orient: horizontal;
			  -webkit-box-align: stretch;
			  display: -moz-box;
			  -moz-box-orient: horizontal;
			  -moz-box-align: stretch;
			  display: box;
			  box-orient: horizontal;
			  box-align: stretch;
			  /* Modern browsers */
			  display: flex;
			  flex-direction: row;
			  align-items: stretch;
			}
			@media (max-width: 540px) {
			  div.text_cell > div.prompt {
				display: none;
			  }
			}
			div.text_cell_render {
			  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
			  outline: none;
			  resize: none;
			  width: inherit;
			  border-style: none;
			  padding: 0.5em 0.5em 0.5em 0.4em;
			  color: #000;
			  box-sizing: border-box;
			  -moz-box-sizing: border-box;
			  -webkit-box-sizing: border-box;
			}
			a.anchor-link:link {
			  text-decoration: none;
			  padding: 0px 20px;
			  visibility: hidden;
			}
			h1:hover .anchor-link,
			h2:hover .anchor-link,
			h3:hover .anchor-link,
			h4:hover .anchor-link,
			h5:hover .anchor-link,
			h6:hover .anchor-link {
			  visibility: visible;
			}
			.text_cell.rendered .input_area {
			  display: none;
			}
			.text_cell.rendered 
			.text_cell.unrendered .text_cell_render {
			  display: none;
			}
			.cm-header-1,
			.cm-header-2,
			.cm-header-3,
			.cm-header-4,
			.cm-header-5,
			.cm-header-6 {
			  font-weight: bold;
			  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
			}
			.cm-header-1 {
			  font-size: 185.7%;
			}
			.cm-header-2 {
			  font-size: 157.1%;
			}
			.cm-header-3 {
			  font-size: 128.6%;
			}
			.cm-header-4 {
			  font-size: 110%;
			}
			.cm-header-5 {
			  font-size: 100%;
			  font-style: italic;
			}
			.cm-header-6 {
			  font-size: 100%;
			  font-style: italic;
			}
		</style>
		<style type="text/css">
			.highlight .hll { background-color: #ffffcc }
			.highlight  { background: #f8f8f8; }
			.highlight .c { color: #408080; font-style: italic } /* Comment */
			.highlight .err { border: 1px solid #FF0000 } /* Error */
			.highlight .k { color: #008000; font-weight: bold } /* Keyword */
			.highlight .o { color: #666666 } /* Operator */
			.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
			.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
			.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
			.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
			.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
			.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
			.highlight .gd { color: #A00000 } /* Generic.Deleted */
			.highlight .ge { font-style: italic } /* Generic.Emph */
			.highlight .gr { color: #FF0000 } /* Generic.Error */
			.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
			.highlight .gi { color: #00A000 } /* Generic.Inserted */
			.highlight .go { color: #888888 } /* Generic.Output */
			.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
			.highlight .gs { font-weight: bold } /* Generic.Strong */
			.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
			.highlight .gt { color: #0044DD } /* Generic.Traceback */
			.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
			.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
			.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
			.highlight .kp { color: #008000 } /* Keyword.Pseudo */
			.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
			.highlight .kt { color: #B00040 } /* Keyword.Type */
			.highlight .m { color: #666666 } /* Literal.Number */
			.highlight .s { color: #BA2121 } /* Literal.String */
			.highlight .na { color: #7D9029 } /* Name.Attribute */
			.highlight .nb { color: #008000 } /* Name.Builtin */
			.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
			.highlight .no { color: #880000 } /* Name.Constant */
			.highlight .nd { color: #AA22FF } /* Name.Decorator */
			.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
			.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
			.highlight .nf { color: #0000FF } /* Name.Function */
			.highlight .nl { color: #A0A000 } /* Name.Label */
			.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
			.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
			.highlight .nv { color: #19177C } /* Name.Variable */
			.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
			.highlight .w { color: #bbbbbb } /* Text.Whitespace */
			.highlight .mb { color: #666666 } /* Literal.Number.Bin */
			.highlight .mf { color: #666666 } /* Literal.Number.Float */
			.highlight .mh { color: #666666 } /* Literal.Number.Hex */
			.highlight .mi { color: #666666 } /* Literal.Number.Integer */
			.highlight .mo { color: #666666 } /* Literal.Number.Oct */
			.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
			.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
			.highlight .sc { color: #BA2121 } /* Literal.String.Char */
			.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
			.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
			.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
			.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
			.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
			.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
			.highlight .sx { color: #008000 } /* Literal.String.Other */
			.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
			.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
			.highlight .ss { color: #19177C } /* Literal.String.Symbol */
			.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
			.highlight .fm { color: #0000FF } /* Name.Function.Magic */
			.highlight .vc { color: #19177C } /* Name.Variable.Class */
			.highlight .vg { color: #19177C } /* Name.Variable.Global */
			.highlight .vi { color: #19177C } /* Name.Variable.Instance */
			.highlight .vm { color: #19177C } /* Name.Variable.Magic */
			.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
		</style>
		<style type="text/css">
			/* Temporary definitions which will become obsolete with Notebook release 5.0 */
			.ansi-black-fg { color: #3E424D; }
			.ansi-black-bg { background-color: #3E424D; }
			.ansi-black-intense-fg { color: #282C36; }
			.ansi-black-intense-bg { background-color: #282C36; }
			.ansi-red-fg { color: #E75C58; }
			.ansi-red-bg { background-color: #E75C58; }
			.ansi-red-intense-fg { color: #B22B31; }
			.ansi-red-intense-bg { background-color: #B22B31; }
			.ansi-green-fg { color: #00A250; }
			.ansi-green-bg { background-color: #00A250; }
			.ansi-green-intense-fg { color: #007427; }
			.ansi-green-intense-bg { background-color: #007427; }
			.ansi-yellow-fg { color: #DDB62B; }
			.ansi-yellow-bg { background-color: #DDB62B; }
			.ansi-yellow-intense-fg { color: #B27D12; }
			.ansi-yellow-intense-bg { background-color: #B27D12; }
			.ansi-blue-fg { color: #208FFB; }
			.ansi-blue-bg { background-color: #208FFB; }
			.ansi-blue-intense-fg { color: #0065CA; }
			.ansi-blue-intense-bg { background-color: #0065CA; }
			.ansi-magenta-fg { color: #D160C4; }
			.ansi-magenta-bg { background-color: #D160C4; }
			.ansi-magenta-intense-fg { color: #A03196; }
			.ansi-magenta-intense-bg { background-color: #A03196; }
			.ansi-cyan-fg { color: #60C6C8; }
			.ansi-cyan-bg { background-color: #60C6C8; }
			.ansi-cyan-intense-fg { color: #258F8F; }
			.ansi-cyan-intense-bg { background-color: #258F8F; }
			.ansi-white-fg { color: #C5C1B4; }
			.ansi-white-bg { background-color: #C5C1B4; }
			.ansi-white-intense-fg { color: #A1A6B2; }
			.ansi-white-intense-bg { background-color: #A1A6B2; }
			.ansi-bold { font-weight: bold; }
		</style>
	</div>
	<div>
		<div>	
		<h1 id="my-asr-book">My ASR Book</h1>
		<h2 id="目录">目录<a class="anchor-link" href="#目录">¶</a></h2><ul>
		<li><a href="https://danteliujie.github.io/asr_processing#简介">0.简介</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#第一章-结构和器件">1. 结构和器件</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#第二章-重采样原理及实现">2. 重采样原理及实现</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#自动回声消除aec原理">3. 自动回声消除(AEC)原理</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#rec（residual-echo-control）">4. 自动回声消除(续)</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#自动回声消除aec实现实例">5. 自动回声消除(AEC)实现实例</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#有源降噪耳机">6. 有源降噪耳机</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#语音检测vad原理和实例">7. 语音检测(VAD)原理和实例</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#声源定位doa">8. 噪声抑制(NS)原理和实例</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#声源定位doa">9. 声源定位(DOA)技术</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#语音信号预加重">10. 语音预加重</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#语音去混响dereverberation">11. 语音去混响</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#声源分离技术">12. 声源分离技术</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#波束形成实例">13. 波束形成(beamforming)实例</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#第十二章-波束形成进阶">14. 波束形成进阶</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#第十三章-基于深度学习deep-learning的语音增强">15. 基于深度学习的语音增强</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#第十四章-kaldi入门">16. kaldi入门</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#kaldi-中文asr实例">17. kaldi中文ASR实例</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#tensorflow入门">18. tensorflow入门</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#唤醒词">19. 唤醒词识别</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#语音识别概述">20. 语音识别概述</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#算法工程化">21. 算法工程化</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#附录1-高斯分布和em算法">A1. 高斯分布和EM算法</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#线程编程">A2. 多线程编程</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#有源噪声控制">A3. 有源噪声控制</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#arm在深度学习上的进展">A4. ARM在深度学习上的进展</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#定点化">A5. 定点化</a></li>
		<li><a href="https://danteliujie.github.io/asr_processing#开发工具合集">A6. 开发工具合集</a></li>
		</ul>
<h1 id="简介">简介</h1>
<p class="comments-section">语音识别的基本任务是将人类语音转换成相应的文字，语音识别技术设计的领域主要包括信号处理，模式识别，概率论，信息论和人工智能等。一个典型的语音识别系统如下：<div class="comments-icon"></div></p>
<ul>
<li>信号处理主要用于声学特征提取，声学模型可以根据该特征计算声学单元各成分的概率，对于基于深度学习的方法则可以直接有声学特征到文字（声学打分含在深度学习的隐藏层中），在日常的嘈杂环境中，信号处理还被用于语音增强，这包括回声消除，主动降噪，波束形成，去混响等等。</li>
<li>声学模型，通常一个字的发音分为多个音节，每个音节又分为若干音素，不同的字是不同音节的有序组合，不同的音节又是不同音素的有序组合，这些有序组合通常使用马尔科夫链来表示，在实际工程计算中使用加权有限自动机(wfst)来表示。特征常用的有LPC，MFCC，在深度学习方法之后也可以采用logFbank做为特征，这在减少计算量的同时，以类似人耳的方式对音频进行了处理，以获得较好的语音识别性能。</li>
<li>发音词典，包含了语音识别系统能够处理的所有词汇及其发音，发音字典建立了声学模型单元和语言模型单元间的映射关系。</li>
<li>语言模型，语言模型是对人类说话习惯性的描述，如"请关门"和"请关闷"，如果给发音”qing guan men“,那么绝大多数人会认为是请关门，语言模型通常基于统计的N元文法及其变体。</li>
<li>解码器，解码器是语音识别系统的核心之一，其根据输入的信号，根据声学特征，语言模型以及发音词典寻找最大概率的词串。 </li>
<li>开源语音识别框架，kaldi用的较为广泛，且kaldi已经支持开源深度学习框架tensorflow，未来将会更加完善，搭建一个demo级别的语音识别系统将轻轻松松。一个语音识别系统的准确率将取决于训练的语料集和文本集合。</li>
</ul>
<p class="comments-section">该书主要针对是否远场语音识别自然交互的应用场景的相关语音技术，包括声学特性，麦克风特性，语音增强技术和语音识别技术。这会涉及到结构设计验证，麦克风选择。<div class="comments-icon"></div></p>
<p class="comments-section">第一章主要是结构和器件在ASR中应用注意事项<br>对于结构设计，主要是结构设计上对麦克风采集到的信号的影响，包括结构震动，腔体效应，管道效应，当前也包括器件的物理特性，主要是麦克风拾音开孔和内部腔体。<div class="comments-icon"></div></p>
<p class="comments-section">第二章~第十三章是基于信号处理方法的语音增强技术<br>对于语音增强技术，主要是指常用的语音信号处理，这包括VAD（voice activity detection）,AEC(automatic echo cacellantion),DOA(direction of arrival),NS(noise suppresion),BF(beamforming), BSS(blind source seperation)等偏重信号处理层面的，会有针对具体开源库的实例算法讲解。<div class="comments-icon"></div></p>
<p class="comments-section">第十四章~十八章是基于kaldi和tensorflow开源框架的语音识别系统搭建.<br>包括特征提取，混合高斯模型/NN（neutral network）声学模型和声学解码,语言模型等。<div class="comments-icon"></div></p>
<p class="comments-section">第十九章<br>最后还会附上算法工程化上的一些笔者体会.<div class="comments-icon"></div></p>
<p class="comments-section">最后的附录是一些数学和信号处理知识点的梳理.<div class="comments-icon"></div></p>
<p class="comments-section">注:似乎在线的对latex格式公式支持不好.可以使用gitbook editor以markdown格式看。<div class="comments-icon-"></div></p>
</section>
	<h1 id="第一章-结构和器件">第一章 结构和器件</h1>
<p class="comments-section">对于自然语音识别场景，仅有语音增强和语音识别算法还不足以确定语音识别的准确性,产品结构引入的声学特性变换和采集器件的非理想因素会导致获取到的语音质量的下降,这些不利影响很难通过算法进行完全的弥补.由于这个方面并不是本书的重点,在此花一个篇章进行概述性聊聊。
首先是产品的结构的影响,在一个好的音响设计系统中,不可避免的会涉及到影响的声学设计,一般市场千元左右的音响,以全频段喇叭加低音辐射盆的结构形式.低音辐射盆用于弥补全频段喇叭在低音上的缺陷,这个设计会在一定程度上对AEC(automatic echo cancellation)提出一定的挑战.且音响质量越好的系统,往往对ASR提出的挑战越大.<div class="comments-icon"></div></p>
<h2 id="喇叭">喇叭</h2>
<h3 id="电学性能">电学性能</h3>
<p class="comments-section">音响上常用的电学性能测试设备是AudioPrecision,这里罗列出比较关心的指标，测试环境的搭建这里略过.<div class="comments-icon"></div></p>
<table>
<thead>
<tr>
<th>测试项</th>
<th>项目的意义</th>
<th>影响的声学特性</th>
</tr>
</thead>
<tbody>
<tr>
<td>THD+N</td>
<td>Total Harmonic Distortion +Noise</td>
<td>描述的是功放器件在给定功率下的谐波失真加噪声,这就意味这经过功放器件后,声音和原始音源差异</td>
</tr>
<tr>
<td>SNR</td>
<td>Signal to noise Ratio</td>
<td>反映的是信号和噪声的比,在无失真前提下,越大越好</td>
</tr>
<tr>
<td>Crosstalk</td>
<td></td>
<td>反映的是一个声道对另一个声道的影响,通常好的影响,我们希望有两个喇叭做左右声道,以增强立体感</td>
</tr>
<tr>
<td>Distortion</td>
<td></td>
<td>不同频率的声音经过功放器件后,其失真是不一样的</td>
</tr>
<tr>
<td>DC offset</td>
<td></td>
<td>反映的是无声音输入时,功放器件的输出情况</td>
</tr>
</tbody>
</table>
<p class="comments-section">实际上电子集成器件技术的发展已经在可以使上述指标可以用相对较低的成本满足ASR的要求了,(但是对于刁钻的人耳而言,差异还挺大,比如常用的低配音响会使用D类功放做为功率放大的核心,但是开关阈值的非线性却会导致失真).通常而定功率下THD+N全频段(20Hz~20KHZ)做到1%之下,大功率情况下也要注意绝对值.
如果想看完整测试报告,可以参考文档:
<a href="https://github.com/shichaog/WebRTC-audio-processing/blob/master/AudioPrecision_test.pdf" target="_blank">Audio Precision test report example</a>
这里需要提及的是,一般而言功放芯片都会有一个codec(TI/yamaha以及新唐),这个codec可以对最终出来的音效进行补偿,包括EQ等.<div class="comments-icon"></div></p>
<h3 id="声学性能">声学性能</h3>
<p class="comments-section">声学性能主要是指喇叭和辐射盆震动的理想性,理论上来说我们希望喇叭和辐射盆的往复运动是线性的,也即喇叭和前进的距离和给的功率是$$L=kx$$关系,但是我们知道实际上在输出功率很小或者输出功率超出额定功率的时候,喇叭的声音是失真的,这是因为电能所转换出的磁能再转变成弹簧的机械能时,他们的各自部分并不是线性的关系,一个直观的就是在弹簧拉到一定程度时,所给的力和弹簧拉动的距离所成的比例有所变动,这会导致最终喇叭推动的空气也不是等比的,失真由此产生;在声学性能上会使用激光测试喇叭的震动情况,当然喇叭的测试项也是使用专门的仪器来完成的,此外所选磁性材料的选择也是一门很深的学问,所以关于喇叭的设计通常都是给专门的喇叭公司来设计.
这部分主要是音响工程但也并不意味ASR人员不需要关注,由于喇叭会发出声音,(AEC)的效果好坏也和这部分有关,当然是给到AEC的参考信号和播放出来的越接近越好.<div class="comments-icon"></div></p>
<h2 id="麦克风">麦克风</h2>
<p class="comments-section">对音响有了一个更深的理解后,再来看看麦克风,麦克风对声音的敏感对要比人耳大多的,如果使用上面的喇叭播放DTMF(双音多频)音源,在人耳可以区分的情况下,未必麦克风采集到的信号,经过算法未必可以判决出来.(产线批量生产时测试麦克风和喇叭好坏的一个一个方法).
这里以数字麦克为例,一般的麦克风为了节省硅片面积,采用多阶数的$$\sum-\Delta$$型模数转换,这就需要在硬件上避免频域混跌,这就需要进行重采样,这就引出了第二章的内容(由于语音技术的民用普及,$$\Delta-\sum$$)的模数转换也出现了,这样就可以不需要进行重采样了,需要关注的指标如下:<div class="comments-icon"></div></p>
<table>
<thead>
<tr>
<th>指标</th>
<th></th>
<th>意义</th>
</tr>
</thead>
<tbody>
<tr>
<td>sensitivity</td>
<td>可以探测到最小声压的声音(如-27dBFS)</td>
</tr>
<tr>
<td>SNR</td>
<td>同样要关心麦克风本身引入的噪声对信号质量的音响</td>
</tr>
<tr>
<td>AOP</td>
<td>Acoustic Overload Point,声压过大导致超过10%失真,这影响到产品的使用场景</td>
</tr>
<tr>
<td>Frequency Response</td>
<td>麦克风对不同频率信号在不同声压下放大一致性如何</td>
</tr>
</tbody>
</table>
<p class="comments-section">另外还要看批量产品的动态误差,温湿度影响(ADC器件的温漂)等.由于人声发音范围是20Hz~20KHz,一般麦克风$$\sum-\Delta$$的截止频率设置在20KHz以上,根据奈奎斯特采用定理,如果要不产生频谱混跌,就意味这采样率$$f_s \ge 40KHz$$,为了便于重采样算法,很多系统设计会采用48KHz采样,而后转成16KHz(通常ASR系统声学特征提取是安装16KHz提取的,这是均衡了数据处理量和性能的综合考虑结果).
如上是单颗麦克风的指标,为了便于远场语音识别之类的应用,大多数产品采用多个麦克风组成的阵列做为语音设备的声音获取来源,由于算法的需要,通常会产生出通道一致性的问题,(在雷达系统中常使用自适应通道均衡技术),但是这在麦克风阵列情况下很少采用.
所以麦克风参数的动态范围一定是要参考的,此外麦克风之间的相位一致性也是很重要的.<div class="comments-icon"></div></p>
<ul>
<li>麦克风采集</li>
<li>采样率，截止频率在8KHz，这要求采样率$$f_s \ge 16KHz$$，为了防止频谱混跌，通常采样率大于$$16KHz$$,经过重采样后到$$16KHz$$</li>
<li>为了减小语音失真，通常处理过程不加AGC，可以的化也不加NS（如果服务端有抗噪训练,如果不能处理噪声，ns也是需要的）</li>
<li>避免语音被截幅（AOP要高，$$120dB@1KHz$$），峰值电平在-20~10dBFS为宜</li>
<li>频谱尽量平坦（$$\pm 3dB$$,$$100-8000Hz$$），有两层意义，一个是麦克风频谱要求尽量频谱，一个是声音传播损耗需要预加重来增强。</li>
<li>总谐波失真要小，小于1%（从$$100Hz-8KHz,@90dB SPL$$）</li>
<li>SNR要高（$$\ge 65dB$$为佳），减小ADC器件本身带来的噪声。</li>
<li>采样有效比特数，其影响的是信噪比，大于等于16bit即可</li>
<li>语音传输到服务端，对识别率由好到差（网络带宽由大到小）是：FLAC/LINEAR16， AWR_WB,OGG_OPUS</li>
</ul>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170925150255204.png" alt=""></p>
<blockquote>
<p class="comments-section">图1.1 频谱平坦度实例<div class="comments-icon"></div></p>
</blockquote>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170925150303814.png" alt=""></p>
<blockquote>
<p class="comments-section">图1.2 THD实例<div class="comments-icon"></div></p>
</blockquote>
<h2 id="结构设计">结构设计</h2>
<p class="comments-section">结构设计上会带来震动,产品外观会带来腔体结构影响,<div class="comments-icon"></div></p>
<p class="comments-section"><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/mic.png" alt="">
图1.3 硅麦
上图是一个顶出音的硅麦(此外还有底出音,通常两种开孔的声学特性是有差异的,此外在批量生成波峰焊接时也是有影响的).
一般上图中的麦克并不是裸露在空气中的,而是在产品的外观里面,这样就相当于把麦克风放在腔体里面,麦克风采集到的信号不在是自由场场景了,对于震动情况,可以通常采样橡胶或者悬浮的方式进行,此外,还将喇叭和麦克风放在物理尺寸最大的两极,而对于腔体通常采样紧贴或者设置导音孔(驻波).最终还是要测试多路麦克风在扫频情况下幅频和相频相应的一致性,验证导引孔之类的结构设计的合理性.<div class="comments-icon"></div></p>
<h2 id="结束语">结束语</h2>
<p class="comments-section">为了设计出良好的ASR系统,对于结构和电声特性还是要积累,以便有一个更加全面的认识,当然,如果是做批量生产的产品,仅仅考虑ASR还是不够的,还要考虑供应链的稳定性,生产测试便利性和高效性等<div class="comments-icon"></div></p>
</section>
<h1 id="第二章-重采样原理及实现">第二章 重采样原理及实现</h1>
<p class="comments-section">重采样在麦克风采集和非同源系统中会用到,前文提到,由于硅麦的设计问题以及ASR系统要求频谱不能混跌,引入了要以较高的频率进行采样.
由于github等类似的网上有很多开源的源码,这里以原理为主.<div class="comments-icon"></div></p>
<h2 id="信号重采样">信号重采样</h2>
<p class="comments-section">假设信号$$x(t)$$是连续时间信号,则以$$t=nT<em>s$$为间隔对信号$$x(t)$$采样,采样后信号为$$x(nT_s)$$,$$n$$是整数, $$T_s$$是采样周期,根据奈奎斯特采样定理,当$$x(t)$$是带限信号,且其频带范围在$$\pm \frac{F_s}{2}$$之间,这是采样不会导致频谱混跌,采样率$$F_s=\frac{1}{T_s}$$,设$$x(\omega)$$是$$x(t)$$的傅里叶变换,则有$$X(\omega)=\int</em>{-\infty}^{+\infty}x(t)e^{-j\omega t}dt$$. 则可以假定:<div class="comments-icon"></div></p>
<p class="comments-section">$$X(\omega)=0, \left | \omega  \right | \ge \pi F_s$$<div class="comments-icon"></div></p>
<p class="comments-section">根据香龙定理,由$$x(nT_s)$$重构$$x(t)$$的公式如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat x(t)\triangleq \sum\limits_{n=-\infty}^{\infty} x(nT_s)h_s(t-nT_s)\equiv x(t) \tag{2.1}$$
此处,<div class="comments-icon"></div></p>
<p class="comments-section">$$h_s(t)\triangleq sinc(F_st) \triangleq \frac{sin(\pi F_s t)}{\pi F_s t} \tag{2.2}$$
以下讨论$$x(t)$$在($$F_s^{'}=\frac{1}{T_s^{'}}$$),仅讨论方程2.1是整数倍情况下的$$T_s^{'}$$.
当新的采样率$$F_s{'}$$小于原始采样率$$F_s$$时,低通滤波截止频率必须是新采样率的二分之一,即$$\frac{F_s^{'}}{2}$$.对于理想的低通滤波器,则有:<div class="comments-icon"></div></p>
<p class="comments-section">$$h(t)=min{{1, F_s^{'}/F_s}}sinc{(min{{F_s,F_s^{'}}}t)} \tag{2.3}$$
前面的增益因子保持通带内单位增益.<div class="comments-icon"></div></p>
<p class="comments-section">$$sinc$$函数$$sinc(t)\triangleq \frac{sin(\pi t)}{\pi t}$$的波形如下:
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/sinc.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图2.1 sinc函数波形<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">公式2.1用卷积公式表示为:$$x * h_s(t)$$.
该卷积过程可以看成是sinc函数的平移叠加,每个sinc函数对应于一个采样点,并且幅度被采样点调制过.所有sinc函数加在一起是原始信号.$$sin(z)$$值为零的地方位于整数处(除$$z=0$$).这意味这$$t=nT_s$$时刻(对应于一个采样点),为和的唯一贡献是采样信号$$x(nT_s)$$.<div class="comments-icon"></div></p>
<p class="comments-section">$$sinc$$函数是理想低通滤波器,实际使用时会进行加窗处理.<div class="comments-icon"></div></p>
<h2 id="22音频重采样">2.2音频重采样</h2>
<p class="comments-section">在音频上,常常遇到重采样问题,如将44.1k/48K/32k以及16k之间进行转换,通常这一过程被称为SRC(sample rate converter),一般有两种实现方式,一种是codec芯片自带了SRC功能,一种是采样软件进行SRC,如Android系统,有些手机厂商为了效率或者利用硬件SRC功能在安卓层会使用硬件实现SRC功能,当然在硬件不支持这一特性时,Android也是提供了软件算法来实现,而且根据重采样的质量和资源消耗情况还提供了几种模式可供选择.
采样率转换的基本思想是抽取和内插,从信号角度看,音频重采样就是滤波.滤波函数的窗口大小以及插值函数一旦被确定,其重采样的性能也就确定了.
抽取可能引起频谱混叠,而内插会产生镜频分量.通常在抽取前先加抗混叠滤波器,在内插后加抗镜频滤波器.在音乐识别里所需语音信号采样率由ASR(automatic speech recognition)声学模型输入特征决定的.<div class="comments-icon"></div></p>
<ul>
<li><p class="comments-section">重采样过程<div class="comments-icon"></div></p>
<ol>
<li>设音频信号的原始采样率是L,新的采样率是M,原始信号长度是N,则新采样率下信号的长度K满足如下关系:</li>
</ol>
<p class="comments-section">$$K=(M/L)*N \tag {2.4}$$<div class="comments-icon"></div></p>
<ol>
<li>对每个离散时间值:$$k(1\le k \le K)$$,则实际值$$n_k$$的值为:
$$n_k = (L/M)*K \tag{2.5}$$
$$n_k$$为在原始采样间隔的情况下,要进行插值的位置.</li>
<li>确定两个加权系数的值,利用第二步计算得到的$$n_k$$值,求两个权值.选择恰当的权值才能让线性差值所有插取的值更加接近插值点的理想幅度值.让$$w_1$$和$$w_2$$分别代表两个重采样权值.$$w_1 = n_k -1, w_2 = 1 - w_1$$</li>
<li>根据两个权值,估计插入点的具体幅度值.
$$y(k) = w<em>{1}x(n+1)+w</em>{2}x(n) \tag {2.6}$$</li>
</ol>
<h2 id="23sinc重采样">2.3sinc重采样</h2>
<p class="comments-section">开源语音处理算法speex使用的就是sinc函数,加窗函数是凯撒窗.
以整数因子抽取(麦克风采样率为48KHz,ASR的声学特征提取信号的采样率是16KHz)为例来说明算法的实现过程.设$$x(n)$$是对模拟信号$$x_a(t)$$以奈奎斯特采样率$$F_x$$采样得到的信号,其频谱为$$X(e^{j\omega})$$,则在频率区间$$0 \le |\omega| \le \pi$$(对应的模拟频率是$$|f| \le \frac{F_x}{2}Hz$$),$$X(e^{j\omega})$$是非零的.现在按整数倍D对$$x(n)$$进行抽取.为了避免频谱混跌,必须先对$$x(n)$$进行抗混跌低通滤波,将$$x(n)$$的有效频带限制在折叠频率$$\frac{F_x}{2D}Hz$$以内,等效的数字频率为$$\pi / D$$弧度以内.<div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">数字域频率和模拟域频率关系
数字域和模拟域频率互相转化的公式如下:
$$\omega = \frac{2\pi f}{f_s}$$
通常所说的频率，在没有特别指明的情况下，指的是模拟频率，其单位为赫兹($$Hz$$)，或者为1/秒($$1/s$$)，数学符号用$$f$$来表示。以赫兹表示的模拟频率表示的是每秒时间内信号变化的周期数。如果用单位圆表示的话，如图1所示，旋转一圈表示信号变化一个周期，则模拟频率则指的是每秒时间内信号旋转的圈数。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170925160135508.jpeg" alt="">
数字频率与模拟频率
模拟频率中还有一个概念是模拟角频率$$\Omega$$，其单位是弧度/秒（$$rad/s$$）。从单位圆的角度看，模拟频率是每秒时间内信号旋转的圈数，每一圈的角度变化数为$$2\pi$$。很显然，旋转$$f$$圈对应着$$2\pi f$$的弧度。即：$$\Omega=2\pi <em>f(rad/s)$$
数字信号大多是从模拟信号采样而得，采样频率通常用$$f_s$$表示。数字频率更准确的叫法应该是归一化数字角频率，其单位为弧度（$$rad$$）,数字符号常用$$\omega$$表示。即:
$$\omega=\frac{2\pi </em>f}{f_s} \tag {2}$$
其物理意义是相邻两个采样点之间所变化的弧度数，如上图所示。
有了上述两个公式，可以在模拟频率与数字频率之间随意切换。假定有一个正弦信号$$x[n]$$，其频率$$f=100Hz$$，幅度为$$A$$，初始相位为$$0$$，则这个信号公式可以表示为：
$$x(t)=A<em>\sin(2</em>\pi <em>100</em>t)$$
用采样频率$$f_s=500Hz$$对其进行采样，得到的数字信号$$x[n]$$为：
$$x[n]=A<em>\sin(2</em>\pi <em>100</em>n/f_s)=A<em>\sin(0.4</em>\pi *n)$$
很明显，这个数字信号的频率为$$0.4\pi$$
$$\omega = \Omega \cdot T =\Omega/f_s$$
由上述讨论可知，对应两个数字频率完全相同的信号，其模拟频率未必相同，因为这里还要考虑采样频率.<div class="comments-icon"></div></p>
</blockquote>
</li>
</ul>
<p class="comments-section">对$$x(n)$$按整数倍因子$$D$$进行抽取,得到信号$$y(m)$$.其原理框图如下:
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/down_sample.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图2.2 整数因子D抽取原理框图<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">理想情况下,抗混叠低通滤波器$$h_D(n)$$的频率响应:<div class="comments-icon"></div></p>
<p class="comments-section">$$H_D(e^{j\omega}) = \left{\begin{matrix}
 1&amp; |\omega| &lt; \pi/D\ 
 0&amp; \pi /D \le |\omega| \le \pi
\end{matrix}\right. \tag{2.7}$$
经过抗混叠低通滤波器后输出为:<div class="comments-icon"></div></p>
<p class="comments-section">$$v(n) = h(n) * x(n) = \sum \limits_{k=0}^\infty h_D(k)x(m-k) \tag{2.8}$$
如果$$h_D(n)$$为因果稳定系统,则式2.8中的卷积求和从0开始是成立的,按照整数被因子D对$$v(n)$$抽取后得到:<div class="comments-icon"></div></p>
<p class="comments-section">$$y(m)=v(mD)=\sum \limits_{k=0}^\infty h_D(k)x(Dm - k) \tag {2.9}$$<div class="comments-icon"></div></p>
<ul>
<li>直接型FIR滤波器实现
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/dir_fir_down.png" alt=""><blockquote>
<p class="comments-section">图2.3采样率转换系统的直接型FIR滤波器结构<div class="comments-icon"></div></p>
</blockquote>
</li>
</ul>
<p class="comments-section">图2.3中的结构就是2.9所表示的滤波过程,在经过抽取器D抽取相应的采样点,在$$I$$表示的插值过程中,会牵涉到在$$x(n)$$相邻样本值之间插入$$I-1$$个零值,如果$$I$$值比较大,则进入FIR滤波器的信号大部分为零,因此乘法运算的结果大部分也是零,即多数乘法是无效的,因此这种运算效率是比较低的.<div class="comments-icon"></div></p>
<ul>
<li>整数因子D抽取直接型FIR滤波器结构
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/D_FIR.png" alt=""><blockquote>
<p class="comments-section">图2.4整数因子D抽取系统的直接型FIR滤波器<div class="comments-icon"></div></p>
</blockquote>
</li>
</ul>
<p class="comments-section">整数因子D抽取系统的直接型FIR滤波器结构如图2.4a所示,该结构中,FIR滤波器以最高采样率$$F_x$$运行,但其输出的每D个样值中抽取一个做为最终输出,丢弃$$D-1$$个样值,这样效率较低.
高效的FIR滤波器将抽取操作D嵌入FIR滤波器结构中,图2.5b所示,a的抽取器在$$n=Dm$$时刻开通,选通FIR滤波器的每个输出做为抽取系统输出序列的一个样值$$y(m)$$.<div class="comments-icon"></div></p>
<p class="comments-section">$$y(m) = \sum \linmits_{k=0}^{M-1} h(k)x(Dm-k) \tag {2.10}$$
b的抽取器在$$n=Dm$$时刻同时开通,选通FIR滤波器输入信号$$x(n)$$的一组延迟:<div class="comments-icon"></div></p>
<p class="comments-section">$$x(Dm), x(Dm-1), x(Dm-2),...,x(Dm-M+1)$$,再进行乘法,加法运算,得到抽取系统输出序列的一个样值,$$y(m) = \sum \limits_{k=0}^{K-1} h(k) x(Dm-K)$$,b的运算量仅是a的D分之一.<div class="comments-icon"></div></p>
<ul>
<li><p class="comments-section">整数因子I内插系统直接型FIR滤波器结构
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/up_Dir_FIR.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图2.5整数因子I内插系统直接型FIR滤波器
根据图2.5给出的结构,需要FIR滤波器以$$IF_x$$速度运行,该效率较低,较高实现效率如下1.6b.
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/up_high_FIR.png" alt=""> 
图2.6整数因子I内插系统的高效实现<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">*多相滤波结构
这种结构是工程实现中用的最多的一种结构,不仅仅是重采样用,在其它需要滤波的场合,使用该滤波器的频次都是比较高的.尤其是在ASIC电路设计上,可以使用几百KB大小存储空间的的低端CPLD实现多路麦克风数据的重采样,在一些集成电路中,也开始集成了这个硬件重采样,它们的滤波结构大部分也是采用这种结构.
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/polyphase_fir.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图2.7多相滤波结构<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">图2.7b的实现结构使用较短的多相滤波器组来实现内插功能,如果滤波器的总长度为$$M=NI$$,则多相滤波器组有$$I$$个长度为$$N=M/I$$的短滤波器构成,且$$I$$个短滤波器轮流分时工作,所以称之为多相滤波器.整数因子I内插系统直接型FIR滤波器的输出$$y(m) = h(m) <em> v(m)$$.零值内插器的输出序列$$v(m)$$是在输入序列$$x(n)$$的两个相邻样值之间插入$$I-1$$个零样值得到,因此$$v(m)$$进入FIR滤波器的M个样值中只有$$N=M/I$$个非零值.即在任意时刻m,计算$$y(m) = h(m) </em> v(m)$$时只有N个非零值与$$h(m)$$中的N个系数相乘.
$$v(m) =\left{\begin{matrix}
x(m/I)&amp; m=0, \pm I, \pm 2, \pm 3\ 
0&amp; other 
\end{matrix}\right. \tag {2.11}$$
所以$$有(m) = \sum \limits<em>{n=0}^{M-1} h(n)v(m-n) = \sum \limits</em>{n=0}^{N-1} h(nI)x(m-n)$$,当$$m=jI+k, k=0,1,2..., I-1,j=0,1...$$时,有:
$$y(m) = \sum \limits<em>{n=0}^{M-1}h(n)v(m-n) = \sum \limits</em>{n=0}^{N-1}h(k+nI)x(m-n) \tag {2.12}$$
上时中$$h(k+nI)$$看做长度$$N=M/I$$的子滤波器的单位脉冲响应,并用$$p_k(n)$$表示,则:<div class="comments-icon"></div></p>
</li>
</ul>
<p class="comments-section">$$p_k(n) = h(k+nI), k=0,1,...,I-1, n=0,1,...,N-1$$<div class="comments-icon"></div></p>
<p class="comments-section">这样,从$$m=0$$开始,整数因子$$I$$内插系统的输出序列$$y(m)$$计算如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$y(m) = \sum \limits_{n=0}^{M-1} h(n)v(m-n) = p_k(n) * x(n) \tag {2.12}$$<div class="comments-icon"></div></p>
<p class="comments-section">当$$m=jI+k$$从零开始增大时,k从0开始以I为周期循环取值,j表示循环周期数.所以式2.12对应的多相滤波器的结构如图2.7所示.输出序列$$y(m)$$就是序列$$y(m)$$就是从$$k=0$$开始,依次循环选取I个子滤波器输出所形成的序列.
多相滤波器的单位脉冲响应为:<div class="comments-icon"></div></p>
<p class="comments-section">$$p_k(n) = h(k+nD), k=0,1,2,...,D-1; n=0,1,2,...,N-1 \tag {2.13}$$
式中,$$N$$为$$p_k(n)$$的长度.一般选择抗混叠FIR滤波器总长度$$M=DN$$, $$N=M/D$$.电子开关以速率$$F_x$$逆时针旋转,从子滤波器$$p_0(n)$$在$$m=0$$时刻开始,并输出$$y(0)$$;然后电子开关以速率$$F_x$$逆时针旋转一周,即每次转到子滤波器$$p_0(n)$$时,输出端就以速率$$F_y = F_x/D$$送出一个$$y(m)$$样值.
从频域角度看,线性重采样适合应用于下采样,有些场景需要先升采样,然后再降采样以提高精度,如将44.1kHz的蓝牙语音信号转换成16KHz的信号,为了减少SRC带来的失真,通常会将44.1KHz信号先升采样到和16KHz的一个公倍数,然后再进行整数倍抽取.<div class="comments-icon"></div></p>
<h2 id="14matlab实现">1.4MATLAB实现</h2>
<p class="comments-section">开源的c/c++重采样代码很多,这里以MATLAB结合绘图,以便有一个直观的对这一过程有个了解.下图是将48KHz信号重采样到16KHz,并绘制了它们的频谱如下:
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/matlab_dow.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图2.8 48KHz人声重采样到16KHz人声<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">上图是同一段语音48KHz和16KHz绘制的重叠图,可以看到在包络上还是有些差异,这些差异是重采样的影响.
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/downsample_3.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图2.9 48KHz和16KHz采样率下的信号时域和频域波形<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">图2.9的第一行是原始48KHz信号的时域和频域波形图,其频域波形在24KHz截止,第二行是48KHz采样率下抽取后信号的时域和频域波形,对于第一行和第二行来看,频谱差异还是比较容易看出来的,第三行是16KHz采样率下绘制的重采样后的时域和频域波形,其截止频率在8KHz,和第二行的信号频率差异性较小.上述示例的MATKAB代码如下:<div class="comments-icon"></div></p>
<pre><code class="lang-MATLAB">clear; close all; clc;
<span class="hljs-comment">%read sound wave </span>
[x,fs]=audioread(<span class="hljs-string">'48k_sound.wav'</span>);
<span class="hljs-comment">% sound(x,48000); </span>
<span class="hljs-comment">%sound(y,fs,nbits);</span>
N=<span class="hljs-built_in">length</span>(x);
tx=(<span class="hljs-number">0</span>:N<span class="hljs-number">-1</span>)/fs;<span class="hljs-comment">%calc the time that correspond to original signal</span>
xf=fft(x);
fx=(<span class="hljs-number">0</span>:N/<span class="hljs-number">2</span>)*fs/N;
figure(<span class="hljs-number">1</span>);
subplot(<span class="hljs-number">321</span>);plot(tx,x);xlabel(<span class="hljs-string">'Time'</span>);title(<span class="hljs-string">'48k original signal'</span>);
subplot(<span class="hljs-number">322</span>);plot(fx,<span class="hljs-built_in">abs</span>(xf(<span class="hljs-number">1</span>:N/<span class="hljs-number">2</span>+<span class="hljs-number">1</span>)));xlabel(<span class="hljs-string">'Freq '</span>);title(<span class="hljs-string">'48k original signal spectrue'</span>);
audiowrite(<span class="hljs-string">'48k_sound.wav'</span>,x,<span class="hljs-number">48000</span>);

<span class="hljs-comment">%decimate signal</span>
k=<span class="hljs-number">1</span>:N/<span class="hljs-number">3</span>;<span class="hljs-comment">% Number to decimate 48k to 16k</span>

<span class="hljs-comment">%filter design kasier</span>
fp=<span class="hljs-number">8000</span>;fc=<span class="hljs-number">12000</span>;as=<span class="hljs-number">100</span>;ap=<span class="hljs-number">1</span>;FS=<span class="hljs-number">48000</span>;
wc=<span class="hljs-number">2</span>*fc/FS; wp=<span class="hljs-number">2</span>*fp/FS;<span class="hljs-comment">%kaiser window</span>
N=<span class="hljs-built_in">ceil</span>((as<span class="hljs-number">-7.95</span>)/(<span class="hljs-number">14.36</span>*(wc-wp)/<span class="hljs-number">2</span>))+<span class="hljs-number">1</span>;
<span class="hljs-built_in">beta</span>=<span class="hljs-number">0.1102</span>*(as<span class="hljs-number">-8.7</span>);
win=kaiser(N+<span class="hljs-number">1</span>, <span class="hljs-built_in">beta</span>);
wvtool(win);
B=fir1(N, wc,win);
freqz(B,<span class="hljs-number">1</span>,<span class="hljs-number">512</span>,fs);
y=filter(B,<span class="hljs-number">1</span>,x)
y=downsample(y,<span class="hljs-number">3</span>);
<span class="hljs-comment">%实际上这里是每隔三个点取一个点，即先低通滤波再取点，和紧接着的下面的三行结果是一样的</span>
<span class="hljs-comment">% y=x(3*k);% 48k---&gt;16k,and data store in y</span>
<span class="hljs-comment">%y=decimate(x,1,3);</span>
<span class="hljs-comment">% y=resample(x,1,3);</span>
M=<span class="hljs-built_in">length</span>(y);<span class="hljs-comment">% M is the length of y </span>
<span class="hljs-comment">%analysis y under the frequency of 48k</span>
ty=(<span class="hljs-number">0</span>:M<span class="hljs-number">-1</span>)/fs; <span class="hljs-comment">%fs equ 48k</span>
subplot(<span class="hljs-number">323</span>);plot(ty,y);xlabel(<span class="hljs-string">'Time of decimate'</span>);title(<span class="hljs-string">'48k decimate by 3'</span>);
yf=fft(y);<span class="hljs-comment">% fft of 48k downsample by 3</span>
fy=(<span class="hljs-number">0</span>:M/<span class="hljs-number">2</span>)*fs/M;
subplot(<span class="hljs-number">324</span>);plot(fy,<span class="hljs-built_in">abs</span>(yf(<span class="hljs-number">1</span>:M/<span class="hljs-number">2</span>+<span class="hljs-number">1</span>)));xlabel(<span class="hljs-string">'Freq'</span>);title(<span class="hljs-string">'48k after decimate by 3'</span>);

<span class="hljs-comment">%decimate data under sample 16k</span>
tz=(<span class="hljs-number">0</span>:M<span class="hljs-number">-1</span>)/(fs/<span class="hljs-number">3</span>);
subplot(<span class="hljs-number">325</span>);plot(tz,y);xlabel(<span class="hljs-string">'Time'</span>);title(<span class="hljs-string">'48k decimate by 3'</span>);
fz=(<span class="hljs-number">0</span>:M/<span class="hljs-number">2</span>)*(fs/<span class="hljs-number">3</span>)/M;
subplot(<span class="hljs-number">326</span>);plot(fz,<span class="hljs-built_in">abs</span>(yf(<span class="hljs-number">1</span>:M/<span class="hljs-number">2</span>+<span class="hljs-number">1</span>)));xlabel(<span class="hljs-string">'Freq '</span>);title(<span class="hljs-string">'16k signal spectrue'</span>);

sound(y,<span class="hljs-number">16000</span>); 

audiowrite(<span class="hljs-string">'16k_sound.wav'</span>,y,<span class="hljs-number">16000</span>);

<span class="hljs-comment">% %ellipord</span>
<span class="hljs-comment">% fp=8000;fc=12000;as=100;ap=1;fs=48000; </span>
<span class="hljs-comment">% wc=2*fc/fs;wp=2*fp/fs; </span>
<span class="hljs-comment">% [n,wn]=ellipord(wp,wc,ap,as);%</span>
<span class="hljs-comment">% [b,a]=ellip(n,ap,as,wn); </span>
<span class="hljs-comment">% freqz(b,a,512,fs);</span>
</code></pre>
<h2 id="重采样性能评估">重采样性能评估</h2>
<p class="comments-section">音频信号的好坏常采用信噪比来评估,同样可以根据重采样后音频信号的信噪比来评估重采样的质量.信噪比的定义如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$SNR =   \left { 10\log<em>{10} \frac{\sum \limits</em>{n=0}^Ms(n)^{2}}{\sum \limits_{n=0}^{M}(s(n)^2 - \hat s(n)^2)} \right } \tag {2.13}$$<div class="comments-icon"></div></p>
<p class="comments-section">式中,$$s(n)$$为原始信号,$$\hat s(n)$$是转换后的同频率音频信号,M为音频信号的长度.信噪比越大意味着总体上原始音频信号和重采样后的信号之间的差距较小.接近的程度越高.在功放的测试中就有信噪比这么一项.
由于语音信号是短时平稳(10ms~30ms),可以将语音信号分帧计算各帧的信噪比,平均后得到信噪比.<div class="comments-icon"></div></p>
<p class="comments-section">$$SNR =   \frac{1}{N} \sum \limits<em>{m=0}^{M-1}\left { 10\log</em>{10} \frac{\sum \limits<em>{n=0}^M x(n)^{2}}{\sum \limits</em>{n=0}^{M}(x(n)^2 - \hat x(n)^2)} \right } \tag {2.14}$$<div class="comments-icon"></div></p>
<p class="comments-section">由于ASR对频谱比较敏感,这和特征提取有关,还需要知道信号频域的失真情况.这通过对原始语音和resample后的语音做STFT后,插值后,根据频谱绘制每一个频点的差异,这样可以看到resample对个别频点的影响是否可以接受的.<div class="comments-icon"></div></p>
<h2 id="结束语">结束语</h2>
<p class="comments-section">本章主要给出来重采样的原理和其理论实现方法,并讲述了在实际的智能设备中使用重采样的场景,并给出了重采样的高效实现方法,这对RTL级(register translation level)是有益的,对做ASIC是可以用上的,最后也给出了评判重采样后信号质量好坏的方法.<div class="comments-icon"></div></p>
<h1 id="自动回声消除aec原理">自动回声消除(AEC)原理</h1>
<p class="comments-section">回声消除在通话中常见,在自然语言交互系统中算是一个新出现的强需求.<div class="comments-icon"></div></p>
<h2 id="回声消除原理">回声消除原理</h2>
<p class="comments-section">回声消除的基本原理是使用一个自适应滤波器对未知的回声信道$$\omega$$进行参数辨识，根据扬声器信号与产生的多路回声的相关性为基础，建立远端信号模型，模拟回声路径，通过自适应算法调整，使其冲击响应和真实回声路径相逼近。然后将麦克风接收到的信号减去估计值，即可实现回声消除功能。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/aec_1.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图3.1回声消除原理<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">$$echo = x* \omega \tag {3.1}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$d = s + echo \tag {3.2}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat y = x * \hat \omega \tag{3.3}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$e = d - \hat y \tag {3.4}$$<div class="comments-icon"></div></p>
<p class="comments-section">式中，$$\omega$$是回声通道的时域冲击响应函数；$$x$$是远端语音；echo是所得回声；s是近端说话人语音，d为麦克风采集到的信号；$$\hat y$$为对回声信号的估计值；e为误差。在电话、视频会议中这里的x通信另一端的语音信号，而在机器语音识别中，这里的x则指机器自身发出的声音。<div class="comments-icon"></div></p>
<p class="comments-section">为了消除较长时间的回声，需要FIR滤波器的阶数尽量的大。时域计算诸多不便，使用频域分块自适应滤波算法。<div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">FFT/IFFT
循环卷积和线性卷积的关系;重叠保留法
功率谱密度
互相关
LMS/NLMS自适应算法<div class="comments-icon"></div></p>
</blockquote>
<h2 id="维纳滤波">维纳滤波</h2>
<p class="comments-section">均放误差(MSE,Mean Square Error),对于离散时间系统,可定义期待响应$$d_k$$为一个希望自适应系统的输出$$y_k$$与之相接近的信号，k为采样时刻。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/aec_2.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图3.2 MSE自适应系统<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">根据图3.2, 可以求得误差信号:<div class="comments-icon"></div></p>
<p class="comments-section">$$\varepsilon_k = d_k - y_k \tag {3.5}$$
自适应线性组合器输出:<div class="comments-icon"></div></p>
<p class="comments-section">$$y_k = W_k^TX_k \tag {3.6}$$
其中:<div class="comments-icon"></div></p>
<p class="comments-section">$$X<em>k = \begin{bmatrix}
x</em>{0k}&amp;x<em>{1k}&amp;\cdots &amp; x</em>{Lk}
\end{bmatrix},
W<em>k = \begin{bmatrix}
w</em>{0k}&amp;w<em>{1k}&amp;\cdots &amp; w</em>{Lk}
\end{bmatrix}<div class="comments-icon"></div></p>
<p class="comments-section">$$
分别为自适应系统在k时刻的输入信号向量和权向量，系统的均方误差为：<div class="comments-icon"></div></p>
<p class="comments-section">$$E(|\varepsilon_k|^2) = E[(d_k - y_k)^{<em>}(d_k-y_k)] = 
E(|d_k|^2)+\mathbf W_k^H E[\mathbf X_k^</em> \mathbf X_k^<em>]\mathbf W_k - 2Re{\mathbf W_k^T E[d_k^</em> \mathbf X_k]} \tag {3.7}<div class="comments-icon"></div></p>
<p class="comments-section">$$
令:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf R = E[\mathbf X^<em> \mathbf X^T] =\begin{bmatrix}
 E[x_0^</em>x_0]&amp; E[x_0^<em>x_1] &amp; \cdots &amp; E[x_0^</em>x_L]\ 
 E[x_1^<em>x_0]&amp; E[x_1^</em>x_1] &amp; \cdots &amp; E[x_1^<em>x_L]\ 
 \vdots&amp; \vdots &amp; \ddots &amp; \vdots \ 
 E[x_L^</em>x_0]&amp; E[x_L^<em>x_1] &amp; \cdots &amp; E[x_L^</em>x_L] 
\end{bmatrix} \tag {3.8}<div class="comments-icon"></div></p>
<p class="comments-section">$$
定义期待响应和输入信号之间的互相关向量为:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf P = E[d^<em>\mathbf X] = \begin{bmatrix}
d^</em>x_0\ 
\vdots\ 
d^*x_L
\end{bmatrix} \tag {3.9}$$
将式3.9简化成下式:<div class="comments-icon"></div></p>
<p class="comments-section">$$\xi(\mathbf w) = E(|d_k|^2)+\mathbf W_k^H \mathbf R \mathbf W_k - 2Re{\mathbf W_k^T \mathbf P} \tag{3.10}$$
理想情况下$$E(|\varepsilon|^2)$$等于零,这是估计值等于观测值,误差越小,估计值和实际值之间的偏差越小.
对式3.10求偏导数,得:<div class="comments-icon"></div></p>
<p class="comments-section">$$\nabla = \frac{\partial}{\partial \mathbf W}[\xi(\mathbf W)] = 2 \mathbf R \mathbf W -2 \mathbf P^* \tag {3.11}$$
最佳权向量处的梯度等于0,于是:<div class="comments-icon"></div></p>
<p class="comments-section">$$\nabla = 2 \mathbf R \mathbf W<em>{opt} - 2 \mathbf P^* = 0 \tag {3.12}$$
最小均放误差输出情况下的最佳权向量$$\mathbf W</em>{opt}$$满足维纳-霍夫方程:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf W_{opt} = \mathbf R^{-1} \mathbf P^* \tag{3.13}$$
值得指出的是MSE准则应用的地方还是很多,在后面阵列波束形成中的一个实例就会用到这一准则.<div class="comments-icon"></div></p>
<h3 id="1-lms算法">1. LMS算法</h3>
<p class="comments-section">$$\varepsilon_k = d_k - \mathbf X_k^T \mathbf W_k \tag {3.14}$$
式中,$$\mathbf X_k$$为输入样本向量,使用单次采样数据$$|\varepsilon_k|^2$$来代替均放误差$$\xi_k$$,这样梯度估计可表示为如下形式:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat \nabla_k = \frac{\partial}{\partial \mathbf W_k}|\varepsilon_k|^2 = \frac{\partial}{\partial \mathbf w_k}[|d_k|^2 + \mathbf W_k^H \mathbf X_k^<em> \mathbf X_k^T \mathbf W_k - 2d \mathbf X_k^</em>] = -2\varepsilon_k \mathbf X_k^* \tag {3.15}$$
基于最速下降法的权向量迭代如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf W<em>{k+1} = \mathbf W - \mu \hat \nabla_k = \mathbf W_k + 2 \mu \varepsilon_k \mathbf X_k^* \tag {3.16}$$
其中$$\mu$$是步长因子,$$0 &lt; \mu &lt; \frac {1}{\lambda</em>{max}}$$, $$\lambda<em>{max}$$是$$\mathbf R</em>{xx}$$的最大特征值.$$\mathbf W(k)$$收敛于由比值决定，该比值$$d=\frac {\lambda<em>{max}}{\lambda</em>{min}}$$决定,他的比值叫做谱动态范围。大的d值意味着较长的时间才能收敛到最佳权值。
该算法用在语音增强的加性噪声消除功能上时，其工程实践并不完全按照式3.14意义来实现。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/AEC_4.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">LMS算法在语音增强中的使用<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">在语音增强中,其目的是获得纯净的语音信号$$s(n)$$即上图中的最后输出信号，输入信号有两种，一种是带噪的语音信号$$s(n)+x(n)$$，另一种是只有噪声的输入$$x(n)$$，在没有人说话的情况下的输入信号，就仅仅是噪声输入. 这里要使得噪声估计$$\hat x(n)$$非常接近$$x(n)$$,对于$$\varepsilon = \sum \limits_n s(n)+x(n) -\hat x(n)$$,这时如果$$\varepsilon^2$$最小，则可以估计出的$$\varepsilon$$最接近$$s(n)$$.述过程可以概述如下:<div class="comments-icon"></div></p>
<ol>
<li>首先获取到噪声输入$$x(n)$$,并存储下来，以64或者128点为总长度不断刷新存储噪声输入$$x(n)+s(n)$$.</li>
<li>采集带噪声的语音信号.</li>
<li>用采集带噪语音信号减去估计到的噪声信号s(n)+x(n)-\hat x(n).</li>
<li>用3的输出作为误差，调节噪声权向量W.
MATLAB实现具体包括如下三个部分：</li>
</ol>
<pre><code>% Loop over input vector
for ii = 1:length(signal_with_noise)
% Update buffer
%输入噪声估计
noise_buf = obj.update_buf(noise_buf, noise(ii));
% Filter this sample with current coefficient values
%通过权向量估计
filter_output = obj.data_filter(coefs, noise_buf);
% Compute error,相减得到
err = signal_with_noise(ii) - filter_output; 
% Update coefficients
coefs = obj.update_coefs(coefs, noise_buf, obj.filter_params.step_size, 
%用调节权向量
obj.filter_params.leakage, err); 
% Build output vector
%存储输入信号的估计值
dout(ii) = err;
</code></pre><h3 id="nlms算法">NLMS算法</h3>
<p class="comments-section">当输入信号幅值较大时,会遇到梯度噪声方法的问题,使得能量低的信号算法收敛速度比较慢,将输入信号按照自身的平均能量进行归一化处理,既可以得到归一化的LMS算法(NLMS),鲁棒性比较好.设输入带噪信号可以表示为:$$x(n)$$,其迭代算法NLMS公式如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf W<em>{n+1} = \mathbf W_n - \mu \hat \nabla_n = \mathbf W_n + \frac{\mu}{N} \frac{e(n)\mathbf x(n)}{\hat \delta_x^2(n)}\tag {3.17}$$
其中,$$\hat \delta_x^2(n) = \frac{1}{N} \sum \limits</em>{n=0}^{N-1}x^2(n-k)$$,其中,N是噪声消除器和回波抵消器的长度.(常取512,1024); $$\mu$$是步长因子.当$$\hat \delta_x^2(k)$$较小时, $$\frac{\mu}{\hat \delta_x^2(n)}$$的值可能较大,这是迭代算法变成如下形式:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf W<em>{n+1} = \mathbf W_n - \mu \hat \nabla_n = \mathbf W_n + \frac{\mu}{N} \frac{e(n) \mathbf x(n)}{\sigma + \hat \sigma_x^2(n)} \tag{3.18}$$
其计算过程如下:
参数:M是滤波器抽头系数(阶数), $$\mu$$是自适应常数,$$0 &lt; \mu &lt; 2 \frac{E[|\mathbf x(n)|^2] E[|\mathbf \varepsilon(n)|^2]}{E[|\mathbf e(n)|^2]}$$, 其中$$E[|\mathbf \varepsilon(n)|^2] = E[|\mathbf W</em>{opt} - \hat W(n)|^2]$$,是权向量均方偏差,$$\mathbf W_{opt}$$是最优维纳解,$$\hat W(n)$$是第n次迭代中得到的估计值.$$E[|\mathbf x(n)|^2]$$是带噪输入语音信号的功率,$$E[|\mathbf e(n)|^2]$$是误差信号功率.<div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">计算过程
1.初始化过程
如果知道抽头权向量$$\hat W(n)$$的先验知识,则用其来初始化$$\hat W(0)$$,否则令$$\hat W(0) = \vec 0$$.
2.数据按帧处理
A)给定的$$\mathbf x(n)$$第n时刻$$MX1$$抽头输入向量,$$d(n)$$是第n时刻的期望响应.
B)要计算的:$$\hat W(n+1)$$是第n+1步抽头权向量估计
计算
对$$n=0,1,2,...$$计算:<div class="comments-icon"></div></p>
<p class="comments-section">$$e(n) = d(n) - \mathbf {\hat W^H(n)} \mathbf X(n) \tag {3.19}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf W<em>{n+1} = \mathbf W</em>{n} - \mu \hat \nabla_n =  \mathbf W_n + \frac{\mu}{N} \frac{e(n)\mathbf X(n)}{\hat \sigma_x^2(n)} \tag {3.20}$$<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">此外还有NLMS变种的各种方法,如SE-LMS(signed-error)LMS, SD-LMS(signal-dependent LMS),LLMS(Leaky LMS),  LNLMS(Leaky NLMS).<div class="comments-icon"></div></p>
<h2 id="块自适应滤波">块自适应滤波</h2>
<p class="comments-section">对参考信号x分段并做FFT变换，分别对各段数据做频域滤波，累加后做FFT反变换，并只取后L(L是原始信号的分段后的长度)点为有效的线性卷积结果，得到的是估计信号，将估计信号从回声信号中去除，得残差信号。计算子带步长，调整各段滤波器系数。这一过程表示如下图。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/aec_5.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图3.4块自适应滤波器<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">设n时刻输入序列$$x(n)$$如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$
\mathbf X(n) = \begin{bmatrix}
 x(n)&amp; x(n-1) &amp; \cdots &amp; x(n-M+1)
\end{bmatrix}^T
\tag{3.21}<div class="comments-icon"></div></p>
<p class="comments-section">$$
对应于长度为M的FIR滤波器在n时刻的抽头权向量为：<div class="comments-icon"></div></p>
<p class="comments-section">$$
\hat {\mathbf W}(n) = \begin{bmatrix}
 \hat {\mathbf w}<em>0(n)&amp; \hat {\mathbf w}_1(n-1) &amp; \cdots &amp; \hat {\mathbf w}</em>{M-1}(n)
\end{bmatrix}^T
\tag{3.22}<div class="comments-icon"></div></p>
<p class="comments-section">$$
根据FIR滤波器原理:<div class="comments-icon"></div></p>
<p class="comments-section">$$y(n) = x(n) \hat w<em>0(n) + x(n-1) \hat w_1(n) + \cdots </em> x(n-M+1)\hat w_{M-1}(n) \tag {3.23}$$
用向量表示如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$y(n) = \mathbf X(n)^T \hat {\mathbf W}(n) \tag {3,24}$$
下面对$$x(n)$$进行分块,设k表示块下标,它与原始样值时间n的关系为:<div class="comments-icon"></div></p>
<p class="comments-section">$$n = kL+i, i=0,1,\cdots, L-1; k = 1,2,\cdots$$
其中L是块的长度. 第k块的数据为$${\mathbf X(kL+i)}_{i=0}^{L-1}$$,其矩阵表示形式如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf A^T(k) = \begin{bmatrix}
 x(kL)&amp; x(kL+1) &amp; \cdots &amp; x(kL+L-1)
\end{bmatrix} \tag {3.25}$$
将滤波器对输入块$$A(k)$$的响应表示如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$y(kL+i) = \hat {\mathbf W}^T(k)\mathbf A(k) = 
\sum \limits_{j=0}^{M-1}\hat w_j(k)x(kL+i-j), i=0, 1,\cdots, L-1 \tag {3.26}<div class="comments-icon"></div></p>
<p class="comments-section">$$
设$$d(kL+i)$$表示期望信号,误差信号表示如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$e(kL+i) = d(kL+i) - y(kL+i) \tag {3.27}$$
考虑滤波器长度M=3，块长度L=3,其三个相邻的数据块是k-1，k， k+1，则第k-1块滤波结果如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$
\begin{vmatrix}
y(3k-3)\ 
y(3k-2)\ 
y(3k-1)
\end{vmatrix} = 
\begin{vmatrix}
x(3k-3) &amp; x(3k-4) &amp; x(3k-5)\ 
x(3k-2) &amp; x(3k-3) &amp; x(3k-4)\ 
x(3k-1) &amp; x(3k-2) &amp; x(3k-3)
\end{vmatrix}
\begin{vmatrix}
w_0(k-1)\ 
w_1(k-1)\ 
w_2(k-1)
\end{vmatrix} \tag{3.28}<div class="comments-icon"></div></p>
<p class="comments-section">$$
则第k块滤波结果如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$
\begin{vmatrix}
y(3k)\ 
y(3k+1)\ 
y(3k+2)
\end{vmatrix} = 
\begin{vmatrix}
x(3k) &amp; x(3k-1) &amp; x(3k-2)\ 
x(3k+1) &amp; x(3k) &amp; x(3k-1)\ 
x(3k+2) &amp; x(3k+1) &amp; x(3k)
\end{vmatrix}
\begin{vmatrix}
w_0(k)\ 
w_1(k)\ 
w_2(k)
\end{vmatrix} \tag{3.29}<div class="comments-icon"></div></p>
<p class="comments-section">$$
则第k+1块滤波结果如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$
\begin{vmatrix}
y(3k+3)\ 
y(3k+4)\ 
y(3k+5)
\end{vmatrix} = 
\begin{vmatrix}
x(3k+3) &amp; x(3k+2) &amp; x(3k+1)\ 
x(3k+4) &amp; x(3k+3) &amp; x(3k+2)\ 
x(3k+5) &amp; x(3k+4) &amp; x(3k+3)
\end{vmatrix}
\begin{vmatrix}
w_0(k+1)\ 
w_1(k+1)\ 
w_2(k+1)
\end{vmatrix} \tag{3.29}<div class="comments-icon"></div></p>
<p class="comments-section">$$
上面的数据矩阵是托伯利兹矩阵，主对角线元素都相同。<div class="comments-icon"></div></p>
<ul>
<li>权向量调整公式如下</li>
</ul>
<p class="comments-section">（权向量的调整）=（步长参数）<em>（抽头输入向量）</em>（误差信号）
因为在块LMS算法中误差信号随抽样速率而变，对于每一个数据块，我们有不同的用于自适应过程的误差信号值。因此，每一个块的抽头权向量更新公式如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat w(k+1) = \hat w(k) + \mu \sum \limits_{i=0}^{L-1}x(kL+i)e(kL+i) \tag {3.30}$$
其梯度向量的估计如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat \nabla(k) = - \frac{2}{L}\sum \limits_{i=0}^{L-1}x(kL+i)e(kL+i) \tag {3.31}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat \nabla(k)$$的无偏估计如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat w(k+1) = \hat w(k) - \frac{1}{2} \mu_B \hat \nabla(k) \tag {3.32}$$<div class="comments-icon"></div></p>
<ul>
<li><p class="comments-section">块LMS算法的收敛性
由于时间平均的缘故，它具有估计精度随快长度增加而大幅提高的特性。然而，长度的增加会导致其收敛速度进一步减慢。后文的快速LMS算法加速了这一过程。<div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">平均时长数
$$\tau<em>{mse,av} = \frac{1}{2\mu_B \lambda</em>{av}} \tag{3.33}$$
其中,$$\tau<em>{av}$$是输入自相关矩阵$$\mathbf R = E[\mathbf X(n) \mathbf X(n)^T]$$
上式中为了使零阶公式成立,$$\mu_B$$必须小鱼$$\frac{1}{\lambda</em>{max}}$$,其中$$\lambda_{max}$$是相关矩阵的最大特征值.
失调
$$\upsilon = \frac{\mu_B}{2L}tr[\mathbf R] \tag {3.34}$$
其中$$tr[\mathbf R]$$是相关矩阵的迹.<div class="comments-icon"></div></p>
</blockquote>
</li>
<li><p class="comments-section">块长的选择
滤波器长度M和块长度L的关系有三种可能:
1.$$L = M$$,从计算的复杂性上看,最佳.
2.$$L &lt; M$$,有降低延迟的好处.
3.$$L &gt; M$$,将产生自适应过程冗余运算.<div class="comments-icon"></div></p>
</li>
</ul>
<h2 id="flms频域lms">FLMS(频域LMS)</h2>
<p class="comments-section">不论是webRTC还是speex开源的AEC算法都是基于频域来做的.
FLMS（Fast LMS）的基本思想是将时域块LMS放到频域来计算。利用FFT算法在频域上完成滤波器系数的自适应。快速卷积算法用重叠相加法和重叠存储法。重叠相加法是将长序列分成大小相等的短片段，分别对各个端片段做FFT变换，再将变换重叠的部分相加构成最终FFT结果，重叠存储法在分段时，各个短的段之间存在重叠，对各个段进行FFT变换，最后将FFT变换得结果直接相加即得最终变换结果。当块的大小和权值个数相等时，运算效率达到最高。
根据重叠存储方法，将滤波器M个抽头权值用等个数的零来填补，并采用N点FFT进行计算，其中$$N = 2M$$，因此，N*1的向量：<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat {\mathbf W}(k) = FFT \begin{bmatrix}
\hat {\mathbf w}(k)\ 
\mathbf 0
\end{bmatrix} \tag {3.35}<div class="comments-icon"></div></p>
<p class="comments-section">$$
表示FFT补零后的系数，抽头权向量为<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat {\mathbf w}(k)$$。值得注意的是频域权向量$$\hat {\mathbf W}(k)$$的长度是时域权向量$$\hat {\mathbf w}(k)$$长度的两倍。相应的令：<div class="comments-icon"></div></p>
<p class="comments-section">$$X(k) = diag\left {  FFT \begin{bmatrix}
 x(kM-M),\cdots,x(kM-1)&amp; x(kM),\cdots, x(kM+M-1)\ 
 K-1, block &amp; K,block 
\end{bmatrix}\right } \tag{3.36}$$
表示对输入数据的两个相继字块进行傅里叶变换得到一个$$N * N$$对角阵.
将重叠存储法应用于3.26得:<div class="comments-icon"></div></p>
<p class="comments-section">$$y^T(k) = \begin{bmatrix}
y(kM) &amp; y(kM+1) &amp; \cdots &amp; y(kM+M-1)
\end{bmatrix} = IFFT[\mathbf X(k) \hat {\mathbf W}(k)], last \mathbf M \tag {3.37}$$
每处理一帧,式3.37只有最后的M个元素被保留,因为前面的N个元素是循环卷积的结果.
设第$$K$$块的$$M*1$$期望响应和误差信号分别如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf d(k) = \begin{bmatrix}
d(kM) &amp; d(kM+1) &amp; \cdots &amp; d(kM+M-1)
\end{bmatrix}^T$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf e(k) = \begin{bmatrix}
e(kM) &amp; e(kM+1) &amp; \cdots &amp; e(kM+M-1)
\end{bmatrix}^T = \mathbf d(k) - \mathbf y(k)$$
根据式3.37,可将$$\mathbf e(k)$$变换到频域,即<div class="comments-icon"></div></p>
<p class="comments-section">$$E(k) = FFT\begin{bmatrix}
\mathbf 0\ 
\mathbf e(k)
\end{bmatrix} \tag {3.38}$$
则在更新权值的相关矩阵如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\Phi(k) = \sum \limits_{i=0}^{L-1}x(kL+i)e(kL_i) = IFFT[\mathbf X^T(k) \mathbf E(k)]$$,的最前面M个元素,则抽头更新过程在频域中的表现如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat {\mathbf W}(k+1) = \hat {\mathbf W}(k) + \mu FFT\begin{bmatrix}
\Phi(k)\ 
\mathbf 0
\end{bmatrix} \tag {3.39}$$<div class="comments-icon"></div></p>
<h2 id="mdf自适应权值调整">MDF自适应权值调整</h2>
<ul>
<li>时域解
对于N阶NLMS算法，其误差调节向量如下式：
$$e(n) = d(n) - \hat y(n) = d(n) - \sum \limits_{k=0}^{N-1}\hat w_k(n)x(n-k) \tag{3.40}$$
权值更新如下:</li>
</ul>
<p class="comments-section">$$\hat w<em>k(n+1) = \hat w_k(n) + \mu \frac{e(n)x^*(n-k)}{\sum</em>{i=0}^{N-1}|x(n-i)|^2}  = 
\hat w<em>k(n) + \mu \frac{d(n) - \sum_i \hat w_i(n)x(n-i)x^*(n-k)}{\sum</em>{i=0}^{N-1}|x(n-i)|^2}
\tag {3.41}$$
其中$$x(n)$$是参考信号,$$\hat w_k(n)$$是$$n$$时刻和步长$$\mu$$的权值更新. 假设滤波后的误差为$$\delta_k(n) = \hat w_k(n) - w_k(n), d(n) = v(n) + \sum_k \hat w_k(n) x(n-k)$$,则误差的迭代关系如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\delta<em>k(n+1) = \delta_k + \mu \frac{v(n) - \sum_i \delta_i(n) x(n-i) x^*(n-k)}{\sum</em>{i=0}^{N-1}|x(n-1)|^2} \tag {3.42}$$
在每一次调节中,滤波器的误差估计为$$\Lambda = \sum_k \delta_k^*(n)\delta_k(n)$$,展开后得如下形式:<div class="comments-icon"></div></p>
<p class="comments-section">$$\Lambda(n+1) = \sum \limits<em>{k=0}^{N-1}\begin{vmatrix}
\delta_k(n) + \mu \frac{v(n) - \sum_i \delta_i(n) x(n-i) x^*(n-k)}{\sum</em>{i=0}^{N-1}|x(n-1)|^2}
\end{vmatrix} \tag {3.43}$$
如果$$x(n)$$和$$v(n)$$是不相关的白噪声信号,则如下式:<div class="comments-icon"></div></p>
<p class="comments-section">$$E{\Lambda(n+1)| \Lambda(n), x(n)} = \Lambda (n) \begin{bmatrix}
1 - \frac{2\mu}{N} + \frac{\mu^2}{N} + \frac{\mu^2\delta<em>y^2}{\Lambda(n)\sum</em>{i=0}^{N-1}|x(n-1)|^2}
\end{bmatrix} \tag{3.44}$$
可以通过求解$$\partial E{ \Lambda(n+1)}/ \partial \mu = 0, \Lambda \ne 0$$:<div class="comments-icon"></div></p>
<p class="comments-section">$$\frac{-2}{N}+ \frac{2\mu}{N}+ \frac{2\mu \sigma<em>y^2}{\Lambda(n) \sum</em>{i=0}^{N-1}|x(n-1|^2} = 0 \tag{3.45}$$
求解后得到最优步长:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mu<em>{opt}(n) = \frac{1}{1+ \frac{\sigma_y^2}{\Lambda(n)(1/N)\sum</em>{i=0}^{N-1}|x(n-i)|^2}} \tag{3.46}$$
期望$$\Lambda(n)(1/N) \sum_{i=0}^{N-1}|x(n-i)|^2$$等于剩余回声的方差$$\sigma_r^2(n)$$,如果剩余回声的方差值等于0,则步长因子等于1, $$r(n) = y(n) - \hat y(n)$$,则有输出信号的方差是:<div class="comments-icon"></div></p>
<p class="comments-section">$$\sigma_e^2(n) = \sigma_v^2(n) + \sigma_r^2(n) \tag{3.48}$$
这样可以求得这种情况下的最优步长因子为:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mu_{opt} \approx \frac{\sigma_r^2(n)}{\sigma_e^2(n)} \tag {3.49}$$
则最优步长因子如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat \mu_{opt}(n) = min(\frac{\hat \sigma_r^2}{\hat \sigma_e^2(n)}, 1) \tag {3.50}$$
当$$\Lambda(n) \approx \frac{\sigma_v^2}{\sigma_x^2(\frac{2}{\mu})-1}$$时,式3.44停止迭代.将3.49带入3.50得到在滤波器系数不更新情况的剩余回声:<div class="comments-icon"></div></p>
<p class="comments-section">$$\delta_r^2(n) \approx min(\frac{1}{2}\delta_r^2(n), \sigma_v^2(n)) \tag {3.51}$$<div class="comments-icon"></div></p>
<ul>
<li>频域解</li>
</ul>
<p class="comments-section">和时域相比,频域可以使步长因子$$\mu(k, l)$$按频域划分,$$Y(k,l)$$和$$E(k,l)$$分别是频域中的记号,其和时域中的$$\hat y(n)$$和$$e(n)$$是对等的关系,k是频域索引,l是帧索引,和3.49类似,可得频域步长因子:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mu_{opt}(k,l) \approx \frac{\sigma_r^2(k,l)}{\sigma_e^2(k,l)} \tag {3.52}$$
假设滤波器有一个和频谱无关的泄露（滤波器的误差）系数$$\eta(l)$$，这将得到:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat \sigma<em>r^2(k,l) = \hat{\eta}(l) \hat{\sigma}</em>{\hat Y}^2(k, l) \tag {3.53}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\eta(l)$$实际上是滤波器回声返回损失增强ERLE.
为了让步长因子调节的更快,使用瞬时估计,$$\hat \sigma_y(k,l) = |Y(k,l)|^2$$和$$\hat \sigma_e(k,l) = |E(k,l)|^2$$,这将使得3.50步长因子变为:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat \mu<em>{opt}(k,l) = min(\hat \eta(l) \frac{\hat Y(k,l)|^2}{|E(k,l)|^2}, \mu</em>{max}) \tag {3.54}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\mu_{max}$$是小于等于1的数,以确保滤波器稳定.<div class="comments-icon"></div></p>
<h2 id="工程中的思考">工程中的思考</h2>
<h3 id="关于步长因子">关于步长因子</h3>
<p class="comments-section">对于视频通话这类场景，两个通信终端的时钟偏斜和漂移是不定的，而音箱场景这个是可以在硬件上加以解决的，但是音箱场景的非线性失真却比通信场景严重的，功率放大模块非线性器件带来的谐波失真，在室内四个方向都发声，是得卷积失真，多次反射回声，声音突变等会加剧问题处理的复杂性；
当前绝大部分的AEC算法基本都基于频域分块处理方法，基于LMS/NLMS、RLS(recursive least square), APA(Affine Projection Algorithm)自适应处理方法。该自适应方的基本公式是：<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf h(n+1) = \mathbf h(n) +\mu e(n) \mathbf x(n) \tag{3.55}$$
这个公式中误差信号e是维纳滤波（相减）后可以计算得到的，而步长因子却无法直接求得，在有些场景中，根据线性代数推导，在稳定性和收敛速度的双重约束下可以得到步长因子$$\mu$$要满足步长因子小于输入信号协方差矩阵的迹的导数：<div class="comments-icon"></div></p>
<p class="comments-section">$$\mu \lt \frac{1}{trace(R<em>{xx})} \le \frac{1}{\lambda</em>{max}} \tag {3.56}$$
针对LMS的一个改进是NLMS算法，这个方法根据输入输入信号的功率对步长进行归一化：<div class="comments-icon"></div></p>
<p class="comments-section">$$\mu = \frac{\beta}{|x(n)|^2} \tag{3.57}$$
其中$$\beta$$是归一化步长因子，$$0\lt \beta \lt 2$$.为了防止分母为零，加小数a，得如下计算公式：<div class="comments-icon"></div></p>
<p class="comments-section">$$h(n+1)= h(n) + \frac{\beta}{|x(n)|^2+a} x(n)e(n) \tag{3.58}$$
这样收敛速度和输入信号的功率绝对值无关。但是这两种算法在输入信号相关性在很高时收敛速度会比较慢。更进一步的可以采用自适应NLMS算法：
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/2018052011012031.png" alt=""><div class="comments-icon"></div></p>
<h3 id="关于误差信号">关于误差信号</h3>
<p class="comments-section">LMS算法消除了线性部分，得到的是残余线性部分和非线性部分之和，非线性部分源于外界的噪声，参考源的卷积响应以及喇叭或者传输路径的影响，常用的方法可以<div class="comments-icon"></div></p>
<p class="comments-section">RERL:ERL+ERLE
RERL:residual_echo_return_loss
ERL:echo_return_loss
ERLE:echo_return_loss_enhancement
psd:power spectral density 功率谱密度
x: far end
d: near end
e: error
s: psd
nlp:non-linear processing<div class="comments-icon"></div></p>
<h2 id="结束语">结束语</h2>
<p class="comments-section">本章主要阐述了AEC要解决的问题,以及常用的时域和频域解决方法,值得一提的是,产品结构布局影响还是很大的,大家可以看到市场上绝大多数智能音响产品的喇叭和麦克风一般在物理尺寸的两端(圆柱形的外形,一般麦克在顶部,喇叭在中下部,且喇叭开口朝下,通过反射锥以弥补听感上损失),可以收音和发音方向相反且隔开的.
有了以上理论,可以从开源的实例(<a href="https://github.com/shichaog/WebRTC-audio-processing" target="_blank">WebRTC MATLAB and c code github Address</a>),下一章以webRTC为实例,剖析MATLAB和C代码,在我的github上有相关开源代码的使用实例.需要注意的是我代码里给定的默认设置值,不一定针对你的场景是最优的,但是效果一定是可以听出来的.<div class="comments-icon"></div></p>
<h2 id="rec（residual-echo-control）">REC（residual echo control）</h2>
<p class="comments-section">使用滤波器来近似非线性响应函数，这样也会得到非线性估计，实际上最早开始部分提到的误差信号e是减去了线性和非线性部分得到的误差信号，滤波器系数可以通过最小能量均分来做为准则进行平滑。非线性部分目前来说算是各个AEC最大的差异体现吧，通常希望滤波器阶数较高，能够处理较长的时间长度（即信号经过多次反射到达的场景）。比较有名的处理的滤波器是volterra 滤波器，使用一阶，二阶和三阶滤波器来去除非线性部分，但随着阶数的增加，计算量也呈现指数方式增加，其一阶的表示式如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf X_1[k] = (x[k], x[k-1], \cdots, x[k-M+1]) \tag {3.60}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf {\hat h_1}= (\hat h_1[0], \hat h_1[1], \cdots, \hat h_1[M-1]) \tag{3.61}$$
二阶的表示如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf x_2[k] = (x^2[k], x[k]x[k-1], \cdots, x[k]x[k-M+1], 
                      x[k-1]x[k-1], \cdots, x[k-M+1]x[k-M+1]) \tag {3.62}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf {\hat h}_1 =  (\hat h_2[0,0], \hat h_2[0,1], \cdots, \hat h_2[0, M-1], \hat h_2[1,1], \cdots, \hat h_2[M-1, M-1]) \tag{3.63}$$
则误差信号可以表示为：<div class="comments-icon"></div></p>
<p class="comments-section">$$e[k] = y[k] -\hat h_0[k] -\mathbf {\hat h}_1[k] \mathbf {x}_1^T[k] -\mathbf {h}_2[k] \mathbf {x}_2^T[k] \tag {3.64}$$
则滤波器的系数更新方程如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat h_0[k+1] =  {\hat h}_0[k] + \mu_0e[k] \tag{3.65}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf {\hat h}_1[k+1] = \mathbf {\hat h}_1[k] + \mu_0e[k]\mathbf x_1[k] \tag{3.65}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf {\hat h}_2[k+1] = \mathbf {\hat h}_2[k] + \mu_0e[k]\mathbf x_2[k] \tag{3.65}$$
这里涉及三个步长更新速率，$$\mu_0$$, $$\mu_1 \lt \frac{2}{||\mathbf x_1||_2^2}$$, $$\mu_2 \lt \frac{\alpha}{||\mathbf x_1||_2^2 ||\mathbf x_2||_2^2}$$,当$$0 \lt \alpha \lt 2$$时，这一过程是收敛的。<div class="comments-icon"></div></p>
<h2 id="erl-vs-erle">ERL vs ERLE</h2>
<p class="comments-section">ERL :echo return loss = (mic in power) / (ref power)
ERLE :echo return loss enhancement = (mic in power) / (power of residual echo signal)
ERL和麦克风采集到的带回声信号和参考信号的比值，单位常用dB表示，比值越高，反映的是回声信号越小，大多数的VoIP设备的ERL值在15~20dB之间。<div class="comments-icon"></div></p>
<h2 id="自动回声消除aec实现实例">自动回声消除(AEC)实现实例</h2>
WebRTC的AEC算法主要包括以下几个重要的模块:<div class="comments-icon"></div></p>
<ol>
<li>回声延迟估计模块</li>
<li>NLMS算法(见前一章)</li>
<li>NLP非线性滤波
4 .CNG(comfort nosie generation),我的github上将噪声产生功能去掉了,因为ASR原来就是需要纯净语音的.
此外,像speex之类的算法还包括双讲检测(DT,double talk)</li>
</ol>
<p class="comments-section">WebRTC MATLAB and c code github Address<div class="comments-icon"></div></p>
<p class="comments-section">回声延迟估计
回声延迟长短对回声抵消器的性能有较大影响，过长的的滤波器抽头会带来较大的延迟，并且语音信号是短时平稳信号，过长的滤波器抽头也不适合短时平稳信号特征。
基于相关的延迟算法。该算法的主要思想是：
设1表示有说话音，0表无说话声（或者很弱的说话声），参考端（远端）信号<div class="comments-icon"></div></p>
<p class="comments-section">和接收端信号可能的组合方式如下：<div class="comments-icon"></div></p>
<p class="comments-section">webrtc默认(1, 0)和（0，1）是不可能发生的。设在时间间隔p上，即<div class="comments-icon"></div></p>
<p class="comments-section">,频带q,<div class="comments-icon"></div></p>
<p class="comments-section">,输入信号<div class="comments-icon"></div></p>
<p class="comments-section">加窗后的功率谱用<div class="comments-icon"></div></p>
<p class="comments-section">表示，其角标表示其加了窗函数。对每个频带的功率谱设定一个门限<div class="comments-icon"></div></p>
<p>,
如果</p>
<p>, 则</p>
<p>;
如果</p>
<p>, 则</p>
<p class="comments-section">;
同理,对于信号<div class="comments-icon"></div></p>
<p class="comments-section">,加窗信号功率谱<div class="comments-icon"></div></p>
<p>和门限</p>
<p>如果</p>
<p>, 则</p>
<p>;
如果</p>
<p>, 则</p>
<p class="comments-section">;
考虑到实际处理的方便，在webrtc的c代码中，将经过fft变换后的频域功率谱分为32个子带，这样每个特定子带<div class="comments-icon"></div></p>
<p class="comments-section">的值可以用1个比特来表示，总共需要32个比特，只用一个32位数据类型就可以表示。
NLMS归一化最小均放只适应算法
见前一章
NLP(非线性滤波)
webrtc采用了维纳滤波器，此处只给出传递函数的表达式，设估计的语音信号的功率谱为<div class="comments-icon"></div></p>
<p class="comments-section">, 噪声的功率谱为<div class="comments-icon"></div></p>
<p class="comments-section">, 则滤波器的传递函数为<div class="comments-icon"></div></p>
<p class="comments-section">对于webRTC的AEC需要注意两点:
延迟要小,因为算法默认滤波器长度是分为12块，每块64点，按照8000采样率，也就是12×8ms=96ms的数据，超过这个长度就处理不了了。
延迟抖动要小，因为算法是默认10块也计算一次参考数据的位置（即滤波器能量最大的那一块），所以如果抖动很大的话，找参考数据不准确的，这样回声就消除不掉了。
实现代码分析
几个名词
RERL-residual_echo_return_loss
ERL-echo return loss
ERLE echo return loss enhancement
NLP non-linear processing
前三个是性能评估的参考标准.
初始化和数据读入
%near is micphone captured signal
fid=fopen('near.pcm', 'rb'); % Load far end
ssin=fread(fid,inf,'float32');
fclose(fid);
%far is speaker played music
fid=fopen('far.pcm', 'rb'); % Load fnear end
rrin=fread(fid,inf,'float32');
fclose(fid);
然后对一些变量赋初值
fs=16000;
NLPon=1; % NLP on
M = 16; % Number of partitions
N = 64; % Partition length
L = M<em>N; % Filter length
VADtd=48;
alp = 0.15; % Power estimation factor 
alc = 0.1; % Coherence estimation factor
step = 0.1875;%0.1875; % Downward step size
上述初始化中，M=16和最新的WebRTC代码并不一致，且最新的WebRTC中支持aec3最新一代算法。
len=length(ssin);
NN=len;
Nb=floor(NN/N)-M;
for kk=1:Nb
    pos = N </em> (kk-1) + start;
可以看出Nb是麦克风采集到的数据块数-16（分区数），这是因为第一次输入了16块，所以这里减掉了16。pos是每一次添加一块时的地址指针。
    %far is speaker played music
    xk = rrin(pos:pos+N-1);
    %near is micphone captured signal
    dk = ssin(pos:pos+N-1);
xk和dk是读取到的64个点，这里是时域信号。
功率计算
    %----------------------- far end signal process
    xx = [xo;xk];
    xo = xk;
    tmp = fft(xx);
    XX = tmp(1:N+1);<div class="comments-icon"></div></p>
<pre><code>dd = [do;dk]; % Overlap
do = dk;
tmp = fft(dd); % Frequency domain
DD = tmp(1:N+1);
</code></pre><p class="comments-section">将xk和上一次的数据结合在一起，做FFT变换，由于两次组合在一起，那么得到的应该是N=128点，这里可以知道得到的谱分辨率是$n<em>fs/N$,$fs$前面设置过了，是16k，则得到的每一个bin的频谱分辨率是16000/128=125Hz。这里XX和DD取了前65个点，这是因为N点FFT变换后频谱是关于N/2对称的，对称的原因是奈奎斯特采样定理，如果$fs=16000Hz$,那么要求采样到的信号的截止频率必然小于等于$fs/2=8000Hz$,对于实信号，N/2~N,实际上表示的是$-fs/2$ ~ $0$之间的频率。第一个点是直流分量，所以取65个点。和上一帧64个点信号合并在一起的另一个原因是使用重叠保（overlap-save）留法将循环卷积变成线性卷积，这里做的FFT变换，就是为了减少时域里做卷积的计算量的。
计算远端信号功率谱
    % ------------------------far end Power estimation
    pn0 = (1 - alp) </em> pn0 + alp <em> real(XX.</em> conj(XX));
    pn = pn0;
平滑功率谱，上一次的功率谱占85%（alp=0.15），后面的频域共轭相乘等于功率是有帕斯瓦尔定理支撑的。pn0是65<em>1的矩阵。
滤波
 XFm(:,1) = XX;
首先将远端信号频谱赋给XFm，XFm是65</em>16的矩阵，16就是前面初始化的M值，这里将XX给第一列，其2~16列对应的是之前的输入频谱。
    for mm=0:(M-1)
        m=mm+1;
        YFb(:,m) = XFm(:,m) .<em> WFb(:,m);
    end
YFb，WFb以及XFm都是65</em>16的矩阵，WFb是自适应滤波器的频谱表示，XFm是原始的speaker数据，上式的意义对应于插图中的$\hat{y}$的频域值，变换到时域后就可以得到$y$的估计值$\hat{y}$.
    yfk = sum(YFb,2);
    tmp = [yfk ; flipud(conj(yfk(2:N)))];
    ykt = real(ifft(tmp));
    ykfb = ykt(end-N+1:end);
首先yfk是651的矩阵，sum求和就是将估计的频谱按行求和，也就是yfk包含了最近16个块的远端频谱估计信息，这样，只要近端麦克采集到的信号里有这16个块包含的远端信号，那么就可以消掉，从这里也可以看出来，容许的延迟差 在1664/16=64ms，也就是说，如果麦克风采集到的speaker信号滞后speaker播放超过64ms，那么这种情况是无法消掉的，当然，延迟差越小越好。
flipud(conj(yfk(2:N))是因为前面计算频谱时利用奈奎斯特定理，也即实数的FFT结果按N/2对称，所以这里为了得到正确的ifft变换结果，先把频谱不全到$fs$.
ykfb就是<div class="comments-icon"></div></p>
<p class="comments-section">.后面再看WFb是如何跟新。
误差估计
   ekfb = dk - ykfb;
dk是麦克风采集到的信号，ykfb是$\hat{y}$,这样得到的是误差信号，理想情况下，那么得到的误差信号就是需要的人声信号，而完全滤除 掉了speaker信号（远端信号）。
    erfb(pos:pos+N-1) = ekfb;
    tmp = fft([zm;ekfb]); % FD version for cancelling part (overlap-save)
    Ek = tmp(1:N+1);
erfb是近端信号数组长度×1矩阵，存放的是全部样本对应的误差信号，这个保存仅仅是为了plot用的。
然后补了64个零，然后做FFT，Ek是误差信号FFT的结果。
自适应调节
   Ek2 = Ek ./(pn + 0.001); % Normalized error
pn是当前帧远端信号功率谱，Ek是误差信号频谱。Ek2是归一化误差频谱。NLMS公式要求。
    absEf = max(abs(Ek2), threshold);
    absEf = ones(N+1,1)<em>threshold./absEf;
    Ek2 = Ek2.</em>absEf;
max的作用是为了防止归一化后误差频谱过小，最终得到的Ek2是一个限幅矩阵，如果该点的值比门限大，则取门限，如果该点的值比门限小，则保持不变。
 mEk = mufb.<em>Ek2;
mufb是步长，对于16000情况，步长取了0.8.NLMS公式。
 PP = conj(XFm).</em>(ones(M,1) <em> mEk')';
    tmp = [PP ; flipud(conj(PP(2:N,:)))];
    IFPP = real(ifft(tmp));
    PH = IFPP(1:N,:);
    tmp = fft([PH;zeros(N,M)]);
    FPH = tmp(1:N+1,:);
    WFb = WFb + FPH;
PP是将远端信号的共轭乘以误差信号频谱，这一项用于调节步长，NLMS（步长=参考信号×步长×误差）的可变步长就提现在这里。PH是频域到时域的变换值。这和前面频域到时域的变换原理一样。WFb是权中系数的更新。
    if mod(kk, 10</em>mult) == 0
        WFbEn = sum(real(WFb.*conj(WFb)));
        %WFbEn = sum(abs(WFb));
        [tmp, dIdx] = max(WFbEn);<div class="comments-icon"></div></p>
<pre><code>    WFbD = sum(abs(WFb(:, dIdx)),2);
    %WFbD = WFbD / (mean(WFbD) + 1e-10);
    WFbD = min(max(WFbD, 0.5), 4);
end
 dIdxV(kk) = dIdx;
</code></pre><p class="comments-section">上述的作用是更新dIdx和dIdxV。这里的更新并不是每一次都更新，一来是为了稳定，而来也是变相的减少计算量，提高实时性。就算是每一次都更新dIdx，WebRTC计算速度评估的结果也是很满意的。WFb是权重向量的频谱表示，WFbEn是权重向量按列求和，得到的是161的矩阵。这样得到的是16个块对权重的累加和。这样的区分度比直接累加和要大。
 [tmp, dIdx] = max(WFbEn);作用就是找到16个块中权重累加和最大值及其对应的索引。
 WFbD首先计算了权重最大那个块dIdx的列，然后将其按行求和，得到的就是651矩阵。WFbD不能低于0.5也不能高于4，算法中并未使用到，plot性能分析时用到。
最后把索引值dIdx存放到dIdxV(kk)中，这样每来一帧，就会有一个最大索引值放到dIdxV向量中。
功率谱密度和相关性计算
NLP
这里的NLP(Non-linear processing)的意思。
        ee = [eo;ekfb];
        eo = ekfb;
        window = wins;
上述作用是将上次的误差和ekfb组合，其中eo可以理解为error old。eo也确实保存了上一次的误差。window是简单将窗函数赋值。
        tmp = fft(xx.<em>window);
        xf = tmp(1:N+1);
        tmp = fft(dd.</em>window);
        df = tmp(1:N+1);
        tmp = fft(ee.<em>window);
        ef = tmp(1:N+1);
上述代码是把xx，dd，ee加窗后做FFT变换，并且取了$fs/2$的频谱部分存放到xf，df以及ef中。加窗的目的是为了减小频谱泄露，提高谱分辨率。
        xfwm(:,1) = xf;
        xf = xfwm(:,dIdx);
        %fprintf(1,'%d: %f\n', kk, xf(4));
        dfm(:,1) = df;
将xf存放到xfwm的第一列，xfwm是65</em>16的矩阵，df做类似处理。
然后把dIdx指向的那一列传给xf，dIdx是之前计算的权重矩阵能量最大的那块的索引，这样从xfwm矩阵里把真正要处理近端信号对应的远端信号（speaker，参考信号）获取到。
        Se = gamma<em>Se + (1-gamma)</em>real(ef.<em>conj(ef));
        Sd = gamma</em>Sd + (1-gamma)<em>real(df.</em>conj(df));
        Sx = gamma<em>Sx + (1 - gamma)</em>real(xf.<em>conj(xf));
计算ef，df和xf的平滑功率谱，gamma这里取值是0.92.相对于8k信号取值略大。它们都是65</em>1的矩阵，包括了直流分量的能力，剩下的64点是$fs/2$及以下的频点能量。
        Sxd = gamma<em>Sxd + (1 - gamma)</em>xf.<em>conj(df);
        Sed = gamma</em>Sed + (1-gamma)<em>ef.</em>conj(df);
计算互功率谱，这里计算了远端信号和近端信号功率谱，如果该值较大，则说明它们的相关性较强，说明近端信号采集到了远端信号的概率很大，这时需要进行噪声抑制，同样的如果误差信号和近端信号功率谱较大，则说明估计的$\hat{y}$是比较准的，误差信号里剩余的远端信号较少，需要进行噪声抑制的概率就小。它们也都是65<em>1矩阵，对应频点的bin。但是上述获得的是绝对值而非相对值，门限设置不容易，需要一个归一化的过程。归一化的过程可以通过求互相关得到。
        cohed = real(Sed.</em>conj(Sed))./(Se.<em>Sd + 1e-10);
        cohedAvg(kk) = mean(cohed(echoBandRange));
        cohxd = real(Sxd.</em>conj(Sxd))./(Sx.<em>Sd + 1e-10);
如上，计算误差信号和近端信号的互相关，1e-10是为了防止除0报错。cohed越大，表示回声越小，cohxd越大，表示回声越大，这里就可以设置一个统一的门限评判上下限了。
cohedMean = mean(cohed(echoBandRange));
计算设置的echoBandRange里频点的均值，如果echoBandRange设置的过大，则低音部分如鼓点声则不易消掉。
        hnled = min(1 - cohxd, cohed);
这里的作用就是把最小值赋值给hnled，该值越大，则说明越不需要消回声。之所以取二者判断，是为了最大可能性的消除掉回声。
        [hnlSort,     hnlSortIdx] = sort(1-cohxd(echoBandRange));
        [xSort, xSortIdx] = sort(Sx);
对1-cohxd（echoBandRange）进行升序排序，同样对Sx也进行升序排序。
hnlSortQ = mean(1 - cohxd(echoBandRange));
对远端和近端信号的带内互相关求均值。hnlSortQ表示远端和近端不相关性的平均值，其值越大约没有回声，与cohed含义一致。
 [hnlSort2, hnlSortIdx2] = sort(hnled(echoBandRange));
对hnled进行升序排序。
        hnlQuant = 0.75;
        hnlQuantLow = 0.5;
        qIdx = floor(hnlQuant</em>length(hnlSort2));
        qIdxLow = floor(hnlQuantLow<em>length(hnlSort2));
        hnlPrefAvg = hnlSort2(qIdx);
        hnlPrefAvgLow = hnlSort2(qIdxLow);
这里主要取了两个值，一个值取在了排序后的3/4处，一个值取在了排序后的1/2处。
            if cohedMean &gt; 0.98 &amp; hnlSortQ &gt; 0.9
                suppState = 0;
            elseif cohedMean &lt; 0.95 | hnlSortQ &lt; 0.8
                suppState = 1;
            end
如果误差和近端信号的互相关均值大于0.98，且远端和近端频带内的互不相关大于0.9，则说明不需要进行回声抑制，将suppState标志设置成0，如果误差和近端信号小于0.95或者远端和近端频带内信号不相关性小于0.8则需要进行印制，在这个范围之外的，回声抑制标志保持前一帧的状态。
            if hnlSortQ &lt; cohxdLocalMin &amp; hnlSortQ &lt; 0.75
                cohxdLocalMin = hnlSortQ;
            end
cohxdLocalMin的初始值是1，表示远端和近端完全不相关，这里判断计算得到的远端近端不相关性是否小于前一次的不相关性，如果小于且不相关性小于0.75，则更新cohxdLocalMin。
            if cohxdLocalMin == 1
                ovrd = 3;
                hnled = 1-cohxd;
                hnlPrefAvg = hnlSortQ;
                hnlPrefAvgLow = hnlSortQ;
            end
如果cohxdLocalMin=1，则说明要么是发现远端和近端完全不相关，要么就是cohxdLocalMin一直没有更新，既然不相关性很大，那么也说明有回声的可能性小，那么使用较小的ovrd（over-driven）值，和较大的hnled（65</em>1）值。
            if suppState == 0
                hnled = cohed;
                hnlPrefAvg = cohedMean;
                hnlPrefAvgLow = cohedMean;
            end
如果suppState==0，则说明不需要进行回声消除，直接用误差近端相关性修正误差信号，hnl的两个均值读取cohed的均值，在这种情况下hnled的值接近于1.
            if hnlPrefAvgLow &lt; hnlLocalMin &amp; hnlPrefAvgLow &lt; 0.6
                hnlLocalMin = hnlPrefAvgLow;
                hnlMin = hnlPrefAvgLow;
                hnlNewMin = 1;
                hnlMinCtr = 0;
                if hnlMinCtr == 0
                    hnlMinCtr = hnlMinCtr + 1;
                else
                    hnlMinCtr = 0;
                    hnlMin = hnlLocalMin;
                    SeLocalMin = SeQ;
                    SdLocalMin = SdQ;
                    SeLocalAvg = 0;
                    minCtr = 0;
                    ovrd = max(log(0.0001)/log(hnlMin), 2);
                    divergeFact = hnlLocalMin;
                end
            end
hnlLocalMin是hnl系数的最小值，在满足这条判断的情况下发现了更小的值，需要对其进行更新，同时表标志设置成1，计数清0，这种情况下回声的概率较大，所以把ovrd设大以增强抑制能力。
            if hnlMinCtr == 2
                hnlNewMin = 0;
                hnlMinCtr = 0;
                ovrd = max(log(0.00000001)/(log(hnlMin + 1e-10) + 1e-10), 5);<div class="comments-icon"></div></p>
<pre><code>        end
</code></pre><p class="comments-section">hnlMinCtr==2，说明之前有满足&lt;0.6的块使得hnlminctr=2，然后其下一块又没有满足&lt;0.6的条件，进而更新了ovrd值。默认是和3比较取最大值，这里调节成5是为了增加抑制效果，实际情况还需要针对系统调试。 0="" hnllocalmin="min(hnlLocalMin" +="" 0.0008="" mult,="" 1);="" cohxdlocalmin="min(cohxdLocalMin" 0.0004="" 跟新上述两个值，mult是$fs="" 8000$,对于16khz，就是2.就是0.0004和0.0002的差异。="" if="" ovrd="" &lt;="" ovrdsm="" 0.01*ovrd;="" else="" 0.1*ovrd;="" end="" 平滑更新ovrdsm，上述结果倾向于保持ovrdsm是一个较大的值，这个较大的值是为了尽量抑制回声。="" eken="sum(Se);" dken="sum(Sd);" 按行求和，物理意义分别是误差能量和近端信号能量。="" 发散处理="" divergestate="="&gt; dkEn
                ef = df;
                divergeState = 1;
            end
        else
            if ekEn<em>1.05 &lt; dkEn
                divergeState = 0;
            else
                ef = df;
            end
        end
如果不进行发散处理，误差能量大于近端能力，则用近端频谱更新误差频谱并把发散状态设置成1，如果误差能量的1.05倍小于近端能量，则发散处理标志设置成0.
        if ekEn &gt; dkEn</em>19.95
            WFb=zeros(N+1,M); % Block-based FD NLMS
        end
如果误差能量大于近端能量×19.95则，将权重系数矩阵设置成0.
        ekEnV(kk) = ekEn;
        dkEnV(kk) = dkEn;
相应能量存放在相应的向量中。
        hnlLocalMinV(kk) = hnlLocalMin;
        cohxdLocalMinV(kk) = cohxdLocalMin;
        hnlMinV(kk) = hnlMin;
上述三个向量保存对应值。
平滑滤波器系数和抑制指数
        wCurve = [0; aggrFact<em>sqrt(linspace(0,1,N))' + 0.1];
权重系数曲线生成，线性均匀分布。
    hnled = weight.</em>min(hnlPrefAvg, hnled) + (1 - weight).<em>hnled;
使用权重平滑hnled，wCurve分布是让65点中频率越高的点本次hnled占的比重越大，反之则以前的平滑结果所占比重大。
od = ovrdSm</em>(sqrt(linspace(0,1,N+1))' + 1);
产生65<em>1的曲线，用作更新hnled的幂指数。
      hnled = hnled.^(od.</em>sshift);
od作为幂指数跟新hnled。
输出回声消除后的信号
 hnl = hnled;
 ef = ef.<em>(hnl);
用hnl系数与误差频谱相乘，即频域卷积，就是将误差信号通过了传递函数为hnnl的滤波器。
        ovrdV(kk) = ovrdSm;
        hnledAvg(kk) = 1-mean(1-cohed(echoBandRange));
        hnlxdAvg(kk) = 1-mean(cohxd(echoBandRange));
        hnlSortQV(kk) = hnlPrefAvgLow;
        hnlPrefAvgV(kk) = hnlPrefAvg;
相关值的暂存
没有开启舒适噪声产生，则Fmix=ef。
    % Overlap and add in time domain for smoothness
    tmp = [Fmix ; flipud(conj(Fmix(2:N)))];
    mixw = wins.</em>real(ifft(tmp));
    mola = mbuf(end-N+1:end) + mixw(1:N);
    mbuf = mixw;
    ercn(pos:pos+N-1) = mola;
则使用重叠想加法获得时域平滑信号。
    XFm(:,2:end) = XFm(:,1:end-1);
    YFm(:,2:end) = YFm(:,1:end-1);
    xfwm(:,2:end) = xfwm(:,1:end-1);
    dfm(:,2:end) = dfm(:,1:end-1);
全体后移一个块，进入下一块迭代。
执行完了之后，如果想听回声消除后信号的声音，使用如下命令：
sound(10*ercn,16000)
其中16000是输入信号的频率。<!--0.6的块使得hnlminctr=2，然后其下一块又没有满足<0.6的条件，进而更新了ovrd值。默认是和3比较取最大值，这里调节成5是为了增加抑制效果，实际情况还需要针对系统调试。--><div class="comments-icon"></div></p>
<p class="comments-section">WebRTC MATLAB and c code github Address<div class="comments-icon"></div></p>
<p class="comments-section">c调用实例见:
<a href="https://github.com/shichaog/WebRTC-audio-processing/blob/master/src/webrtc_audio_processing.cc" target="_blank">https://github.com/shichaog/WebRTC-audio-processing/blob/master/src/webrtc_audio_processing.cc</a>
speex AEC
代码见上述库里的speex_aec.m文件.
首先需要自己读取文件并设置相关的初始值
给出自己的一个样例
fid=fopen('near.pcm', 'rb'); % Load far end
ssin=fread(fid,inf,'float32');
fid=fopen('far.pcm', 'rb'); % Load fnear end
rrin=fread(fid,inf,'float32');
ssin=ssin(1:4096<em>200);
rrin=rrin(1:4096</em>200);
Fs=16000;
filter_length=4096;
frame_size=128;
speex_mdf_out = speex_mdf(Fs, rrin, ssin, filter_length, frame_size);
执行完之后，需要播放出来听：
sound(speex_mdf_out.e,16000)<div class="comments-icon"></div></p>
<h1 id="有源降噪耳机">有源降噪耳机</h1>
<h2 id="原理">原理</h2>
<p class="comments-section"><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/降噪耳机原理.png" alt="">
图1 有源降噪耳机原理（网络盗图，侵删）<div class="comments-icon"></div></p>
<p class="comments-section">原理如上图所示，要达到声波叠加等于零，需要dsp运算速度快，延迟小，早期的设计都采用模拟电路来解决延迟问题。<div class="comments-icon"></div></p>
<h2 id="自适应lms算法">自适应LMS算法</h2>
<p class="comments-section"><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/LMS算法.png" alt="">
图2 LMS算法在自适应方面的应用<div class="comments-icon"></div></p>
<p class="comments-section">算法的原理，假设$$e(n)$$是图一中麦克风采集到的消声后的信号，$$d(n)$$是真实的环境噪声信号，$$x(n)$$被称为参考信号，这可以通过麦克（非耳塞上的麦克）来获取， 而$$w(n)x(n)$$是扬声器发出的信号，用$$w(n)x(n)$$这一加权值来预测$$d(n)$$的值，如果预测的准，那么$$e(n)$$就等于0，即麦克风的震膜没有震动，人也就听不到噪声了。这个算法称为LMS算法。<div class="comments-icon"></div></p>
<p class="comments-section">基本的算法就是LMS算法，其变种还有FxLMS，RLS以及多通道的情景。
算法代码示意片段git地址<div class="comments-icon"></div></p>
<pre><code>https://github.com/shichaog/anc_for_earphone
</code></pre>
<h1 id="语音检测vad原理和实例">语音检测(VAD)原理和实例</h1>
<p class="comments-section">VAD(voice activity detection)广泛应用于语音编码（网络/无线/有线传输）和语音识别（ASR）。VAD的准确对前端算法也是比较关键的，这里所说的是语音/非语音（非语音/静音）检测，一个VAD系统通常包括两个部分，特征提取和语音/非语音判决；常用的特征提取可以分为五类：<div class="comments-icon"></div></p>
<ul>
<li>基于能量</li>
<li>频域</li>
<li>倒谱</li>
<li>谐波 </li>
<li><p class="comments-section">长时信息 
基于能量的特征常用硬件实现，谱（频谱和倒谱）在低SNR可以获得较好的效果。当SNR到达0dB时，基于语音谐波和长时语音特征更具有鲁棒性。
当前的判决准则可以分为三类：基于门限，统计模型和机器学习。<div class="comments-icon"></div></p>
<h2 id="特征选取准则">特征选取准则</h2>
<p class="comments-section">对于VAD分类问题，特征尤为重要，好的特征应该能具备如下特性：<div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">区分能力
含噪语音帧分布和仅仅噪声帧分布的分离度，理论上，好的特征能够让语音和噪声两类没有交集<div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">噪声鲁棒性
背景噪声会造成语音失真，进而语音特征也会失真，影响到提取到的特征的区分能力。 <div class="comments-icon"></div></p>
<h3 id="基于能量的特征">基于能量的特征</h3>
<p class="comments-section">基于能量的准则是检测信号的强度，并且假设语音能量大于背景噪声能量，这样当能量大于某一门限时，可以认为有语音存在。然而当噪声大到和语音一样时，能量这个特征无法区分语音还是纯噪声。
早先基于能量的方法，将宽带语音分成各个子带，在子带上求能量；因为语音在2KHz以下频带包含大量的能量，而噪声在2~4KHz或者4KHz以上频带比0~2HKz频带倾向有更高的能量。这其实就是频谱平坦度的概念，webrtc中已经用到了。在信噪比低于10dB时，语音和噪声的区分能力会加速下降。<div class="comments-icon"></div></p>
<h3 id="频域特征">频域特征</h3>
<p class="comments-section">通过STFT将时域信号变成频域信号，即使在SNR到0dB时，一些频带的长时包络还是可以区分语音和噪声。<div class="comments-icon"></div></p>
<h3 id="倒谱特征">倒谱特征</h3>
<p class="comments-section">对于VAD，能量倒谱峰值确定了语音信号的基频(pitch)，也有使用MFCC做为特征的.<div class="comments-icon"></div></p>
<h3 id="基于谐波的特征">基于谐波的特征</h3>
<p class="comments-section">语音的一个明显特征是包含了基频$F_0$及其多个谐波频率，即使在强噪声场景，谐波这一特征也是存在的。可以使用自相关的方法找到基频。<div class="comments-icon"></div></p>
<h3 id="长时特征">长时特征</h3>
<p class="comments-section">语音是非稳态信号。普通语速通常每秒发出10~15个音素，音素见的谱分布是不一样的，这就导致了随着时间变化语音统计特性也是变化的。另一方面，日常的绝大多数噪声是稳态的（变化比较慢的），如白噪声/机器噪声。<div class="comments-icon"></div></p>
<h2 id="判决准则">判决准则</h2>
<h3 id="门限">门限</h3>
<p class="comments-section">门限通常是根据训练数据集特征预先得到的，对于噪声变化的场景，需要使用自适应门限，可以将能量$$\eta=f（E），\eta \in {E<em>0,E_1}$$，将能量最小记作$$E_0$$和能量最大记作$$E_1$$，对应的门限为$$\eta_0, \eta_1$$则可以得到如下自适应门限：
$$\eta=
\left{\begin{matrix}
\eta_0 &amp; E\le E_0 \ 
\frac{\eta_0-\eta_1}{E_0 -E_1} E + \eta_0 - \frac{\eta_0-\eta_1}{1-E_1/E_0} &amp; E_0 &lt; E &lt; E_1 \ 
\eta_1 &amp; E \ge E_1
\end{matrix}\right.
\tag {5.1}$$
绝大多数使用了平滑策略来跟新门限，
$$\hat \eta=\alpha \eta+(1-\alpha)\eta</em>{new} \tag{5.2}$$<div class="comments-icon"></div></p>
<h3 id="统计模型方法">统计模型方法</h3>
<p class="comments-section">统计模型的方法最先源于似然比检验（likelihood ratio test LRT），这种方法假设语音和背景噪声是独立高斯分布，这样它们的DFT系数可以用高斯随机变量来描述，这一方法考虑两种假设，$H<em>N$和$H_S$分别表示非语音和语音。给定第k帧谱$$X_k=[X_k,1\cdots X_k,L]^T$$，它们的概率密度函数如下：
$$p(X_k|H_N)=\Pi</em>{i=1}^L \frac{1}{\pi \lambda<em>{N,i}}\exp(-\frac{|X_k,i|^2}{\lambda</em>{N,i}}) \tag{5.3}$$
$$p(X<em>k|H_S)=\Pi</em>{i=1}^L\frac{1}{\pi(\lambda<em>{N,i}+\lambda</em>{S,i})}\exp(-\frac{|X<em>{k,i}|^2}{\lambda</em>{N,i}+\lambda_{S,i}}) \tag{5.4}$$<div class="comments-icon"></div></p>
</li>
</ul>
<p class="comments-section">其中$$i$$是频点索引，$$\lambda<em>N=[\lambda</em>{N,1}\cdots \lambda<em>{N,L}]^T$$并且$$\lambda_S=[\lambda</em>{S,1} \cdots \lambda_{S,L}]^T$$分别是噪声和语音的方差。这些参数可以通过噪声估计和谱减的方法从训练数据集中获得。然后可以获得第i个频段的似然比$$\Lambda_i$$，这样可以获得最终的似然比检验：<div class="comments-icon"></div></p>
<p class="comments-section">$$\Lambda<em>i=\frac{p(X</em>{k,i}|H<em>S)}{p(X</em>{k,i}|H_N)}\tag{5.5}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\log \Lambda=\frac{1}{L}\sum_{i=1}^L \log \Lambda_i, if \log \Lambda \ge \eta,then, H_S,else H_N \tag {5.6}$$
在webrtc中，使用的就是这一思想，此外，还可以借鉴到webrtc中定点化的思想在里面。
由于1.6中左侧的log总是正数，这一似然比偏向$H_S$，所以减小只有噪声时似然比波动的decision-directed（DD）方法被提出来，DD方法在语音到非语音变换区域容易发生错误。又有基于平滑的方法被提出来。
上述是基于高斯的统计模型，有其它学者提出使用DCT或者KL做为特征的拉普拉斯，伽马分布以及双侧伽马分布可以更好的描述语音和噪声的分布。这在低信噪比时可以获得更好的VAD检测结果。<div class="comments-icon"></div></p>
<h3 id="机器学习方法">机器学习方法</h3>
<p class="comments-section">这一思想是语言的模型是通过学习得到的，目前训练数据集都是通过传统的VAD方法进行标注的，如果标注的数据里就有不准确的，那么如何通过NN获得更准确的VAD，这类方法的难点个人认为更多的是实际强噪声场景下的训练数据集标注的问题。<div class="comments-icon"></div></p>
<h2 id="vad实例">VAD实例</h2>
<p class="comments-section">本文以webRTC为例说明,webRTC是一个多特征综合评估.同上一章,
代码在
<a href="https://github.com/shichaog/WebRTC-audio-processing" target="_blank">VAD example code download Address</a>.<div class="comments-icon"></div></p>
<h3 id="高斯分布">高斯分布</h3>
<p class="comments-section">高斯分布又称为正态分布（Normal distribution/Gaussian distribution）。
若随机变量X服从一个数学期望为μ，标准差为σ^2的高斯分布，则：<div class="comments-icon"></div></p>
<p class="comments-section">$$X~N(\mu，\sigma^2)$$
其概率密度函数为：<div class="comments-icon"></div></p>
<p class="comments-section">$$f(x)=1/(\sqrt{2\pi\sigma}) e^{\frac{-[(x-\mu)]^2}{(2 \sigma^2 )}} \tag{5.7}$$
高斯在webRTC中的使用：<div class="comments-icon"></div></p>
<p class="comments-section">$$f(x_k |Z，r_k)=1/\sqrt{2\pi} e^{-(x_k-u_z )^2/(2\sigma^2 )} \tag {5.8}$$
x_k是选取的特征向量，webRTC中指x_k是六个子带的能量（子带是80~250Hz，250~500Hz，500Hz~1K， 1~2K，2~3K，3~4KHz，变量feature_vector存放的就是子带能量序列），r_k是均值u_z和方差σ的参数结合，这两个参数决定了高斯分布的概率。$$Z=0$$情况是计算噪声的概率，$$Z=1$$是计算是语音的概率。<div class="comments-icon"></div></p>
<p class="comments-section">这里采用最高频率是4KHz的原因是，webRTC中程序将输入（48KHz，32HKz，16KHz）都下采样到8KHz，这样根据奎斯特频率定理，有用的频谱就是4KHz以下。<div class="comments-icon"></div></p>
<p class="comments-section">当然也可以采用8KHz截止频率，这样就需要自己训练和修改高斯模型的参数了，这个算法我试过，要比基于DNN的方法好用，灵活性大些，体现在参数自适应更新上，举例来说，在夜晚安静家庭场景中，噪声的均值就比较低的，白天周边环境噪声多了时，噪声特征的均值也会随之调整，针对DNN的方法，参数一旦训练完毕，那么适用的场景的就定了，如果要增大适用场景，首先要收集目标场景的数据，标注好的数据重新训练（通常要增加参数数量），这样的过程会导致<div class="comments-icon"></div></p>
<ol>
<li>数据收集成本高，</li>
<li>参数过多计算代价大（VAD一般是一直工作的）。</li>
</ol>
<h3 id="算法流程">算法流程</h3>
<ul>
<li>1.设置VAD激进模式</li>
</ul>
<p class="comments-section">共四种模式，用数字0~3来区分，激进程度与数值大小正相关。
0: Normal，1：low Bitrate， 2：Aggressive；3：Very Aggressive
这些激进模式是和以下参数是息息相关的。<div class="comments-icon"></div></p>
<pre><code>&lt;comman_audio/vad/vad_core.c&gt;
// Mode 0, Quality.
static const int16_t kOverHangMax1Q[3] = { 8, 4, 3 };
static const int16_t kOverHangMax2Q[3] = { 14, 7, 5 };
static const int16_t kLocalThresholdQ[3] = { 24, 21, 24 };
static const int16_t kGlobalThresholdQ[3] = { 57, 48, 57 };
// Mode 1, Low bitrate.
static const int16_t kOverHangMax1LBR[3] = { 8, 4, 3 };
static const int16_t kOverHangMax2LBR[3] = { 14, 7, 5 };
static const int16_t kLocalThresholdLBR[3] = { 37, 32, 37 };
static const int16_t kGlobalThresholdLBR[3] = { 100, 80, 100 };
// Mode 2, Aggressive.
static const int16_t kOverHangMax1AGG[3] = { 6, 3, 2 };
static const int16_t kOverHangMax2AGG[3] = { 9, 5, 3 };
static const int16_t kLocalThresholdAGG[3] = { 82, 78, 82 };
static const int16_t kGlobalThresholdAGG[3] = { 285, 260, 285 };
// Mode 3, Very aggressive.
static const int16_t kOverHangMax1VAG[3] = { 6, 3, 2 };
static const int16_t kOverHangMax2VAG[3] = { 9, 5, 3 };
static const int16_t kLocalThresholdVAG[3] = { 94, 94, 94 };
static const int16_t kGlobalThresholdVAG[3] = { 1100, 1050, 1100 };
</code></pre><p class="comments-section">它们在计算高斯模型概率时用到。<div class="comments-icon"></div></p>
<ul>
<li>2.帧长设置
A）  共有三种帧长可以用到，分别是80/10ms，160/20ms，240/30ms，实际上目前只支持10ms的帧长。
B）  其它采样率的48k，32k，24k，16k会重采样到8k来计算VAD。
 之所以选择上述三种帧长度，是因为语音信号是短时平稳信号，其在10ms~30ms之间可看成平稳信号，高斯马尔科夫等信号处理方法基于的前提是信号是平稳的，在10ms~30ms，平稳信号处理方法是可以使用的。</li>
<li>3.高斯模型中特征向量选取
在WebRTC的VAD算法中用到了聚类的思想，只有两个类，一个类是语音，一个类是噪声，对每帧信号都求其是语音和噪声的概率，根据概率进行聚类，当然为了避免一帧带来的误差也有一个统计量判决在算法里，那么问题来了，选择什么样的特征作为高斯分布的输入呢?这关系到聚类结果的准确性，也即VAD性能，毋庸置疑，既然VAD目的是区分噪声和语音，那么噪声信号和语音信号这两种信号它们的什么特征相差最大呢?选择特征相差比较大自然能得到比较好的区分度。
众所周知，信号的处理分类主要有时域，频域和空域，从空域上看，webRTC的VAD是基于单麦克的，噪声和语音没有空间区分度的概念，在多麦克风场景，确实基于多麦克风的VAD算法，从时域上看，而者都是时变信号，且短时信号变化率比较小，所以推算来推算去只有频域的区分度可能是比较好的。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/vad_1.png" alt=""> <blockquote>
<p class="comments-section">图5.1 汽车噪声频谱
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/vad_2.png" alt="">
图5.2 粉红噪声频谱 
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/vad_3.png" alt="">
图5.3 白噪声频谱
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/vad_4.png" alt="">
语音声谱<div class="comments-icon"></div></p>
</blockquote>
</li>
</ul>
<p class="comments-section">从以上四个图中，可以看到从频谱来看噪声和语音，它们的频谱差异还是比较大，且以一个个波峰和波谷的形式呈现。
 WebRTC正式基于这一假设，将频谱分成了6个子带。它们是：
    80Hz~250Hz，250Hz~500Hz,500Hz~1K,1K~2K,2K~3K,3K~4K。
 分别对应于:
     feature[0],feature[1],feature[2],...,feature[5]。
可以看到以1KHz为分界，向下500HZ，250Hz以及170HZ三个段，向上也有三个段，每个段是1KHz，这一频段涵盖了语音中绝大部分的信号能量，且能量越大的子带的区分度越细致。
  我国交流电标准是220V~50Hz，电源50Hz的干扰会混入麦克风采集到的数据中且物理震动也会带来影响，所以取了80Hz以上的信号。
在webRTC计算的函数在filter_bank.c文件中，前面说的基于激活的DNN也可以是基于fbank特征。<div class="comments-icon"></div></p>
<ul>
<li>4.高通滤波器设计
高通滤波器的作用有两点：1.滤除直流分量，2提升高频成分（人耳对3.5KHz最为敏感）
```
  // High pass filtering, with a cut-off frequency at 80 Hz, if the |data_in| is<br>  // sampled at 500 Hz.<br>  //<br>  // - data_in      [i]   : Input audio data sampled at 500 Hz.<br>  // - data_length  [i]   : Length of input and output data.<br>  // - filter_state [i/o] : State of the filter.<br>  // - data_out     [o]   : Output audio data in the frequency interval<br>  //                        80 - 250 Hz.<br>  static void HighPassFilter(const int16_t* data_in, size_t data_length,  <pre><code>                         int16_t* filter_state, int16_t* data_out) {  
size_t i;  
const int16_t* in_ptr = data_in;  
int16_t* out_ptr = data_out;  
int32_t tmp32 = 0;  
</code></pre></li>
</ul>
<pre><code>  // The sum of the absolute values of the impulse response:  
  // The zero/pole-filter has a max amplification of a single sample of: 1.4546  
  // Impulse response: 0.4047 -0.6179 -0.0266  0.1993  0.1035  -0.0194  
  // The all-zero section has a max amplification of a single sample of: 1.6189  
  // Impulse response: 0.4047 -0.8094  0.4047  0       0        0  
  // The all-pole section has a max amplification of a single sample of: 1.9931  
  // Impulse response: 1.0000  0.4734 -0.1189 -0.2187 -0.0627   0.04532  

  for (i = 0; i &lt; data_length; i++) {  
    // All-zero section (filter coefficients in Q14).  
    tmp32 = kHpZeroCoefs[0] * *in_ptr;  
    tmp32 += kHpZeroCoefs[1] * filter_state[0];  
    tmp32 += kHpZeroCoefs[2] * filter_state[1];  
    filter_state[1] = filter_state[0];  
    filter_state[0] = *in_ptr++;  

    // All-pole section (filter coefficients in Q14).  
    tmp32 -= kHpPoleCoefs[1] * filter_state[2];  
    tmp32 -= kHpPoleCoefs[2] * filter_state[3];  
    filter_state[3] = filter_state[2];  
    filter_state[2] = (int16_t) (tmp32 &gt;&gt; 14);  
    *out_ptr++ = filter_state[2];  
  }  
}  
</code></pre><pre><code>WebRTC在设计该滤波器上还是很有技巧的，技巧有二：
1.    定点数计算，指两个方面，一是滤波系数量化，而是计算过程的定点化，高斯模型计算也使用了这一技巧。
2.    舍入技巧，减少运算量。
下面就来看看，这些技巧是如何使用的，首先根据代码的注释可以看出，
全零点和全极点脉冲响应的实际上是浮点数，它们脉冲响应分别是：
0.4047 -0.8094  0.4047  0       0        0
1.0000  0.4734 -0.1189 -0.2187 -0.0627   0.04532
所以可见应该是六阶方程，但是超过3阶后，零点全零，极点数值较小，这时适当增大第三个数值，达到减少计算次数的目的。


量化是按照2的十四次方进行定点化。这是因为最差情况下，零极点的放大倍数不超过两倍，所以16位数可以表示的下来。其零极点绘图如下：
![](/assets/vad5.png)
对这两个图的解释就忽略了，能够看懂上述代码和两张图的意义，就可以更改滤波器的特性了，对不要相位信息的，采用IIR比FIR达到相同的增益平坦度需要的阶数要少。频响如下:
![](/assets/vad6.png)
&gt;小技巧
常常在一些源代码中可以看到滤波器系数已经给定,如何根据滤波器系数查看频响函数.
</code></pre><p class="comments-section">b=1;
a=[...];
fvtool(b,a)<div class="comments-icon"></div></p>
<pre><code>WebRtcVad_CalculateFeatures函数计算每个子带的能量。能量结果存放在features数组里，然后调用GmmProbability计算概率。
</code></pre><p class="comments-section">int WebRtcVad_CalcVad8khz(VadInstT<em> inst, const int16_t</em> speech_frame,<br>                          size_t frame_length)<br>{<br>    int16_t feature_vector[kNumChannels], total_power;  <div class="comments-icon"></div></p>
<pre><code>// Get power in the bands  
total_power = WebRtcVad_CalculateFeatures(inst, speech_frame, frame_length,  
                                          feature_vector);  

// Make a VAD  
inst-&gt;vad = GmmProbability(inst, feature_vector, total_power, frame_length);  

return inst-&gt;vad;  
</code></pre><p>} </p>
<pre><code>###计算流程
高斯模型有两个参数H0和H1，它们分表示的是噪声和语音，判决测试使用LRT（likelihood ratio test）。分为全局和局部两种情况。
![](/assets/vad7.jpeg)
&gt;计算流程

* a)高斯概率计算采用的高斯公式如下：
这里其实采用了两个参数高斯分布，但是假设了这两个参数是相互独立的：
$$p(x,y)= \frac{1}{2\pi\sigma_1 \sigma_2 \sqrt{1-\rho^2}}\exp{[-\frac{1}{2(1-\rho^2)}  \begin{Bmatrix}
\frac{(x-u_1)^2}{\sigma_1^2} - \frac{2\rho(x-\mu_1)(y-\mu_2)}{\sigma_1 sigma_2}+\frac{(y-\mu_2)^2}{\sigma_2^2}
\end{Bmatrix}]} \tag{5.9}$$
为了减少计算量,将这两个搞事变量看成是不相关的,则有
$$p(x,y) = p(x)\cdot p(y) = \exp[-\frac{(x-\mu_1)^2}{\sigma_1^2}]\cdot \exp[- \frac{(x-\mu_2)^2}{\sigma_2^2}] \approx \exp[-\frac{(x-\mu_1)^2}{\sigma_1^2}]+ \exp[-\frac{(x-\mu_2)^2}{\sigma_2^2}]\tag {5.10}$$
这里可以看到做了三个简化，第一个是把指数前的系数省掉了，这是因为在做似然比检验时，可以消掉，第二个简化是假设这两个高斯分布是不相关的，第三个简化是将乘法近似化简成加法。使用高斯分布这个在kaldi中也有例子，kaldi中基于GMM的语言模型分成三种，第一种是类似这里的独立同分布模型，第二种是互相关对角阵元素非零，第三种是全高斯互相关模型。
用多个高斯来近似一个语音包络这种方法，在插值时也用到，即用多个sinc函数进行分数位插值。
这样$$p(x) = \frac{1}{\sigma} \exp {- \frac{(x-\mu)^2}{2\sigma^2}}$$,其省略了$$\frac{1}{\sqrt{2\pi}}$$,在做似然比计算时,省略值不产生计算误差.

* b)对每一个子带(特征), 计算二元高斯对数似然比.
$$L(x,i) = log \begin{pmatrix}
\frac{P_s(x,i)}{P_n(x,i)} \approx

\frac{\exp\{-\frac{(x-\mu_{xs})^2}{\sigma_{xs}^2\}}
+ \exp\{-\frac{(x-\mu_{ys)^2}}{\sigma_{ys}^2}\}}
{\exp\{-\frac{(x-\mu_{xn})^2}{\sigma_{xn}^2\}}
+ \exp\{-\frac{(x-\mu_{yn)^2}}{\sigma_{yn}^2}\}
}
\end{pmatrix} \tag{5.11}$$
其中,x,y是输入的两个特征,$$\mu_{xs}$$和$$\mu_{ys}$$是对应语音一个子带的两个均值,同理$$\mu_{xn}$$和$$\mu_{yn}$$是一个子带内的两个均值.这是假设x和y是不相关得到的联合概率.
* c)对数似然比
分为全局和局部，全局是六个子带之加权之和，而局部是指每一个子带则是局部，所以语音判决会先判断子带，子带判断没有时会判断全局，只要有一方过了，就算有语音，公式表达如下：
$$L_t(x,y; i) = \sum_{i=1}^6 k_iL_i(x,y;i)$$
$$L_t$$是似然比加权之和,$$L_i$$的似然比.
$$F_{vad} = \left\{\begin{matrix}
1 &amp; L_i &gt; T_{tau} || L_t &gt; T_a\\ 
0 &amp; else
\end{matrix}\right.
\tag{5.12}
$$
###和判决准则相关的参数在vad_core.c文件.
</code></pre><pre><code>// Spectrum Weighting  
static const int16_t kSpectrumWeight[kNumChannels] = { 6, 8, 10, 12, 14, 16 };  
static const int16_t kNoiseUpdateConst = 655; // Q15  
static const int16_t kSpeechUpdateConst = 6554; // Q15  
static const int16_t kBackEta = 154; // Q8  
// Minimum difference between the two models, Q5  
static const int16_t kMinimumDifference[kNumChannels] = {  
    544, 544, 576, 576, 576, 576 };  
// Upper limit of mean value for speech model, Q7  
static const int16_t kMaximumSpeech[kNumChannels] = {  
    11392, 11392, 11520, 11520, 11520, 11520 };  
// Minimum value for mean value  
static const int16_t kMinimumMean[kNumGaussians] = { 640, 768 };  
// Upper limit of mean value for noise model, Q7  
static const int16_t kMaximumNoise[kNumChannels] = {  
    9216, 9088, 8960, 8832, 8704, 8576 };  
// Start values for the Gaussian models, Q7  
// Weights for the two Gaussians for the six channels (noise)  
static const int16_t kNoiseDataWeights[kTableSize] = {  
    34, 62, 72, 66, 53, 25, 94, 66, 56, 62, 75, 103 };  
// Weights for the two Gaussians for the six channels (speech)  
static const int16_t kSpeechDataWeights[kTableSize] = {  
    48, 82, 45, 87, 50, 47, 80, 46, 83, 41, 78, 81 };  
// Means for the two Gaussians for the six channels (noise)  
static const int16_t kNoiseDataMeans[kTableSize] = {  
    6738, 4892, 7065, 6715, 6771, 3369, 7646, 3863, 7820, 7266, 5020, 4362 };  
// Means for the two Gaussians for the six channels (speech)  
static const int16_t kSpeechDataMeans[kTableSize] = {  
    8306, 10085, 10078, 11823, 11843, 6309, 9473, 9571, 10879, 7581, 8180, 7483  
};  
// Stds for the two Gaussians for the six channels (noise)  
static const int16_t kNoiseDataStds[kTableSize] = {  
    378, 1064, 493, 582, 688, 593, 474, 697, 475, 688, 421, 455 };  
// Stds for the two Gaussians for the six channels (speech)  
static const int16_t kSpeechDataStds[kTableSize] = {  
    555, 505, 567, 524, 585, 1231, 509, 828, 492, 1540, 1079, 850 };  
</code></pre><p>```</p>
<h3 id="参数跟新">参数跟新</h3>
<p class="comments-section">噪声均值更新，WebRtcVad_FindMinimum函数对每个特征，求出100个帧里头的前16个最小值。每个最小值都对应一个age，最大不超过100，超过100则失效，用这个最小值来跟新噪声。<div class="comments-icon"></div></p>
<p class="comments-section">$$u(n) = \alpha<em>{n1}u</em>{n1}(n-1) + \alpah<em>{n2}u</em>{n2}(n-1)\tag {5.13}$$
模型参数跟新<div class="comments-icon"></div></p>
<p class="comments-section">跟新噪声均值，语音均值，噪声方差，语音方差，自适应也就体现在这里。<div class="comments-icon"></div></p>
<p class="comments-section">噪声跟新：只在非语音帧进行跟新，
噪声均值更新:<div class="comments-icon"></div></p>
<p class="comments-section">$$u<em>{nj}(n) = u</em>{nj}(n-1) + [1-F<em>{vad}(n)]\cdot K</em>{\triangle} \cdots \nabla<em>{u</em>{nj}}$$
语言模型均值更新:<div class="comments-icon"></div></p>
<p class="comments-section">$$u<em>{sj}(n) = u</em>{sj}(n-1) + [1 - F<em>{vad}(n)] \cdot</em>{\triangle<em>n} \cdot \nabla</em>{\sigma_{nj}} \cdot \frac{p(x(n)| H_0)}{p_0(x(n)|H_0) + p_0(x(n)|H_1)}$$
语音模型方差更新:<div class="comments-icon"></div></p>
<p class="comments-section">$$\sigma<em>{sj}(n) = \sigma</em>{sj}(n-1) + F<em>{vad}(n) \cdot C</em>{\triangle<em>n} \cdot \nabla</em>{\sigma_{sj}} \frac{p(x(n)| H_1)}{p_0(x(n)|H_0) + p_0(x(n)|H_1)}$$<div class="comments-icon"></div></p>
<h2 id="本章小节">本章小节</h2>
<p class="comments-section">本章主要概述了VAD方法的所使用的语音帧的特征,然后简述了三种判决准则,基于门限(滑窗以及自适应门限调整),统计模型方法(主要给出了高斯模型,当然有文献显示拉普拉斯在混响场景下更好),基于NN的方法并未做过多讨论,在ASR识别中会引入这一方法.
最后关于区分语音/非语音帧的初始参数方法EM算法.<div class="comments-icon"></div></p>
<h2 id="噪声抑制ns原理和实例">噪声抑制(NS)原理和实例</h2>
<p class="comments-section">NS(noise suppression),有些文献也称之为NR(noise reduction),本章给出的NS的算法(依然基于webRTC)对于ASR而言是有损的,需要优化后在进行做为ASR系统的NS模块.
本章和VAD那章有相似的原理和假设,公式上会减少很多.<div class="comments-icon"></div></p>
<h2 id="算法原理">算法原理</h2>
<p class="comments-section">该算法的核心思想是采用维纳滤波器抑制估计出来的噪声。<div class="comments-icon"></div></p>
<p class="comments-section">$$y(t) = x(t) + n(t) \tag {6.1}$$
上式中和分别表示语音和噪声，而表示麦克风采集到的信号。经过频谱变换之后得到：<div class="comments-icon"></div></p>
<p class="comments-section">$$Y(\omega) \approx (X(\omega)+N(\omega) \tag {6.2}$$
上式中假设了噪声和语音是不相干的,在相干噪声场景有使用sinc函数模型也有使用贝塞尔函数模型做为相干噪声场模型的,通过后置滤波加以滤除.
它们的频谱关系如上式6.2，可以看出语音和噪声是加性且不相关的关系，对于非加性关系有回声消除（Automatic echo cancellation）等算法对不同场景进行抑制。根据中心极限定义，一般认为噪声和语音分布服从均值为0，方差为$$\mu$$的正态分布。但是也有采用$$L(0, \sigma)$$和$$\Gamma(k, \theta) $$分布的情况。
这里的中心思想就是Y从中估计噪声N，然后抑制噪声以获得语音，即：<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat X(\omega) = Y(\omega) - \hat N(\omega) \tag{6.4}$$
所以对噪声的估计准确性是至关重要的，噪声估计的越准得到的结果就越好，由此又多出来几种估计噪声的方法。<div class="comments-icon"></div></p>
<ol>
<li>基于VAD检测的噪声估计，VAD对进行检测，如果检测没有语音，则认为噪声，这是对噪声的一种估计方法。
2.基于全局幅度谱最小原理，该估计认为幅度谱最小的情况必然对应没有语音的时候。
3.还有基于矩阵奇异值分解原理估计噪声的
webRTC没有采用上述的方法，而是对似然比（VAD检测时就用了该方法）函数进行改进，将多个语音/噪声分类特征合并到一个模型中形成一个多特征综合概率密度函数，对输入的每帧频谱进行分析。其可以有效抑制风扇/办公设备等噪声。</li>
</ol>
<p class="comments-section">其抑制过程如下：
   对接收到的每一帧带噪语音信号，以对该帧的初始噪声估计为前提，定义语音概率函数，测量每一帧带噪信号的分类特征，使用测量出来的分类特征，计算每一帧基于多特征的语音概率，在对计算出的语音概率进行动态因子（信号分类特征和阈值参数）加权，根据计算出的每帧基于特征的语音概率，修改多帧中每一帧的语音概率函数，以及使用修改后每帧语音概率函数，更新每帧中的初始噪声（连续多帧中每一帧的分位数噪声）估计。<div class="comments-icon"></div></p>
<p class="comments-section">基于特征的语音概率函数通过使用映射函数（sigmoid/tanh又称S函数，在神经元分类算法中常用为种子函数）将每帧的信号分类特征映射到一个概率值而得出的。<div class="comments-icon"></div></p>
<p class="comments-section">分类特征包括：随时间变化的平局似然比，频谱平坦度测量以及频谱模板差异测量。频谱模板差异测量以输入信号频谱与模板噪声频谱的对比为基础。<div class="comments-icon"></div></p>
<p class="comments-section">信号分析：包括缓冲、加窗和离散傅立叶变换(DFT) 的预处理步骤<div class="comments-icon"></div></p>
<p class="comments-section">噪声估计和过滤包括:初始噪声估计、后验和先验SNR的判决引导(DD)更新、语音/噪声可能性测定，可能性测定是基于似然比(LR)因子进行的，而似然比是使用后验和先验SNR，以及语音概率密度函数(HF)模型 (如高斯、拉普拉斯算子、伽马、超高斯等)，还有根据特征建模、噪声估计更新并应用维纳增益滤波器确定的概率而确定的。<div class="comments-icon"></div></p>
<p class="comments-section">信号合成：离散傅立叶逆变换、缩放和窗口合成。<div class="comments-icon"></div></p>
<p class="comments-section">初始噪声估计是以分位数噪声估计为基础。噪声估计受分位数参数控制，该参数以q表示。根据初始噪声估计步骤确定的噪声估计，仅能用作促进噪声更新/估计的后续流程的初始条件。<div class="comments-icon"></div></p>
<h3 id="setfeatureextractionparameters">set_feature_extraction_parameters</h3>
<p class="comments-section">设置了特征提取使用到的参数，当前WebRTC噪声抑制算法使用了LRT特征/频谱平坦度和频谱差异度三个指标，没有使用频谱熵和频谱方差这两个特征。<div class="comments-icon"></div></p>
<h2 id="webrtcnsinitcore">WebRtcNs_InitCore</h2>
<p class="comments-section">NS（noise suppression）模块初始化，后面的代码按fs=8000，来分析.<div class="comments-icon"></div></p>
<pre><code class="lang-c"><span class="hljs-comment">//语音数据的长度，8k/10ms的数据量是80  </span>
self-&gt;blockLen = <span class="hljs-number">80</span>;  
<span class="hljs-comment">//分析长度，由于是在频域分析，将长度像上取2的幂次，最小的值是128，实际上是fft的长度。  </span>
self-&gt;anaLen = <span class="hljs-number">128</span>;  
<span class="hljs-comment">//窗函数，采用混合汉宁平顶窗函数  </span>
self-&gt;window = kBlocks80w128;
</code></pre>
<h3 id="初始化fft用到的相关存储成员">初始化FFT用到的相关存储成员</h3>
<pre><code> // Initialize FFT work arrays.  
  self-&gt;ip[0] = 0;  // Setting this triggers initialization.  
  memset(self-&gt;dataBuf, 0, sizeof(float) * ANAL_BLOCKL_MAX);  
  WebRtc_rdft(self-&gt;anaLen, 1, self-&gt;dataBuf, self-&gt;ip, self-&gt;wfft);  

//是滑动分析窗，针对80点128的fft而言，每一次会保留前一帧的128-80=48个点的数据，而不是对80点简单填充0变成128点做fft。  
//但这会带来合成上的问题，通常采用加窗以防止重叠带来的突变。可以使用做fft变换一样的窗函数。但这要求窗函数保幂映射，即重叠  
//区部分窗口的平方和必须为1.  
 memset(self-&gt;analyzeBuf, 0, sizeof(float) * ANAL_BLOCKL_MAX);  
//dataBuf存储的是原始时域信号  
  memset(self-&gt;dataBuf, 0, sizeof(float) * ANAL_BLOCKL_MAX);  
//syntBuf是谱减法，减去噪声后变换到时域的信号  
  memset(self-&gt;syntBuf, 0, sizeof(float) * ANAL_BLOCKL_MAX);  

  // For HB processing. 这是高频部分，最多有两个band，  
  memset(self-&gt;dataBufHB,  
         0,  
         sizeof(float) * NUM_HIGH_BANDS_MAX * ANAL_BLOCKL_MAX);
</code></pre><h3 id="初始化分位数估计用到的变量">初始化分位数估计用到的变量</h3>
<pre><code>  // For quantile noise estimation.  
  memset(self-&gt;quantile, 0, sizeof(float) * HALF_ANAL_BLOCKL);  
//3帧同步估计，lquantile是对数分位数。density是概率密度，计算分位数用到概率密度的。  
 for (i = 0; i &lt; SIMULT * HALF_ANAL_BLOCKL; i++) {  
    self-&gt;lquantile[i] = 8.f;  
    self-&gt;density[i] = 0.3f;  
  }  

  for (i = 0; i &lt; SIMULT; i++) {  
//我的理解counter是一个权值，代表的每一帧对分位数估计而言其所占的比重。  
  self-&gt;counter[i] =  
        (int)floor((float)(END_STARTUP_LONG * (i + 1)) / (float)SIMULT);  
  }  

  self-&gt;updates = 0;
</code></pre><h3 id="维纳滤波器初始化">维纳滤波器初始化</h3>
<pre><code>    for (i = 0; i &lt; HALF_ANAL_BLOCKL; i++) {  
      self-&gt;smooth[i] = 1.f;  
    }
</code></pre><h3 id="设置抑制噪声的激进度">设置抑制噪声的激进度</h3>
<pre><code>// Set the aggressiveness: default.  
self-&gt;aggrMode = 0;
</code></pre><h3 id="噪声估计使用到的">噪声估计使用到的</h3>
<pre><code>    // Initialize variables for new method.  
    self-&gt;priorSpeechProb = 0.5f;  // Prior prob for speech/noise.  
    // Previous analyze mag spectrum.  
    memset(self-&gt;magnPrevAnalyze, 0, sizeof(float) * HALF_ANAL_BLOCKL);  
    // Previous process mag spectrum.  
    memset(self-&gt;magnPrevProcess, 0, sizeof(float) * HALF_ANAL_BLOCKL);  
    // Current noise-spectrum.  
    memset(self-&gt;noise, 0, sizeof(float) * HALF_ANAL_BLOCKL);  
    // Previous noise-spectrum.  
    memset(self-&gt;noisePrev, 0, sizeof(float) * HALF_ANAL_BLOCKL);  
    // Conservative noise spectrum estimate.  
    memset(self-&gt;magnAvgPause, 0, sizeof(float) * HALF_ANAL_BLOCKL);  
    // For estimation of HB in second pass.  
    memset(self-&gt;speechProb, 0, sizeof(float) * HALF_ANAL_BLOCKL);  
    // Initial average magnitude spectrum.  
    memset(self-&gt;initMagnEst, 0, sizeof(float) * HALF_ANAL_BLOCKL);  
    for (i = 0; i &lt; HALF_ANAL_BLOCKL; i++) {  
      // Smooth LR (same as threshold).  
      self-&gt;logLrtTimeAvg[i] = LRT_FEATURE_THR;  
    }
</code></pre><h3 id="特征量，计算噪声用到">特征量，计算噪声用到</h3>
<pre><code>// Feature quantities.  
  // Spectral flatness (start on threshold).  
  self-&gt;featureData[0] = SF_FEATURE_THR;  
  self-&gt;featureData[1] = 0.f;  // Spectral entropy: not used in this version.  
  self-&gt;featureData[2] = 0.f;  // Spectral variance: not used in this version.  
  // Average LRT factor (start on threshold).  
  self-&gt;featureData[3] = LRT_FEATURE_THR;  
  // Spectral template diff (start on threshold).  
  self-&gt;featureData[4] = SF_FEATURE_THR;  
  self-&gt;featureData[5] = 0.f;  // Normalization for spectral difference.  
  // Window time-average of input magnitude spectrum.  
  self-&gt;featureData[6] = 0.f;  

  // Histogram quantities: used to estimate/update thresholds for features.  
  memset(self-&gt;histLrt, 0, sizeof(int) * HIST_PAR_EST);  
  memset(self-&gt;histSpecFlat, 0, sizeof(int) * HIST_PAR_EST);  
  memset(self-&gt;histSpecDiff, 0, sizeof(int) * HIST_PAR_EST);  

  // Update flag for parameters:  
  // 0 no update, 1 = update once, 2 = update every window.  
  self-&gt;modelUpdatePars[0] = 2;  
  self-&gt;modelUpdatePars[1] = 500;  // Window for update.  
  // Counter for update of conservative noise spectrum.  
  self-&gt;modelUpdatePars[2] = 0;  
  // Counter if the feature thresholds are updated during the sequence.  
  self-&gt;modelUpdatePars[3] = self-&gt;modelUpdatePars[1];
</code></pre><h3 id="白噪声和粉红噪声">白噪声和粉红噪声</h3>
<pre><code>    self-&gt;signalEnergy = 0.0;  
    self-&gt;sumMagn = 0.0;  
    self-&gt;whiteNoiseLevel = 0.0;  
    self-&gt;pinkNoiseNumerator = 0.0;  
    self-&gt;pinkNoiseExp = 0.0;
</code></pre><h3 id="computespectralflatness函数">ComputeSpectralFlatness函数</h3>
<p class="comments-section">频谱平坦度计算，该算法假设语音比噪声有更多的谐波。语音频谱往往会在基频（基音）和谐波中出现峰值，而噪声频谱则相对平坦。因此其作为区分噪声和语音的一个特征。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/6_1.png" alt=""><div class="comments-icon"></div></p>
<p class="comments-section">频谱度计算时N表示STFT后频率点数，B代表频率带的数量，K是频点指数，j是频带指数。每个频带包括大量的频率点。就128个频率点可分成4个频带（低带，中低频带，中高频带，高频），每个频带32个频点。对于噪声Flatness偏大且为常数，而对于语音，计算出的数量则偏下且为变量。这四个频段对于语音信号差异是比较大的，对于噪声是比较小的，根据上面的公式，如果接近于1，则是噪声，（噪声的幅度谱趋于平坦），二对于语音，上面的N次根是对乘积结果进行N次缩小，相比于分母部分，缩小的数量级是倍数的，所以语音的平坦度较小，是趋近于0的。<div class="comments-icon"></div></p>
<pre><code class="lang-c"><span class="hljs-comment">// Compute spectral flatness on input spectrum.  </span>
<span class="hljs-comment">// |magnIn| is the magnitude spectrum.  </span>
<span class="hljs-comment">// Spectral flatness is returned in self-&gt;featureData[0].  </span>
<span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">ComputeSpectralFlatness</span><span class="hljs-params">(NoiseSuppressionC* self,  
                                    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* magnIn)</span> </span>{  
  <span class="hljs-keyword">size_t</span> i;  
  <span class="hljs-keyword">size_t</span> shiftLP = <span class="hljs-number">1</span>;  <span class="hljs-comment">// Option to remove first bin(s) from spectral measures.  </span>
  <span class="hljs-keyword">float</span> avgSpectralFlatnessNum, avgSpectralFlatnessDen, spectralTmp;  

  <span class="hljs-comment">// Compute spectral measures.  </span>
  <span class="hljs-comment">// For flatness.  </span>
  avgSpectralFlatnessNum = <span class="hljs-number">0.0</span>;  
  avgSpectralFlatnessDen = self-&gt;sumMagn;  
  <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; shiftLP; i++) {   
<span class="hljs-comment">//跳过第一个频点，即直流频点Den是denominator（分母）的缩写，avgSpectralFlatnessDen是上述公式分母计算用到的  </span>
    avgSpectralFlatnessDen -= magnIn[i];  
  }  
  <span class="hljs-comment">// Compute log of ratio of the geometric to arithmetic mean: check for log(0) case.  </span>
  <span class="hljs-comment">// 计算分子部分，numerator(分子)，对log(0)是无穷小的值，所以计算时对这一情况特殊处理。  </span>
  <span class="hljs-keyword">for</span> (i = shiftLP; i &lt; self-&gt;magnLen; i++) {  
    <span class="hljs-keyword">if</span> (magnIn[i] &gt; <span class="hljs-number">0.0</span>) {  
      avgSpectralFlatnessNum += (<span class="hljs-keyword">float</span>)<span class="hljs-built_in">log</span>(magnIn[i]);  
} <span class="hljs-keyword">else</span> {  
<span class="hljs-comment">//TVAG是time-average的缩写，对于能量出现异常的处理。利用前一次平坦度直接取平均返回。这里平滑因子是0.3.  </span>
      self-&gt;featureData[<span class="hljs-number">0</span>] -= SPECT_FL_TAVG * self-&gt;featureData[<span class="hljs-number">0</span>];  
      <span class="hljs-keyword">return</span>;  
    }  
  }  
  <span class="hljs-comment">// Normalize.  </span>
  avgSpectralFlatnessDen = avgSpectralFlatnessDen / self-&gt;magnLen;  
  avgSpectralFlatnessNum = avgSpectralFlatnessNum / self-&gt;magnLen;  

  <span class="hljs-comment">// Ratio and inverse log: check for case of log(0).  </span>
  spectralTmp = (<span class="hljs-keyword">float</span>)<span class="hljs-built_in">exp</span>(avgSpectralFlatnessNum) / avgSpectralFlatnessDen;  

  <span class="hljs-comment">// Time-avg update of spectral flatness feature.  </span>
  self-&gt;featureData[<span class="hljs-number">0</span>] += SPECT_FL_TAVG * (spectralTmp - self-&gt;featureData[<span class="hljs-number">0</span>]);  
  <span class="hljs-comment">// Done with flatness feature.  </span>
}
</code></pre>
<h3 id="computespectraldifference函数">ComputeSpectralDifference函数</h3>
<p class="comments-section">有关噪声频谱的另一个假设是，噪声频谱比语音频谱更稳定。因此，可假设噪声频谱的整体形状在任何给定阶段都倾向于保持相同。这第三个特征用于测量输入频谱与噪声频谱形状的偏差。<div class="comments-icon"></div></p>
<p class="comments-section">计算公式变成如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$avgDiffNormMagn = var(magnIn) - cov(magnIn, magnAvgPause)^2 / var(magnAvgPause)$$<div class="comments-icon"></div></p>
<h3 id="computesnr函数">ComputeSnr函数</h3>
<p class="comments-section">根据分位数噪声估计计算前后信噪比。<div class="comments-icon"></div></p>
<p class="comments-section">后验信噪比指观测到的能量与噪声功率相关的输入功率相比的瞬态SNR：
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/62.png" alt="">
其中Y是输入含噪声的频谱，见公式1.N是噪声频谱，先验SNR是与噪声功率相关的纯净（未必是语音）信号功率的期望值，可表示为：
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/63.png" alt="">
其中X是指输入的纯净信号，这里对应的指语音信号。在WebRTC实际的计算中，并没有采用平方数量级，而是采用了量级数量级。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/64.png" alt="">
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/65.png" alt="">
由于纯净信号是未知信号，先验SNR的估计是上一帧经估计的先验SNR和瞬态SNR $$\sigma<em>k(m)$$的平均值:
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/66.png" alt="">
上式中H对应与代码中的smooth是上一帧的维纳滤波器，用于对瞬时SNR平滑，前一项是上一帧的先验SNR，后一项是先验SNR的瞬态估计。其通过判决应道DD进行跟新，时间平滑参数是$$\gamma</em>{dd}$$其值越大，流畅度越高，延迟也会越大，程序中选择的是0.98.<div class="comments-icon"></div></p>
<pre><code class="lang-c">    <span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">ComputeSnr</span><span class="hljs-params">(<span class="hljs-keyword">const</span> NoiseSuppressionC* self,  
                           <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* magn,  
                           <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* noise,  
                           <span class="hljs-keyword">float</span>* snrLocPrior,  
                           <span class="hljs-keyword">float</span>* snrLocPost)</span> </span>{  
      <span class="hljs-keyword">size_t</span> i;  

      <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; self-&gt;magnLen; i++) {  
        <span class="hljs-comment">// Previous post SNR.  </span>
        <span class="hljs-comment">// Previous estimate: based on previous frame with gain filter. 此处公式7加了平滑，对应公式8的前半部分  </span>
        <span class="hljs-keyword">float</span> previousEstimateStsa = self-&gt;magnPrevAnalyze[i] /  
            (self-&gt;noisePrev[i] + <span class="hljs-number">0.0001f</span>) * self-&gt;smooth[i];  
        <span class="hljs-comment">// Post SNR.  </span>
        snrLocPost[i] = <span class="hljs-number">0.f</span>;  
        <span class="hljs-keyword">if</span> (magn[i] &gt; noise[i]) {  
          snrLocPost[i] = magn[i] / (noise[i] + <span class="hljs-number">0.0001f</span>) - <span class="hljs-number">1.f</span>;<span class="hljs-comment">//实际上magn中包括语音和noise，减一是将噪声减去，获得后验snr  </span>
        }  
        <span class="hljs-comment">// DD estimate is sum of two terms: current estimate and previous estimate.  </span>
        <span class="hljs-comment">// Directed decision update of snrPrior.  </span>
        snrLocPrior[i] =  
            DD_PR_SNR * previousEstimateStsa + (<span class="hljs-number">1.f</span> - DD_PR_SNR) * snrLocPost[i];<span class="hljs-comment">//此处是公式8的计算  </span>
      }  <span class="hljs-comment">// End of loop over frequencies.  </span>
    }
</code></pre>
<h3 id="speechnoiseprob函数">SpeechNoiseProb函数</h3>
<pre><code class="lang-c">    该函数参数的意义。  
    <span class="hljs-comment">// |magn| is the input magnitude spectrum.输入信号幅度谱，包括信号和噪声  </span>
    <span class="hljs-comment">// |noise| is the noise spectrum.  </span>
    以下两个概率由ComputeSnr函数计算得到。  
    <span class="hljs-comment">// |snrLocPrior| is the prior SNR for each frequency.  </span>
    <span class="hljs-comment">// |snrLocPost| is the post SNR for each frequency.</span>
</code></pre>
<p class="comments-section">代码中和第一个特征相关的计算是：<div class="comments-icon"></div></p>
<pre><code class="lang-c">    <span class="hljs-comment">// Compute feature based on average LR factor.  </span>
    <span class="hljs-comment">// This is the average over all frequencies of the smooth log LRT.  </span>
    logLrtTimeAvgKsum = <span class="hljs-number">0.0</span>;  
    <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; self-&gt;magnLen; i++) {  
      tmpFloat1 = <span class="hljs-number">1.f</span> + <span class="hljs-number">2.f</span> * snrLocPrior[i];  
      tmpFloat2 = <span class="hljs-number">2.f</span> * snrLocPrior[i] / (tmpFloat1 + <span class="hljs-number">0.0001f</span>);  
      besselTmp = (snrLocPost[i] + <span class="hljs-number">1.f</span>) * tmpFloat2;  
      self-&gt;logLrtTimeAvg[i] +=  
          LRT_TAVG * (besselTmp - (<span class="hljs-keyword">float</span>)<span class="hljs-built_in">log</span>(tmpFloat1) - self-&gt;logLrtTimeAvg[i]);  
      logLrtTimeAvgKsum += self-&gt;logLrtTimeAvg[i];  
    }  
    logLrtTimeAvgKsum = (<span class="hljs-keyword">float</span>)logLrtTimeAvgKsum / (self-&gt;magnLen);  
    self-&gt;featureData[<span class="hljs-number">3</span>] = logLrtTimeAvgKsum;
</code></pre>
<p class="comments-section">要看懂上述代码所蕴含的物理和数学意义，必须先看动下面的推导。
计算speech/nosie的probability。该概率返回在probSpeechFinal参数中,先推到语音/噪声概率计算方法，先来看语音/噪声的概率模型，定义语音状态为$$H^{k,m} = H_1^{k,m}$$, 定义噪声状态为$$H^{k,m} = H_0^{k,m}$$，其中m是帧，k是频率。则语音/噪声的概率可表示为：<div class="comments-icon"></div></p>
<p class="comments-section">$$P(H^{k,m}|Y_k(m), {F})$$
这一概率取决于观测到的额噪声输入频谱系数$$Y_k(m)$$以及所处理信号的一些特征数据（如信号的分类特征），也就是这里的{F}。特征数据可以是有噪输入频谱，过往频谱数据，模型数据等。如特征数据{F}可以包括频谱平坦度测量，谐振峰值距，LPC残余以及模板匹配等。根据贝叶斯准则，语音/噪声概率可表示为：<div class="comments-icon"></div></p>
<p class="comments-section">$$P(H^{k,m}) = P(H|Y<em>k(m), {F}) \alpha P(Y_K(m)|H,{F})q</em>{k,m}(H|{F})P({F}) \tag {6.8}$$
其中$$p({F})$$是以信号的特征数据为基础的先验概率,该值在下方一个或多个表达式中被设置为一个常数.数量$$q<em>{k,m}(H|{F})$$是特征数据$${F}$$下的语音/噪声概率, 在忽略$${F}$$为基础的先验概率$$p{F}$$,简化$$q</em>{k,m}(H<em>1|{F}) = q$$以及$$q</em>{k,m}(H_0|{F}) = 1 - q$$,则标准化的语音概率可写作为:<div class="comments-icon"></div></p>
<p class="comments-section">$$P(H_1^{k,m}|Y_k(m), {F}) = \frac{P(Y_k(m)|H_1,{F})q}{P(Y_k(m)|H_1,{F})q + P(Y_k(m)|H_0,{F})(1-q)} \tag {6.9}$$
上式简写为:<div class="comments-icon"></div></p>
<p class="comments-section">$$P(H_1^{k,m}|Y_k(m), {F}) =  \frac{q\Delta_k}{q\Delta_k + 1-q} \tag{6.10}$$
其中似然比(LR)为:<div class="comments-icon"></div></p>
<p class="comments-section">$$\Delta<em>k = 
\frac{P(Y_k(m)|H_1,{F})}{P(Y_k(m)|H_0,{F})}$$
上述$$\triangle_k$$的表达式中,$$P(Y_k(m)|H</em>{1,0},{F})$$通过线性模型和针对语音和噪声频谱系数的高斯概率密度函数（PDF）假设来确定。具体来说，有噪输入信号的线性模型表达式为：语音状态下：<div class="comments-icon"></div></p>
<p class="comments-section">$$Y_k(m) = X_k(m) + N_k(m), H=H_1$$
噪声状态下是:<div class="comments-icon"></div></p>
<p class="comments-section">$$Y_k(m) = N_k(m), H=H_0$$
假设高斯概率密度函数使用复杂系数$${X_k, N_k}$$,则数量$$P(Y_k(m) |H, {F})$$表示如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$P(Y_k(m)|H_0,{F}) = P(Y_k(m)|H_0)
 \infty
  \frac{1}{\left \langle |N_k|^2 \right \rangle}
  \exp\begin{Bmatrix}<div class="comments-icon"></div></p>
<ul>
<li>\frac{|Y_k|^2}{\left \langle |N_k|^2 \right \rangle}
\end{Bmatrix}$$</li>
</ul>
<p class="comments-section">$$P(Y_k(m)|H_1,{F}) = P(Y_k(m)|H_1)
 \infty
  \frac{1}{\left \langle |N_k|^2 \right \rangle + \left \langle |X_k|^2 \right \rangle}
  \exp\begin{Bmatrix}<div class="comments-icon"></div></p>
<ul>
<li>\frac{|Y<em>k|^2}{\left \langle |N_k|^2 \right \rangle + \left \langle |X_k|^2 \right \rangle}
\end{Bmatrix}$$
由于完全可以根据线性模型和高斯PDF假设确定概率，因此可将特征依赖从上述表达式中删除。这样，似然比$$\triangle_k$$变成:
$$
\Delta_k = \frac{P(Y_k(m)|H_1)}{P(Y_k(m)|H_0)} = 
\frac{\exp\begin{Bmatrix}
\frac{\rho_k(m)\sigma_k(m)}{(1+\rho_k(m))}
\end{Bmatrix}}{1+\rho_k(m)}
\tag {6.11}$$
其中,$$\rho_k(m)$$是未知信号的SNR（即先验SNR），$$\sigma_k(m)$$是频率K和帧m的后验SNR（即后验SNR或瞬态SNR）。在一个现实例子中，上述表达式中使用的先验SNR和后验SNR由量级定义进行估计，算式为：
$$\sigma_k(m) = |Y_k(m)|\left \langle |N_k(m)| \right \rangle$$
$$\rho_k(m) = |X_k(m)|\left \langle |N_k(m)| \right \rangle$$
根据上述表达式，语音/噪声状态概率可通过似然比$$\triangle_k$$和数量q获得，其中，似然比根据频变后验和先验SNR确定，数量是基于特征或基于模型的概率，详细描述参加下文。因此，语音/噪声状态概率可以表示为：
$$P(H_1|Y_k(m), {F}) = \frac{q\Delta_k}{q\Delta_k + 1 -q}$$
$$P(H_0Y_k(m), {F}) = 1 - P(H_1Y_k(m), {F})$$
有时帧到帧之间的频变似然比因子$$\triangle_k$$会有很大的波动,所以采用经过时间平滑处理的似然比因子：
$$\log(\tilde \Delta_k(m)) = \gamma</em>{lrt}\log(\tilde \Delta<em>k(m-1))+(1-\gamma</em>{lrt})\log(\Delta<em>k(m))$$
上面这个公式就是先前代码所做的事情，但是代码里又没有完全按照这里的公式推导来的。这里先用
$$\rho_k(m)$$和$$\sigma_k(m)$$对上述代码进行简化处理.
$$tmpFloat1 = 1 + 2\rho_k$$
$$tmpFloat2 = \frac{2\rho_k^2}{1 + 2\rho_k^2}$$
$$besselTmp = (1 + \sigma_k)\frac{2\rho_k}{1+2\rho_k}$$
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/67.png" alt="">
该式的最后一项是如下式子取对数所得。值得注意的是上述的代码并未完全按照公式计算得来。
$$\Delta_k = \frac{P(Y_k(m)|H_1)}{p(Y_k(m)|H_0} = 
\frac{\exp\begin{pmatrix}
\frac{\rho_k(m)\sigma_k(m)}{1+\rho_k(m)}
\end{pmatrix}  }{1+\rho_k(m)} \tag{6.12}
$$
经过时间平滑处理的似然比因子的几何平均数（包括所有频率）可用作对基于帧的语音/噪声分类的可靠测量结果：
$$\log\begin{pmatrix}
\prod \limits_k \tilde \Delta_k(m)
\end{pmatrix}^{1/N} = \frac{1}{N}\sum \limits</em>{k=1}^{N}\log(\tilde \Delta_k(m))  \tag {6.13}$$<pre><code>  logLrtTimeAvgKsum = (float)logLrtTimeAvgKsum / (self-&gt;magnLen);
</code></pre>在语音/噪声概率计算时，使用高斯假设作为语音PDF模型，从而获得似然比。在其它模型中，概率密度PDF模型也可以用作测量似然比的基础，包括拉普拉斯算子，伽马，超高斯。举个例子，当高斯假设可合理表示噪声时，该假设并不一定适用于语音，尤其是在较短的时帧中（如～10ms）。在这种情况下，可以使用另一种语音PDF模型，但这很可能会增加复杂性。
要在噪声估计和过滤流程中确定语音/噪声的概率，这不仅需要本地SNR（即先验SNR和瞬态SNR）的引导，还要结合从特征建模中获得的语音模型/认知内容。将语音模 型/认知内容并入到语音/噪声概率确定中，能让噪声抑制流程更好地处理和区分极不稳定的 噪声水平。如果仅依靠本地SNR，可能会造成可能性偏差。这里对包含本地SNR和语音特征/模型数据的每个帧和频率更新和适应基于特征的概率$$q<em>{k,m}(H|F)$$.可以简写为$$q</em>{k,m}(H|F) = q<em>{k,m}$$.因此此处所述流程仅以帧为基础对数量$$q</em>{k,m}(H|F)$$建模和更新，所以变量k就受到抑制。</li>
</ul>
<p class="comments-section">基于特征的概率的更新可采用一下模型：<div class="comments-icon"></div></p>
<p class="comments-section">$$q<em>m = \gamma_qq</em>{m-1} + (1 - \gamma_q)M(z,w) \tag{6.14}$$
其中,$$\gamma_q$$是一个平滑阐述,$$M(z)$$是给定时间和频率的映射函数(如在0和1之间)此映射函数中的变量Z是Z=F-T，其中F是被测特征，T是阈值。参数w则代表映射函数的形状/宽度特征。映射函数根据测量出的特征以及阈值和宽度参数，将时频槽划分为语音（M接近1）或噪声（M接近0）.<div class="comments-icon"></div></p>
<p class="comments-section">在噪声估计和过滤流程中，确定语音/噪声可能性时，会考虑语音信号的以下特征：（1）LRT均值，可以基于本地SNR得出，（2）频谱平坦度，可基于语音谐波模型得出，以及（3）频谱模板差异测量。还可以使用其它语音信号特征作为补充或替代特征。<div class="comments-icon"></div></p>
<p class="comments-section">1.LRT均值特征<div class="comments-icon"></div></p>
<p class="comments-section">经过时间平滑处理的似然比（LR）因子的几何平均数是语音/噪声状态的可靠指标：<div class="comments-icon"></div></p>
<p class="comments-section">$$F<em>1 = \log\begin{pmatrix}
\prod \limits_k \tilde \Delta(m)
\end{pmatrix}^{1/N} = \frac{1}{N} \sum \limits</em>{k=1}^{N}\log\begin{pmatrix}
 \tilde \Delta(m)
\end{pmatrix} \tag {6.15}$$
其中经过时间处理的LR因子根据前文所述表达式得出。使用LRT均值特征时，映射函数M（z）的一个示例可能是“S”型曲线函数，例如：<div class="comments-icon"></div></p>
<p class="comments-section">$$M(z) = 0.5 *(tanh(w_1z_1)+ 0.5)$$<div class="comments-icon"></div></p>
<pre><code class="lang-c"><span class="hljs-comment">// Compute indicator function: sigmoid map.  </span>
indicator0 =  
    <span class="hljs-number">0.5f</span> *  
    ((<span class="hljs-keyword">float</span>)<span class="hljs-built_in">tanh</span>(widthPrior * (logLrtTimeAvgKsum - threshPrior0)) + <span class="hljs-number">1.f</span>);
</code></pre>
<p class="comments-section">$$z=T_1 - F_1$$,其中$$F_1$$是特征,$$w_1$$是一个过渡/宽度参数,用于控制从0到1的映射的平滑性,阈值参数$$T_1$$需要根据参数设置来确定.<div class="comments-icon"></div></p>
<p class="comments-section">2.频谱平坦度特征<div class="comments-icon"></div></p>
<p class="comments-section"> 为获得频谱平坦度特征，假设语音比噪声有更多的谐波行为。然而，语音频谱往往会在基频（基音）和谐波中出现峰值，而噪声频谱则相对平坦。因此，至少在某些布置中，本地频谱平坦度测量的综合可用做区分语音和噪声的良好判断依据。<div class="comments-icon"></div></p>
<p class="comments-section">在计算频谱平坦度时，N代表频率槽的数量，B代表频率带的数量。k是频率槽指数，j是频率带指数。每个频率带将包括大量的频率槽。举例来说，128槽的频率频谱可分成4个频率带（低，中低，中高，高）。每个频率带包括32个槽。在另一个示例中，仅使用一个包括所有频率的频率带。频谱平坦度可以通过计算输入幅度谱的几何平均数与算术平均数的比值得出：<div class="comments-icon"></div></p>
<p class="comments-section">$$F_2 = \frac{\prod \limits_k |Y_k|^{1/N}}{\frac{1}{N}\sum_k|Y_k|} \tag {6.16}$$
其中N表示频率带中的频率数。对于噪声，计算出的数量$$F_2$$偏大且为常数，而对于语音，计算出的数量则偏小且为变量。同样，用于对基于特征的先验概率进行更新的映射函数M(z)的一个示例可表示为s型曲线函数：<div class="comments-icon"></div></p>
<p class="comments-section">$$M(z) = 0.5* (tanh(w_2h_2)+0.5)$$<div class="comments-icon"></div></p>
<p class="comments-section">$$z=T_2- F_2$$<div class="comments-icon"></div></p>
<pre><code class="lang-c"><span class="hljs-comment">// Compute indicator function: sigmoid map.  </span>
indicator1 =  
    <span class="hljs-number">0.5f</span> *  
    ((<span class="hljs-keyword">float</span>)<span class="hljs-built_in">tanh</span>((<span class="hljs-keyword">float</span>)sgnMap * widthPrior * (threshPrior1 - tmpFloat1)) +  
     <span class="hljs-number">1.f</span>);
</code></pre>
<p class="comments-section">3.频谱模板差异特征<div class="comments-icon"></div></p>
<p class="comments-section">除了频谱平坦度特征的噪声相关假设之外，有关噪声频谱的另一个假设是，噪声频谱比语音频谱更温度。因此，可假设噪声频谱的整体形状在任何给定阶段都倾向与保持相同。模板频谱通过更新频谱（最初被设为零）中极有可能是噪声或语音停顿的区段来确定。该比较结果是对噪声的保守估计，其中仅对语音概率确定低于阈值（如$$P(H_1|Y_k(m),{F} &lt; \lambda)$$的区段处跟新了噪声。在其它分布中，模板频谱也可能被导入到算法中，或从对应不同噪声的形状中筛选出来。考虑到输入频谱$$Y_k(m)$$和模板频谱（表示为$$\alpha_k(m)$$),如想获得频谱模板差异特征，可首先将频谱差异测量定义为：$$J = \sum \limits_k|Y_k(m) - (\alpha \alpha_k(m)+\mu)|^2$$,其中,$$(\alpha,\mu)$$是形状参数，包括线性位移和振幅参数，是通过将J最小化获得的。$$(\alpha, \mu)$$通过线性方程获得，因此可对每个帧轻松抽取此参数。在某些示例中，这些参数可表明输入频谱（在音量增加的情况下）的任何简单位移/标度变化。之后该特征将成为标准话的测度。<div class="comments-icon"></div></p>
<p class="comments-section">$$F_3 = \frac{J}{Norm}$$
其中标准化是所有频率以及之前时帧在某些时间窗口的平均输入频谱。<div class="comments-icon"></div></p>
<p class="comments-section">$$Norm = \frac{1}{W} \sum \limits<em>{n=0}^W \sum \limits_k |Y_k(n)|^2$$
如上所述，频谱模板差异特征可测量出模板或习得噪声频谱与输入频谱的差异/偏差。至少在某些布置中，这种频谱模板差异特征可用于修正特征的语音/噪声概率$$q</em>{k,m}(H|F)$$.如果$$F_3$$较小, 可将输入帧频谱视作“接近”模板频谱，且很可能将该输入帧视为噪声。另一方面，如果频谱模板差异特征值较大，则表示输入帧频谱与噪声模板频谱差异很大，则判为语音。使用S曲线将频谱模板差异特征映射为概率权数。需要重点强调的是，频谱模板差异特征测量比频谱平坦度特征测量更普遍。如果一个模板具备恒定的平坦频谱，则频谱模板差异特征可简化为对频谱平坦度的测量。<div class="comments-icon"></div></p>
<p class="comments-section">可以在频谱模板差异测量中加入$$W_k$$, 以突出频谱中的特定频率带:<div class="comments-icon"></div></p>
<p class="comments-section">$$J= \sum \limits_k W_k |Y_k(m)- (\alpha\alpha_k(m)+\mu)|^2$$
上述多个特征（LRT均值，频谱平坦度和频谱模板差异）可在语音/噪声概率的更新模板中同时出现，如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$q<em>m(H|F_1,F_2,F_3) = q_m = \gamma_pq</em>{m-1} + (1-\gamma_p)[\tau_1M(F_1 -T_1) + \tau_1M(F_2 -T_2) + \tau_1M(F_2 - T_2)]$$
不同特征预示不同信息。这些后续补充，以提供一个更稳定，更具适应性的语音/噪声概率更新。<div class="comments-icon"></div></p>
<pre><code class="lang-c">    <span class="hljs-comment">// Combine the indicator function with the feature weights.  </span>
    indPrior = weightIndPrior0 * indicator0 + weightIndPrior1 * indicator1 +  
               weightIndPrior2 * indicator2;
</code></pre>
<p class="comments-section">最后把log转换到正常的概率。 <div class="comments-icon"></div></p>
<pre><code class="lang-c">    <span class="hljs-comment">// Final speech probability: combine prior model with LR factor:.  </span>
    gainPrior = (<span class="hljs-number">1.f</span> - self-&gt;priorSpeechProb) / (self-&gt;priorSpeechProb + <span class="hljs-number">0.0001f</span>);  
    <span class="hljs-keyword">for</span> (i = <span class="hljs-number">0</span>; i &lt; self-&gt;magnLen; i++) {  
      invLrt = (<span class="hljs-keyword">float</span>)<span class="hljs-built_in">exp</span>(-self-&gt;logLrtTimeAvg[i]);  
      invLrt = (<span class="hljs-keyword">float</span>)gainPrior * invLrt;  
      probSpeechFinal[i] = <span class="hljs-number">1.f</span> / (<span class="hljs-number">1.f</span> + invLrt);  
    }
</code></pre>
<h2 id="噪声估计-updatenoiseestimate函数">噪声估计 UpdateNoiseEstimate函数</h2>
<p class="comments-section">语音/噪声概率确定后，将执行噪声估计更新，表示如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$|\hat N_k(m)| = \gamma_n|\hat N_k(m-1)| + (1-\gamma_n)\left \lfloor<br>P(H_1|Y_k(m))|\hat N_k(m-1) + P(H_0|Y_k(m))Y_k(m)
\right \rfloor \tag {6.18}$$<div class="comments-icon"></div></p>
<pre><code class="lang-c">probSpeech = self-&gt;speechProb[i];  
probNonSpeech = <span class="hljs-number">1.f</span> - probSpeech;  
<span class="hljs-comment">// Temporary noise update:  </span>
<span class="hljs-comment">// Use it for speech frames if update value is less than previous.  </span>
noiseUpdateTmp = gammaNoiseTmp * self-&gt;noisePrev[i] +  
                 (<span class="hljs-number">1.f</span> - gammaNoiseTmp) * (probNonSpeech * magn[i] +  
                                          probSpeech * self-&gt;noisePrev[i]);
</code></pre>
<p class="comments-section">其中$$|\hat N_k(m)|$$是帧/时间为m，频率槽为k时对噪声频谱量级的估计。参数控制噪声更新的平滑度，第二个期限则使用输入频谱和上次噪声估计对噪声进行跟新，然后根据如上所述的语音/噪声概率进行加权，这可表示为： <div class="comments-icon"></div></p>
<p class="comments-section">$$P(H_1|Y_k(m)) = \frac{q_m \Delta_k(m)}{q_m\Delta_k(m) + 1- q_m} \tag{6.19}$$
其中LR因子$$\Delta_k(m)$$是:<div class="comments-icon"></div></p>
<p class="comments-section">$$\Delta_k(m) = \frac{\exp\begin{pmatrix}
\frac{\rho(m)\sigma_k(m)}{1+\rho_k(m)}
\end{pmatrix}}{1+\rho_k(m)} \tag{6.20}$$
数量$$q_m$$是基于模型或基于特征的语音/噪声概率，得自上述具有多个特征的更新模型。上述噪声估计模型会对噪声可能性较大（即语音可能性较小）的每个帧和频率槽的噪声进行更新。对于噪声可能性不大的帧和频率槽，则将对信号中上一个帧的估计作为噪声估计。<div class="comments-icon"></div></p>
<p class="comments-section">噪声估计更新流程收到语音/噪声概率和平滑参数$$\rho_n$$的控制，平滑参数可被设为像0.85这样的值。在不同的示例中，对于语音概率超过阈值参数$$\lambda$$的区域，平滑参数可能会被增加到$$\rho_n = 0.99$$以防止语音开始处的噪声水平增加过高。在一个或多个布置中，阈值参数被设定为$$\rho_n = 0.2/0.25$$.
完成噪声估计更新后，噪声估计和过滤流程采用维纳增益滤波器以减少或消除来自输入帧的估计噪声量。标准维纳滤波器表达如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$H_w(k,m) = \frac{\left \langle |X_k(m)|^2 \right \rangle}{\left \langle |Y_k(m)|^2 \right \rangle} = 1 - \frac{\left \langle |N_k(m)|^2 \right \rangle}{\left \langle |Y_k(m)|^2 \right \rangle} \approx 1 -
\frac{|\hat N_k(m)|^2}{|Y_k(m)|^2 }
\tag{6.21}<div class="comments-icon"></div></p>
<p class="comments-section">$$
其中$$\hat N_k(m)$$是估计得出的噪声频谱系数，$$Y_k(m)$$是观测到的有噪频谱系数，$$X_k(m)$$是纯净语音频谱（帧为m，频率为k）。之后，平方量级可被量级替代，维纳滤波器变成：<div class="comments-icon"></div></p>
<p class="comments-section">$$H_w(k,m) = 1 - \frac{\hat N_k(m)}{Y_k(m)}$$
在一种或多种常规方法中，会对滤波器直接应用时间平均法，以减少任何的帧间波动。根据本发明的某些方面，维纳滤波器用先验SNR表示，而判决引导（DD）更新则用于对先验SNR进行时间平均计算。维纳滤波器可用先验SNR表示为：<div class="comments-icon"></div></p>
<p class="comments-section">$$H_w(k,m) = \frac{\rho_k(m)}{1 + \rho_k(m)} \tag {6.22}$$
其中,$$\rho_k(m)$$代表上文定义的先验SNR，将噪声频谱替换为估计得出的噪声频谱：<div class="comments-icon"></div></p>
<p class="comments-section">$$\rho_k(m) = \frac{\left \langle |X_k(m)| \right \rangle}{|N_k(m)|}$$
如上所述，按照DD更新估计先验SNR。该增益滤波器通过取底和过相减参数，可得出：<div class="comments-icon"></div></p>
<p class="comments-section">$$H<em>{w,dd}(k,m) = max{H</em>{min,\frac{\rho_k(m)}{\beta+\rho_k(m)}}}$$
因为DD更新明确对先验SNR进行时间平均计算，所以不会对该增益滤波器再进行外部时间平均计算。参数$$\beta$$是根据噪声抑制系统中实施的噪声抑制器的主动配置定义的。<div class="comments-icon"></div></p>
<p class="comments-section">维纳滤波器应用到输入量级频谱中，以获得经抑制的信号。在噪声估计和过滤流程中采用维纳滤波器会得出：<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat X<em>k(m) = H</em>{w,dd}$(k,m)Y_k(m) \tag {6.23}$$<div class="comments-icon"></div></p>
<h2 id="信号合成">信号合成</h2>
<p class="comments-section">信号合成包括各种后验噪声抑制处理，以生成包括纯净语音的输出帧。在应用维纳滤波器后，使用反向DFT将帧转换回时域。在一个或多个布置中，转换回时域可表达为：<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat x(n,m) = Re[\frac{1}{N} \sum \limits_k^{N-1}\hat X_k(m)e^{j2\pi\frac{n}{N}k}] \tag {6.24}$$
其中,$$\hat X_k(m)$$是经维纳滤波器抑制后估计得出的语音，$$\hat x(n,m)$$是相应的时域信号，其中时间索引为n，帧索引为m。<div class="comments-icon"></div></p>
<p class="comments-section">在反向DFT之后，作为信号合成流程的一部分，对经噪声抑制的信号实施能量缩放。能量缩放可用于帮助重建语音帧，且重建方式可增加经抑制后的语音的能量。例如，实施缩放时应确保只有语音帧会放大到一定程度，而噪声帧保持不变。由于噪声抑制可能降低语音信号水平，因此在缩放过程中对语音区段适当放大是有益处的。在一个布置中，根据语音帧在噪声估计和过滤流程中的能量损失，对该帧实施缩放。增益情况可通过该语音帧在噪声抑制处理前后的能量比来确定。<div class="comments-icon"></div></p>
<p class="comments-section">$$K = \sqrt{\frac{能量<em>{处理后}}{能量</em>{处理前}}}$$
在当前示例中，可根据下方模型提取标度：<div class="comments-icon"></div></p>
<p class="comments-section">$$标度 = A(K)\hat P(H_1|m) + B(k)(1 - \hat P(H_1|m))\tag {6.25}$$
其中, $$\hat P(H_1|m)$$是帧m的语音概率,通过取所有频率的语音概率函数$$P(H_1|Y_k(m),{F})$$的平均值而得:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat P(H_1|m) = \sum \limits_k P(H_1|Y_k(m),{F}) \tag {6.26}$$
在上述标度方程中，如果概率$$\hat P(H_1|m)$$接近1,则第一项将较大，如果是噪声，则第二想将较大。
在上述标度方程中，参数$A(K)$,$B(k)$是控制输入帧的缩放.
信号合成包括窗口合成操作，该操作提供估计得出的语音的最终输出帧。在一个示例中，窗口合成为：<div class="comments-icon"></div></p>
<p class="comments-section">$$标度(m-1)w(n) \hat x(n+M,m-1) + 标度(m)w(n) \hat x(n,m), 0 \le n \le M$$
其中，标度参数由每个帧的上述标度方程式得出.<div class="comments-icon"></div></p>
<h2 id="参数估计">参数估计</h2>
<p class="comments-section">基于特征的语音/噪声概率函数的更新模型包括应用到特征测量的多个特征加权$$\tau_1$$和阈值$$T_i$$参数:<div class="comments-icon"></div></p>
<p class="comments-section">$$q<em>m(H|F_1,F_2,F_3) = q_m = \gamma_pq</em>{m-1} + (1 - \gamma_p)[\tau_1M(F_1 - T_1) + \tau_2M(F_2 -T_2) + \tau_3M(F_3 - T_3)] \tag {6.27}$$
这些加权$$\tau_1$$和$$T_i$$用于防止不可靠的特征测量进入更新模型.映射函数也包括宽度参数$$w_i$$以及控制映射函数的形状:<div class="comments-icon"></div></p>
<p class="comments-section">$$M = M(F_i - T_i, w_i)$$
例如, 如果给的输入的LRT均值特征$F_i$不可靠.
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/68.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">NS处理前后时域图
前一张是经过NS处理的，后一张是原始麦克风采集到的数据（实际是48KHz resample到16KHz的）。听感上声音质量确实变好了，但是不一定语音识别效果就好，这时由于语音识别系统和人耳判决准则的差异性导致的，看上面二者的频谱图.
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/69.png" alt="">
这张图上下两个之间的对应关系，细节差异自己看，语音识别采用梅尔倒谱时，上述频谱有变化，如果按有噪声的情况训练，那么噪声也可能被当成语音特征学习到，（如果语音采集在环境基本一致的情况下极其容易发生，而后处理也没有的话），这将导致识别率下降出现，但是如果噪声压制过于厉害，损伤到语音信号频谱，同样识别率也将下降；信号处理是细粒度的方法。
在github里同样有测试音源文件:
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/611.png" alt="">
<a href="https://github.com/shichaog/WebRTC-audio-processing" target="_blank">github 代码仓库地址</a><div class="comments-icon"></div></p>
</blockquote>
<h1 id="声源定位doa">声源定位(DOA)</h1>
<p class="comments-section">DOA(direction of arrival),在三维空间中,除了时域,频域,还可以利用空域信息对信号进行处理,基于阵列麦克的远场语音识别场景,一些声源分离技术(beamforming, blind source seperation)会要使用到声源方位信息.声源定位技术并不仅限于单个目标源的定位,且对于ASR场景的声源目标是宽带信号.
此外,定位出声源方向,还有益于产品的交互体验(寻向灯,以及电机转动姿态).
声源定位技术主要分为五种:<div class="comments-icon"></div></p>
<ul>
<li><ol>
<li>互相关技术</li>
</ol>
</li>
<li><ol>
<li><p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/de.png" alt=""></p>
<blockquote>
<p class="comments-section">图7.1延迟差<div class="comments-icon"></div></p>
</blockquote>
</li>
</ol>
<p class="comments-section">假设入射的声波是平面波,且夹角是$$\theta$$,则有:
$$x_i(t) = s(t-\Delta_i) \tag{7.1}$$
因为后续会有频域方法计算DOA,这里给出时间上的延迟和频域上的关系:
$$s(t-\tau) \leftrightarrow  S(f)e^{-j2\pi f \tau} \tag {7.2}$$<div class="comments-icon"></div></p>
<h2 id="互相关技术">互相关技术</h2>
<p class="comments-section">互相关方法具有计算量小,实时性好而被大多数系统中使用,其基于阵元之间的差异时间差(Time-Delay/Frequency-Delay)进而提取出声源距离阵元的位置信息,根据不同的麦克风对就可以在三维空间中唯一确定一个声源,互相关方法也是有缺点的,其抗造性能差,在混响场景准确性也会降低,也有一些算法针对混响场景进行了优化,如SRP-PHAT.此外还有一些波束扫描算法,基本思想是在可能的空间点中做波束合成,然后根据合成后的各个方向上的功率最大值认为是声源方法.
两个麦克风之间的TDOA估计可以通过麦克风之间广义互相关（GCC， generalized cross-correlation）计算得到，如下两个麦克风。
$$\hat \tau<em>{\frac {GCC}{x1x2}} = \arg \max \limits</em>{\tau}{\frac{GCC}{x1x2}}(\tau) \tag {7.3}$$
这里:
$${\tau}({\frac{GCC}{x1x2}}) \triangleq IFFT{ \Psi<em>{x1x2}(f) } = \int</em>{-\infty}^{\infty}(\Phi(f)S<em>{x1x2}e^{j2\pi f \tau}df \tag{7.4}$$
其中,$$\Phi(f)$$是频域权重函数,$$S</em>{x1x2}(f)$$是互相关的频域计算.
$$S_{x1x2} \triangleq E[X_1(f)X_2*(f)] \tag {7.5}$$
频域权重函数$$\Phi(f)$$对延迟估计的影响较大.<div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">经典互相关法
理想远场模型$$\Phi(f) =1$$<div class="comments-icon"></div></p>
</li>
<li>平滑互相关变换(SCOT, smoothed coherence transform)
$$\Phi(f) = - \frac{1}{\sqrt{E{ |X_1(f)|^2|X_2(f)|^2}}}  \tag {7.6}$$
$$X_n(f) = \sum \limits_k x_n(k)e{-j2\pi k}, n=1,2,\cdots \tag {7.7}$$
这里推导一下SCOT适应场景:</li>
</ul>
<p class="comments-section">$$\begin{multline}<div class="comments-icon"></div></p>
<p class="comments-section">\Psi<em>{\frac{SCOT}{x_1x_2}} (f)= \frac{\alpha_1\alpha_2 e^{-j2\pi f \tau</em>{12}}E{ |S(f)|^2}}{\sqrt{E[ |X<em>1(f)|^2 E|X_2(f)|^2}}\  = \frac{\alpha_1 \alpha_2 e^{-j2\pi f \tau</em>{12}}E|S(f)|^2}{\sqrt{\alpha<em>1^2E|S(f)|^2 +\sigma</em>{b<em>1}^2(f)}} \times
\frac{1}{\sqrt{\alpha_2^2E|S(f)|^2+ \sigma</em>{b<em>2}^2(f)}} \
=\frac{e^{-j2\pi f \tau</em>{12}}}{\sqrt{1+\frac{1}{SNR_1(f)}}\cdot \sqrt{1+\frac{1}{SNR_2(f)}}}
\end{multline}<br>\tag{7.8}<div class="comments-icon"></div></p>
<p class="comments-section">$$
假设两个麦克风采集到的信息的信噪比相等,则有:<div class="comments-icon"></div></p>
<p class="comments-section">$$\Psi<em>{\frac{SCOT}{x_1x_2}}(f) = \frac{SNR(f)}{1+SNR(f)} \times e^{-j2\pi f \tau</em>{12}} \tag{7.9}$$
从上式可以看出,计算的准确性适用于信噪比较高的场合,但是对于高混响场景来说效果就不行,实际效果也确实是这样.<div class="comments-icon"></div></p>
<ul>
<li>相位变换法
权重变成由相位组成而不是互功率谱幅度。
$$\Phi(f) = \frac{1}{S<em>{x_1x_2}(f)} \tag{7.10}$$
由此可得$$\Psi</em>{\frac{PHAT}{x<em>1x_2}}(f) = e^{-j2\pi f \tau</em>{12}}$$,这和SCOT在SNR趋于无穷大时结果一致.
GCC算法在适度噪声和无混响环境下较好,且计算量小,跟踪实时性好,对于混响,需要采用倒谱滤波器以滤除混响的影响.</li>
</ul>
<p class="comments-section">在工程实现上时域卷积等于频域相乘,在频域里进行计算以减小计算量,且在对频点的处理上,声源发声所在的频点能量较高,可以选择一部分频点以减少无关频点的扰动影响.<div class="comments-icon"></div></p>
<h1 id="语音信号预加重">语音信号预加重</h1>
<p class="comments-section">元音能量主要集中在$$1KHz$$以下，并且以6dB/十倍频的速度下降，可以使用欲加重技术增强高频能量，这在回声消除以及语音识别中的特征提取（共振峰， LPC）中用到，声道的终端是口和唇，口唇辐射对低频影响比较小，但是对高频段影响比较大，欲加重技术技术为了提升高频分辨率，欲加重的传递函数是$$H(z) = 1 - aZ^{-1}$$。通常欲加重系数的取值在$$0.9 &lt; a &lt; 1.0$$，如speex在做aec时，选择的欲加重系数就是0.9。其在代码中使用差分方程实现：<div class="comments-icon"></div></p>
<p class="comments-section">$$y(n) = x(n) - ax(n-1) \tag{8.1}$$
欲加重系数衰减200Hz以下下的频率成分。<div class="comments-icon"></div></p>
<p class="comments-section">另外,还有有采用notch filter的例子,用于消除$$50Hz$$和$$60Hz$$的工频干扰.<div class="comments-icon"></div></p>
<h1 id="语音去混响dereverberation">语音去混响(Dereverberation)</h1>
<p class="comments-section">当声源发声时,由于反射和延迟现象,会出现同一个声音多次到达人耳的情形,人耳能明显区别出的是回声(这时反射和原始声音到达时间差可长达50ms,甚至数百上千毫秒).如果反射的声音和原始声音时间在10ms~30ms之间,由于人耳的时间掩蔽特性会增强人耳的听感,但是对于ASR语音识别是有影响的.这被称为混响.RT60 是标准的混响时间测量方法，表示从测试信号突然停止到声压级降低 60 dB 所用的时间。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/Screenshot from 2017-12-07 15:32:55.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图9.1纯净和有混响语音时域波形
当前去混响算法主要分为三个类别:<div class="comments-icon"></div></p>
<ol>
<li>波束形成
波束形式是空域滤波方法,来自其它方法的混响由于空域上的选择性会被滤除,这就要求目标方向是正确的,在360度的3D场景中,需要先定位出目标方向,而在定位目标方向时,混响是有影响的.波束形成技术内容和实例加起来比较多,后面章节再展开.</li>
<li>语音增强技术方法
当前去混响方法主要分为三个类别:</li>
<li><ol>
<li>基于统计模型的去混响方法</li>
</ol>
</li>
<li><ol>
<li>基于LPC方法</li>
</ol>
</li>
<li><ol>
<li>基于特征值分解法</li>
</ol>
</li>
</ol>
</blockquote>
<p class="comments-section">3.盲反卷积
声学脉冲响应的未知情况下,根据观察到的信号,设计出反向滤波器来削弱混响的影响.<div class="comments-icon"></div></p>
<h2 id="基于语音增强方法">基于语音增强方法</h2>
<h3 id="谱减法">谱减法</h3>
<ol>
<li><p class="comments-section">基本思想
估计每一帧的音素及其能量,对新出现的帧,减掉其之前各帧中存在的音素能量.
混响能量的功率谱密度(power spectral density,psd)可以用下式近似表示:
$$\gamma<em>{rr} [n,k] = e^{-2\Delta T}\gamma</em>{xx}[n - T,k] \tag{9.1}$$
$$\gamma<em>{rr}$$是混响语音的psd,n是离散时间索引,k是离散频点索引.衰减因子$$\Delta = \frac{3ln(10)}{RT</em>{60}}$$,
根据谱减法有:
$$|\hat S[m,k]| = (|X| - \sqrt{\gamma_{rr}})[m,k] \tag {9.2}$$
$$\hat S$$是估计的纯净语音STFT值,$$X$$是带混响的采集到的语音的STFT值,$$m$$是帧索引,<div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">混响时间估计
在Amazon的AWS有篇文献<a href="https://ai2-s2-pdfs.s3.amazonaws.com/4f52/14f4d29aad063a84cd8a51ac1688dd104b21.pdf" target="_blank">Blind estimation of reverberation time</a>,其给出了混响时间估计方法,就是式子9.1中的$$$RT<em>{60}$$,进而可以求出$$\Delta$$.该文献中将9.1中的指数部分使用$$a[n]$$来代替:
$$a[n] = a^n = (e^{\frac{-1}{\tau}{r}})^n = e^{\frac{-n}{r} } \tag{9.3}$$
由声音衰减模型可得衰减信号$$y(n) = a(n)x(n)$$的似然估计如下:
$$L(\mathbf y; a, \sigma) = (\frac{1}{2 \pi a^{(N-1)}\sigma^2})^{N/2} \times 
\exp(- \frac{\Sigma</em>{n=0}^{N-1}a^{-2n}y(n)^2}{2\sigma^2}) \tag {9.4}$$
为了求得参数$$a$$和$$\sigma$$,对9.4取对数得:
$$\ln L(\mathbf y; a, \sigma) = - \frac{N(N-1)}{2}\ln(a) - \frac{N}{2}\ln(2\pi \sigma^2) - \frac{1}{2\sigma^2}\sum \limits<em>{n=0}^{N-1} a^{-2n}y(n)^2\tag {9.5}$$
对9.5式分别对$$a$$和$$\sigma$$求偏导数,并令其等于零,可以得$$a$$,$$\sigma$$理论最优解.
$$\frac{\partial \ln(L(\mathbf y; a, \sigma))}{\partial a}
= \frac{1}{a \sigma^2} \sum \limits</em>{n=0}^{N-1} na^{-2n} x(n)^2 - \frac{N(N-1)}{2a} \tag{9.6}$$
然而要解9.6并不好解,可以进行量化以减少计算量$$a \in [0,1)$$,可以假设a可以取的值是$$a \in A={a<em>1, a_2, \cdots, a_Q}$$, 通常建$$Q \le 10$$,对于绝大多数情况可以将$$Q=2$$.则式9.4可以写为:
$$L(a_j; \mathbf y) = - \frac{N}{2}{ (N-1)\ln(a_j) - ln(\frac{2\pi}{N}\sum \limits</em>{n=0}^{N-1}a<em>j^{-2n}x[n]^2 - 1) } \tag {9.7}$$
对于逐帧计算的情况,$$a$$的值可能会随着帧而发生波动.这就需要在波动情况下为每一帧选择最优的估计.说话间隙的无声期声音衰减并不会变化.设$$\beta = a^{-2}$$,则可以定义:
$$g[n] = \beta^{N-1} \sum \limits</em>{r=n-N+1}^{n} \beta^{r-n}x[r]^2 \tag {9.8}$$
这样,式子9.6的递归求解过程可以变成下式:
$$g[n+1] = \beta^{-1}(g[n] + \beta^nx[n+1]^2 -x[n+1-N]^2) \tag{9.9}$$
为了加速这一计算过程,可以预先计算好所有的$$\beta$$, $$ln(a_j)$$和$$ln(2\pi/n)$$,最后,$$ln(g[n])$$可以使用查找表计算得到.<div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">谱减法改进
当估计到的回声能量大于信号的能量值时,谱减法会得到负值.这可以通过将谱减法转化成权重的形式,这在webrtc的很多算法中都有这个思想.
$$G[m,k] = \frac{|X[m,k]| - \gamma_{rr}^{\frac{1}{2}}[m,k]}{|X[m,k]|} \tag{9.10}$$<div class="comments-icon"></div></p>
</li>
</ol>
<p class="comments-section">剔除负值可以使用门限的方法如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$|\hat S[m,k]| = \left{\begin{matrix}
G[m,k]Xpm,k] &amp; 当 \ge \lambda \sqrt{\gamma<em>{rr}[m,k]}\ 
\gamma \sqrt{\gamma</em>{rr}[m,k]} &amp; 其它
\end{matrix}\right. \tag{9.11}$$
当$$\lambda = 0.1$$,则衰减为20dB.可以进一步使用下式提升混响的psd估计.<div class="comments-icon"></div></p>
<p class="comments-section">$$\gamma<em>{rr}[\hat m,k] = \eta \gamma</em>{rr}[\hat m - 1, k] + (1 -\eta)|X[m,k]|^2 \tag{9.12}, \eta = \frac{1/(2\Delta)}{1/(2\Delta) + O/(f_s)}$$<div class="comments-icon"></div></p>
<h3 id="基于lpc的去混响">基于LPC的去混响</h3>
<ol>
<li>LPC(linear predictive coding)</li>
</ol>
<p class="comments-section">语音信号$$x(n)$$可以使用p阶线性预测器来表示:<div class="comments-icon"></div></p>
<p class="comments-section">$$s(n) = - \sum \limits_{i=1}^{P} a_is(n-i)+ e(n) \tag{9.13}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$a_i$$是预测系数,$$e(n)$$是预测误差.LPC的系数可以做成预测误差滤波器:<div class="comments-icon"></div></p>
<p class="comments-section">$$A(z) =  1 + \sum \limits_{i=1}^p a_iz^i \tag {9.14}$$
其对应的全零点滤波器是:<div class="comments-icon"></div></p>
<p class="comments-section">$$V(z) = \frac{1}{1 + \sum_{i=1}^p a_iz^i} = \frac{1}{A(z)}  \tag{9.15}$$
使用MSE准则计算系数:<div class="comments-icon"></div></p>
<p class="comments-section">$$J = E[e^2(n)] = E{ (s(n) - \sum \limits_{i=1}^p a_is(n-i))^2} \tag {9.16}$$
令$$\frac{ \partial J} {\partial a_i} = 0 $$可以得到如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\sum \limits_{u=1}^P a_uE{s(n-i)s(n-u)} = E{s(n)s(n-u)}; 1 \le u \le  \tag {9.17}P$$
式9,17用矩阵表示为下式:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf R<em>{ss} \mathbf a = \mathbf r</em>{ss}; \mathbf a = [a_1, a_2, \cdots, a_p]^T \tag {9.18}$$
则在10ms~30ms这段稳态时间内,语音信号的LPC系数如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat {\mathbf a} = \hat {\mathbf R}<em>{ss}^{-1} \hat {\mathbf r}</em>{ss}  \tag{9.19}$$
由于互相关矩阵$$\mathbf R_ss$$是Toeplitz矩阵,可以使用Levinson-Durbin算法高效计算获得.<div class="comments-icon"></div></p>
<ol>
<li><p class="comments-section">混响场景的LPC
麦克风采集到的信号可以用下式表示:
$$x(n) = \mathbf h^T \mathbf s(n) + V(n) \tag {9.20}$$
$$x$$是麦克风采集到的信号,$$\mathbf s$$声源发声的原始信号.对于混响存在的场景,则麦克风采集到的信号可以表示为:
$$x(n) = \sum \limits<em>{i=1}^{p} b</em>{i}x(n-i) + e(n) \tag{9.21} $$
则LPC的系数安装9.19可得:
$$\hat {\mathbf b} = \hat {\mathbf R<em>{xx}}^{-1} \hat {\mathbf r</em>{xx}} \tag{9.22}$$<div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">多通道优化
对于多通道情况,可以将代价函数进行平局,这样最后求得的系数是多个通道系数的均值,这样有助于减小绕动干扰,增加系统的鲁棒性.也可以结合bf方法
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/Screenshot from 2017-12-07 16:46:39.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图9.2 混响语音增强
注[插图源于Speech Dereverberation Springer]<div class="comments-icon"></div></p>
</blockquote>
</li>
</ol>
<p class="comments-section">一个去混响开源算法是<a href="http://www.kecl.ntt.co.jp/icl/signal/wpe/index.html" target="_blank">WPE</a><div class="comments-icon"></div></p>
<h2 id="小节">小节</h2>
<p class="comments-section">本章主要给出了混响问题描述和统计模型以及LPC编码两种思路,统计模型的思路和前面章节的方法非常相似,这里也并未分析具体的WPE算法的代码,另外,还有一些基于多通道的盲源辨识方法.<div class="comments-icon"></div></p>
<h1 id="声源分离技术">声源分离技术</h1>
<p class="comments-section">在远场语音识别场景,不可避免的会遇到同时有多个人说话的场景(鸡尾酒效应),也可能会遇到周围有噪声源的情景(电器,风扇等),通过前面章节的方法基于单个麦克风并不能获得很好的性能,基于多通道的声源分离在现今也是广泛被采用的技术.
声源分离技术主要分为三类:<div class="comments-icon"></div></p>
<p class="comments-section">1.波束形成技术(BF, beamforming)
2.盲源分离技术(BSS, blind source seperation)
3.时频掩码技术(T-F masking, time-frequency masking)<div class="comments-icon"></div></p>
<p class="comments-section">以上的单个方法是可以实现声源分离,并增强信噪比(SNR)的,但是也有将上述类别融合的方法.本偏文章主要是阐述各个方法的基本思想和基本原理,了解了这些后,可以看第十一章,将BF和T-F技术集合在一起的一个声源分离的c代码实例.<div class="comments-icon"></div></p>
<h2 id="101波束形成技术">10.1波束形成技术</h2>
<blockquote>
<p class="comments-section">在正式的波束形成方法之前,先补充一下频域处理的一些点:
1.<a href="http://www.iqiyi.com/paopao/u/1427478235/#curid=9475523109_a8c306441a0feadf3a2dc12729078ec2" target="_blank">FFT时的重叠保留法</a>
2.<a href="http://www.iqiyi.com/paopao/u/1427478235/#curid=9475518109_2e5390ce503c18b63d85606cfa841c88" target="_blank">FFT的重叠相加法</a>
3.如果要<a href="http://blog.csdn.net/shichaog/article/details/75692855" target="_blank">回顾FFT</a>
3.<a href="http://blog.csdn.net/shichaog/article/details/77379998" target="_blank">子带技术</a><div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">阵元的空间分布以及波束形成的算法对最终的结果都会有影响.<div class="comments-icon"></div></p>
<ol>
<li>线阵模型
线阵模型就是麦克风在同一条直线上。下面绘制的是8麦克，间距1cm时的直角坐标和极坐标图，在直角坐标系中，横坐标是-90度到90度，纵坐标是阵列增益方向图，最高点在横坐标是0度位置，相比第二个峰值约大13dB左右。由极坐标同样可以看出，一圈是360度，从内到外同心圆是增益等高线。从图中依然可以看到增益，另外还有一个指标也可以在这两张图体现出来，即空间分辨率。从直角坐标可以看出约-8~+8度，从这个角度进来的信号相对于其它角度进来的信号会全部被放大约10dB，如果这个角度有多个人声或者离人很近的地方有噪声，则会降低SNR。解决方法是根据使用场景设计合适的空间角分辨率。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/101.png" alt=""><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/102.png" alt=""><blockquote>
<p class="comments-section">图10.1线阵直角和极坐标增益图<div class="comments-icon"></div></p>
</blockquote>
</li>
</ol>
<p class="comments-section"><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/103.png" alt="">
阵元对天线方向图的影响
3个麦克可达到的SNR在6dB左右，空间分辨率(3dB带宽)比8麦克大。此处8麦克空域混叠，导致增益下降。<div class="comments-icon"></div></p>
<p class="comments-section"><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/104.png" alt="">
可以看到，图中d的单位是米，即3阵元间距在0.75cm，1cm，以及1.5cm时增益图。即间距越大，可达到的增益越大，但其空间分辨率变低。增益差值超过一倍。<div class="comments-icon"></div></p>
<p class="comments-section">阵元对频率影响
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/105.png" alt="">
综合来说，3麦克需要考虑麦克风之间间距，输入信号空间分辨率，以及使用场景。间距越大其指向的方向信号增益会越大，但空间分辨率随之降低，线阵模型可得到增益约6dB左右，圆阵增益也可达到这个指标.<div class="comments-icon"></div></p>
<h3 id="声场">声场</h3>
<p class="comments-section">对于中高音，声音在室内以反射和散射为主，这一过程不断重复和往复直到能量变成零（吸收和传输损耗），这一过程约有16次之多。对于低音室内更像一个谐振腔，波长满足谐振条件的声波将会被放大，随着说话位置的位置变化，增强和对消的低音频率也会变化。<div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">Schroeder frequency：
室内声音的谐振腔频率和反射/散射频率的分界点。对于居家室内场景该频率一般在100Hz～200Hz之间，在室内播放一个谐振频率的声波，人在室内不同的位置听到的音量差异是比较明显的，而对于中高音差别并不明显。 <div class="comments-icon"></div></p>
<p class="comments-section">散射噪声场：
散射噪声场中，噪声能量向各个方向传播的概率是相等的。
包含若干个来自方向上均匀分布的相位随机的平面波，<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">假设空间中任意一点的声波压强表达式如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$p(t,\vec{r<em>1})=\lim \limits</em>{n\to \infty}\frac{1}{\sqrt{n}}\sum_{i=1}^nA_icos(\omega+\varphi_i) \tag {10.0}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\vec r_1$$位置向量，参考点可以任意选取，则另外一点的声压表达时如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$p(t,\vec{r<em>2})=\lim \limits</em>{n\to \infty}\frac{1}{\sqrt{n}}\sum_{i=1}^nA_icos(\omega+\varphi_i+(\vec {r}_2-\vec{r}_1)\cdot \vec{k}_i) \tag {10.1}$$
其中$$\vec{k}_1, \vec{k}_2,\vec{k}_3,...$$是平面波的波数向量。<div class="comments-icon"></div></p>
<h3 id="相干和非相干噪声">相干和非相干噪声</h3>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170725112643375.png" alt=""></p>
<blockquote>
<p>相干噪声</p>
</blockquote>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170725112751442.png" alt=""></p>
<blockquote>
<p class="comments-section">非相干噪声<div class="comments-icon"></div></p>
<p class="comments-section">波数（k）
沿着波的传播方向单位长度内波的全周期数。$k=1/波长=\frac{f}{c}$,也可定义成$k=2\pi/波长$，这样可以理解成相位随距离的变化率。<div class="comments-icon"></div></p>
<h3 id="远场和近场">远场和近场</h3>
<p class="comments-section"><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170702182140630.png" alt="">
远场模型可以看成是平面波（左），传播方向是$\vec {\zeta^0}$，远场要看成是球面波（右）,传播方向$\vec {\zeta_m^0}$<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">远场和近场的临界值是$$2d^2/ \lambda<em>{min}$$(d为两个阵元之间的距离, $$\lambda</em>{min}$$是声波波长最小值),当声源距离大于临界值时,使用远场模型,反之使用近场模型.<div class="comments-icon"></div></p>
<h3 id="delay-sum-波束形成">delay-sum 波束形成</h3>
<p class="comments-section">对非相干噪声效果较好，如空间白噪声，然而，如果噪声源是相干的，降噪的程度依赖于噪声的方向，在室内混响场景下，并不能获得很好的效果。
设输入信号是$$s(t)$$，叠加噪声是加性的$$n(t)$$，则第m个麦克风观测到的信号是：<div class="comments-icon"></div></p>
<p class="comments-section">$$x_m(t)=s(t)+n_m(t)$$
对于线阵模型的实例是:
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170702184043326.png" alt="">
则将每一路信号经过冲击响应延迟后再相加可以得到时间匹配上的信号。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170702184334052.png" alt="">
则延迟和输出是：<div class="comments-icon"></div></p>
<p class="comments-section">$$y(t)=\sum \limits_{m=0}^{M-1}w_mx_m(t-[M-m-1]T)$$
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170702184732288.png" alt=""><div class="comments-icon"></div></p>
<h3 id="filter-and-sum">filter-and-sum</h3>
<p class="comments-section"><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170702185644420.png" alt="">
和delay-sum相比其使用了幅度和相位不一致的权重。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170702185819109.png" alt=""><div class="comments-icon"></div></p>
<h3 id="自适应波束形成">自适应波束形成</h3>
<p class="comments-section">基于目标和干扰声源的统计特性，根据一些准则，如最大信噪比准则（MSNR，maximum signal-to-noise ratio）, 最小均方误差（MMSE， minimum mean-squared error),最小方差无失真响应（MVDR， minimum variance distortionless reponse）, 以及线性约束最小方差(LCMV, linear constriant minimum variance),其在增强目标信号的同时，抑制干扰信号。
典型的bf有，Frost beamformer， GSC beamformer， LCMV beamformer， MVDR beamformer。Frost/mvdr等，在相干噪声场，可以得到较高的信噪比改善，但是在弱相干噪声场和在散射噪声场中，性能不如固定波束形成。其一种结构可以如下：
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170725112848402.png" alt=""><div class="comments-icon"></div></p>
<ul>
<li>后置滤波作用
可以用来去除非相干噪声，但是在相干噪声情况下性能退化，甚至不可用。zelinski后置滤波器的结构体如下：
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170725112945808.png" alt="">
apab(adaptive post-filter for an arbitrary beamformer） 后置滤波器:
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170725121038285.png" alt="">
通常将自适应滤波器和后置滤波器结合起来以抑制相干和非相干噪声。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170725113146423.png" alt="">
一张频谱图，可以反映它们之间的对比关系:
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170725121542676.png" alt=""><h2 id="102盲源分离问题">10.2盲源分离问题</h2>
盲源分离技术仅根据观察到的每一路混叠信号估计原始多路信号，独立成分分析（independent component analysis）卷积混合情况的盲源分离技术。第一部分麦克风数量大于声源数量的ICA的方法，第二部分是麦克风数量小于声源数量的时频分集方法，第三部分是基于最大后验概率的单麦克盲源分离技术。
盲源分离解决的问题：</li>
<li><em>线性模型，又称瞬时模型</em></li>
<li><em>卷积模型，语音更符合这个模型</em>
盲源分离存在两种不确定性：分离后信号的排列顺序和复振幅（幅值和相位）。当源信号之间相互独立时，如果对源信号矢量进行变换，当且仅当变换后的信号之间保持相互独立，该变换矩阵可以分解为一个满秩对角阵和一个转置矩阵的乘积，仅改变源信号的幅度和排列顺序，并没有改变信号波形。
独立分量分析证明了只要通过适当的线性变换，使得变换后的各个信号之间相互独立就可以实现源信号的盲分离，即将盲源分离问题，转换为对独立分量分析的求解问题。可以使用的判决准则：<pre><code>  &gt;1.自然梯度
  2.随机梯度
</code></pre></li>
<li><p class="comments-section">信息论法：
1.基于互信息法：对比函数是一个用来衡量变换后各个信号之间相互独立程度的实值标量函数，当且仅当各个输出信号之间相互独立时，对比函数取得最大值或最小值。
2.基于信源的非高斯性测度（仅指峭度）
3.近似负嫡准则，成为FastICA。计算简单，收敛速度快，不需要任何步长参数，且迭代稳定，占用内存少。还可以通过非线性函数的适当选取来找到最优解。<div class="comments-icon"></div></p>
<ul>
<li>统计量方法：
1.基于二阶统计量
2.基于高阶统计量，BP网络分离信号方法，能分离线性和非线性混合，</li>
</ul>
</li>
</ul>
<p class="comments-section">PCA主成份分析。高斯噪声分离。
ICA，非高斯噪声分离，（前提假设线性，静态混合）<div class="comments-icon"></div></p>
<ul>
<li>研究集中于以下几点：
1.欠定问题，源数目多于传感器（麦克风）
2.单通道问题，只有一个传感器，分离出多个源
3.多维BSS，
4.非线性BSS，
5.盲解卷积（声源场景）
6.全局收敛算法
7.非平稳环境下的BSS        </li>
</ul>
<ul>
<li>信号预处理,分离前
1.中心化，即去均值。
2.白化，</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th>项目</th>
<th>波束形成算法</th>
<th>基于ICA的盲源分离算</th>
</tr>
</thead>
<tbody>
<tr>
<td>优势</td>
<td>已经商用</td>
<td>能够动态跟踪声源</td>
</tr>
<tr>
<td>劣势</td>
<td>动态跟踪声源性能差</td>
<td>分离矩阵计算量较大；硬件和麦克风成本较高</td>
</tr>
</tbody>
</table>
<h3 id="ica方法">ICA方法</h3>
<p class="comments-section">语音信号的卷积盲分离:<div class="comments-icon"></div></p>
<ul>
<li>基于块扩展的自然梯度法</li>
<li>空时扩展的FastICA方法
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/106.png" alt=""><blockquote>
<p class="comments-section">瞬时混合模型BSS分离<div class="comments-icon"></div></p>
</blockquote>
</li>
</ul>
<p class="comments-section">$$m$$是位置语音源$${s_i(k)}, 1\le i \le m$$数量，观察到$n$个输出$${x_j(k)}, 1\le j \le n$$.<div class="comments-icon"></div></p>
<p class="comments-section">$$x<em>j(k)=\sum</em>{i=1}^m {a_{ji}}s_i(k)+v_j(k) $$<div class="comments-icon"></div></p>
<p class="comments-section">$$a_{ji}$$是矩阵$$A(n<em>m)$$线性时不变系统的系数，$$v_j(k)$$是加性噪声。瞬时混合盲源分离技术就是要找到一个$m</em>n$的解混合矩阵$B$.<div class="comments-icon"></div></p>
<p class="comments-section">$$y<em>i(k)=\sum</em>{j=1}^n{b_{ij}(k)x_j(k)}$$
然而室内语音由于反射和混响，其要使用卷积混合模型进行分离。<div class="comments-icon"></div></p>
<h3 id="卷积混合分离模型">卷积混合分离模型</h3>
<p class="comments-section">$$x<em>j(k)=\sum</em>{l=-\infty}^\infty\sum<em>{i=1}^m {b</em>{jil}}s_i(k-l) $$
这里忽略了噪声，其响应的矩阵表示是：<div class="comments-icon"></div></p>
<p class="comments-section">$$y(k)=\sum_{l=0}^{L-1}B_lX(K-l)$$
这里要求输入信号之间是统计独立的，短时平稳的。<div class="comments-icon"></div></p>
<h3 id="使用空域独立特性的时域卷积盲分离">使用空域独立特性的时域卷积盲分离</h3>
<p class="comments-section">使用信源统计独立性进行BSS的方法可以分为两类：<div class="comments-icon"></div></p>
<ul>
<li>基于密度匹配法</li>
<li>基于对比函数法<h4 id="密度匹配法使用自然梯度">密度匹配法使用自然梯度</h4>
对于瞬时混合，仍然可以从ICA的信息最大化的推导出密度匹配法。在线性混合条件下，上面两个准则认为联合概率密度函数如下：
$$p(s)=\prod_{k=1}^mp_s(s_k)$$
将观察到的信号$$X(k)=[x_1(k)...x_m(k)]^T$$根据系统解混合矩阵$B$线性变换得到的输出向量$y(k)=B(k)X(k)$,密度匹配BSS方法是使输出向量的概率密度函数$$p_y{y}$$匹配模型的概率密度函数$\widehat{p}_y(y)$.
$$p_y{y}$$和$\widehat{p}_y(y)$的差异可以使用相对熵（KL散度）来度量：
$$D(p_y||\widehat{p}_y(y))=\int{p_y(y)}log(\frac{p_y}{\widehat{p}_y(y)})$$
式中$$dy=dy_1...d_m$$.
此外还有对比函数法等方法<h3 id="频域卷积bss">频域卷积BSS</h3>
对中等程度的混响就需要较长的多通道FIR滤波，这可以放到频域来做，因为时域卷积对应频域相乘。从时域变换到频域可以通过滑窗DFT或者通过短时傅里叶变换（STFT）。窗长内的信号要是准静态的，将T时间内的采样点写成矩阵$X(k)$:
$$X(k)=[X(k),X(k+1),...,X(k+T-1)]$$
其DFT如下：
$$X(\omega,k)=\sum_{\tau=0}^{T-1}X(k+\tau)e^{-j2\pi\omega\tau/T}$$
做FFT通常会进行加窗处理。观测信号和源信息源的关系如下：
$$X(\omega,k)\approx{A(\omega)s(\omega,k)}$$
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/107.png" alt=""><blockquote>
<p class="comments-section">频域盲分离框图<div class="comments-icon"></div></p>
</blockquote>
</li>
</ul>
<p class="comments-section">$$y(\omega,k)=B(\omega,k)A(\omega)s(\omega,k)=G(\omega,k)S(z)$$<div class="comments-icon"></div></p>
<h3 id="分离准则">分离准则</h3>
<p class="comments-section">盲源分离方法建立在对声源和混合矩阵的不同假设的基础上，一般声源是独立的，至少是不相关的。分离的判决准则可以分为两类，一类是基于高阶统计量（HOS，higher-order statics），和二阶统计量（SOS，second-order statics）。N表示声源数，M表示麦克风数。这在后文一个实例中使用到这一准则。<div class="comments-icon"></div></p>
<table>
<thead>
<tr>
<th>$$N&lt;M$$</th>
<th>$$N=M$$</th>
<th>$$N&gt;M$$</th>
</tr>
</thead>
<tbody>
<tr>
<td>子空间方法</td>
<td>二阶或者三阶累积量</td>
<td>非静态，列优先，互质</td>
</tr>
<tr>
<td>降到瞬时混合情况求解</td>
<td>1.SOS/HOS对于2*2系统 2.独立功率谱 3.累计量的书序大于二，ML准则</td>
<td>互累计量，时频域稀疏</td>
</tr>
</tbody>
</table>
<h3 id="卷积盲源分离频域法">卷积盲源分离频域法</h3>
<p class="comments-section">卷积盲源分离包括两类主要方法：时域和频域。设$s_1,...,s_N$是N个源信号。{x_jk}是第${j}$个麦克风从第${k}$个声源在卷积模型下的观察的信号值。<div class="comments-icon"></div></p>
<p class="comments-section">$$X<em>{ji}(t)=\sum</em>{l=0}^ph<em>{jk}(l\bullet{t_s})s_k(t-l\bullet{t_s})$$
其中$t(t_s=1/f_s)$是时间的离散表示，h</em>{jk}是声源k到麦克风$j$的脉冲响应。假设N个声源，则有：<div class="comments-icon"></div></p>
<p class="comments-section">$$x<em>j{t}=\sum</em>{k=1}^N{x<em>jk(t)}=\sum</em>{k=1}^{N}\sum<em>{l=0}^P{h</em>{jk}(l\bullet{t_s})}s_k(t-l\bullet{t_s})$$
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/108.png" alt=""><div class="comments-icon"></div></p>
<p class="comments-section">对分离效果的好坏使用信号/干扰比（signal-to-interference ratio, SIR）和信号/失真比（signal-to-distortion ratio）。<div class="comments-icon"></div></p>
<p class="comments-section">$$InputSTR<em>i=10\log</em>{10}\frac{\sum<em>t|x</em>{ji}(t)|^2}{\sum<em>t|\sum</em>{k\neq{i}}x_{jk}(t)|^2}$$ (dB)<div class="comments-icon"></div></p>
<p class="comments-section">$$OutputSTR<em>i=10\log</em>{10}\frac{\sum<em>t|y</em>{ji}(t)|^2}{\sum<em>t|\sum</em>{k\neq{i}}y_{jk}(t)|^2}$$ (dB)<div class="comments-icon"></div></p>
<p class="comments-section">$$y_{ik}$$是$$s_k$$在输出端$$y_i$$的部分。
SDR定义如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$SDR<em>i=10\log</em>{10}\frac{\sum<em>t|\alpha_i{y</em>{ji}(t-\delta)}|^2}{\sum<em>t|y</em>{ji}-\alpha<em>{i}x</em>{ji}(t-\delta<em>{i})|^2}$$
失真定义于分母，这通过找到合适的幅度调节因子$$\alpha</em>{i}$$和时间调节因子$$\delta_{i}$$最小化失真获得SDR。最优$$\delta_i$$通过最大化互相关获得：<div class="comments-icon"></div></p>
<p class="comments-section">$$\delta<em>i=argmax</em>{\delta}\sum<em>ty</em>{ji}(t)(t-\delta)$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\alpha_i$$通过最小均方误差获得：<div class="comments-icon"></div></p>
<p class="comments-section">$$\alpha<em>i=\frac{\sum_t{y</em>{ii}(t)x<em>{ji}(t-\delta)}}{\sum_t|x</em>{ji}(t-\delta)|^2}$$
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/109.png" alt=""><div class="comments-icon"></div></p>
<p class="comments-section">通过STFT将每一个麦克风观察到的时域信号$x_j(t)$转换到频域时间信号$$x_j(n,f)$$:<div class="comments-icon"></div></p>
<p class="comments-section">$$x<em>j(n,f)\leftarrow\sum_tx_j(t)win_a(t-nSt_s)e^{-t2{\pi}ft}$$
每一个离散的频点$f{\in}{0,\frac{1}{L}f_s,...,\frac{L-1}{L}f_s    }$.分析用的窗函数可以使用汉宁窗。如果帧长$L$能够包括绝大部分的脉冲响应$h</em>{jk}$,则卷积模型可近似使用瞬时模型近似：<div class="comments-icon"></div></p>
<p class="comments-section">$$x<em>{jk}(n,f)=h</em>{jk}(f)s_k(n,f)$$
则有：<div class="comments-icon"></div></p>
<p class="comments-section">$$x<em>j(n,f)=\sum</em>{k=1}^Nx<em>{jk}(n,f)=\sum</em>{k=1}^Nh_jk(f)s_k(n,f)$$
向量表示如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf{X}(n,f)=\sum_{k=1}^N\mathbf{h}_k(f)\mathbf{s}_k(n,f)$$
接下来的操作，分离，置换以及缩放和T-F掩码都在频域做。复共轭矩阵的关系如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$x_j{n,\frac{m}{L}f_s}=x_j^*{n,\frac{L-m}{L}f_s,m=1,...,\frac{L}{2}-1$$
这里最关键的就是求分离矩阵，这个非常类似AEC情况下的回声消除求权重的过程，由于AEC是一维，BSS是多维且矩阵间互相影响，所以其计算特别耗时。
但是这时可以利用声源定位技术，在大概知道多个声源的方向，这时分离矩阵的系数应该让各个方向上能量和其它方向上的能量比值最大。
另外针对也许特殊的场景，不放考虑使用DNN方式，先前训练好几组权重系数。
这里给出一个matlab版本程序，需要说明的是，这个程序在无背景噪声，线阵，麦克风大间距下效果非常明显。<div class="comments-icon"></div></p>
<h2 id="103-t-ftime-frequency">10.3 T-F(Time-Frequency)</h2>
<p class="comments-section">利用信号的稀疏性（对每一个时频点，都只有一个信号占绝大部分），通过对信号时频域加掩码的方式抑制信号，这样可以将视频点分成N个类，每个类对应于一个声源，这个方法的一个典型是DUET（Degenerate Unmixing Estimation Technique）.
1.对原始信号做STFT
2.特征提取，根据STFT后的频点选择特征，$$\Theta(f,t)$$，大多数这类方法选择，幅度比和相位差做为特性。
3.聚类，如k-means方法。
4.分离，<div class="comments-icon"></div></p>
<h2 id="本章小节">本章小节</h2>
<p class="comments-section">本章主要介绍了声源分离技术的原理,阐述了在beamformering技术中,相关参数设置对波束形成方向图的影响,也介绍了几种常见的beamformer技术,而后有介绍了基于盲源分离方法.这是后续实例章节的理论基础.<div class="comments-icon"></div></p>

<h1 id="波束形成实例">波束形成实例</h1>
<p class="comments-section">本文基于webRTC源码剖析其基于delay-sum和T-F masking混合的声源分离方法.该算法在计算的复杂度,性能和鲁棒性之间做了很好的均衡.鲁棒性主要是目标估计错误的情况而提出的.该算法的核心是采用了非线性后置滤波处理.<div class="comments-icon"></div></p>
<h2 id="编译测试文件">编译测试文件</h2>
<p class="comments-section">gsc@gsc-250:~/webrtc-checkout/src/webrtc/modules/audio_processing$ 目录下的"audio_processing_tests.gypi" 文件的153 行，如下：<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/111.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">gypi编译脚本<br>该文件是gypi（generated your project  included）格式的文件，语法类似json和python。编译的目标是nonlinear_beamformer_test。该文件编译的源文件是<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">beamformer/nonlinear_beamformer_test.cc<div class="comments-icon"></div></p>
<p class="comments-section">查找该目标的所在位置：<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/112.png" alt=""><div class="comments-icon"></div></p>
<p class="comments-section">测试文件处理流程:<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/113.png" alt=""><br>该文件处理流程比较清晰，首先根据读入的wav文件，获得采样率和通道数（麦克风个数），然后获得麦克的物理位置，<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/114.png" alt=""><br>从输出可以看出，输入信号采样率是16KHz，三通道，输出信号的采样率是16KHz，一个通道，将三个通道合成为了一个通道。根据这个可执行程序，就可以一步步打印相关的中间执行结果，这有利于更好的弄懂WebRTC的beamforming算法.<div class="comments-icon"></div></p>
<h2 id="算法的基本思想">算法的基本思想</h2>
<p class="comments-section">每个频点的幅度增益(实部和虚部)根据,<div class="comments-icon"></div></p>
<ol>
<li>目标信号的空域协方差矩阵.</li>
<li>干扰信号的空域协方差矩阵.</li>
<li>临近频点之间波动情况.</li>
</ol>
<p class="comments-section">由于语音信号是宽带信号,所以适合将语音信号从时域转换到频域,然后针对每一个频点做处理,这样处理的粒度可以非常的细.对每个频点处理的细则如下,如果说使用delay-sum方法合成的波束变换到频域后,对应频点能量较大,则说明这个频点含有目标信息的量就比较大,应该保留的比例会相对较大,同理,如果是干扰方向计算得到的频点能量较大,则削弱这个频点的力度也要大,如果没有显示给出干扰方向,可以选择第一主瓣做为干扰方向,如果用户的使用场景可以确定目标方向的,则就可以直接使用用户给定的目标方向.<div class="comments-icon"></div></p>
<p class="comments-section">对最终波束形成之后的频域信息,可以统计得到目标是否存在的概率,一个直观个理解是如果目标(声源)正确得大,则对应波束输出频点中应该含有声源的语音信息,不论是基于能量还是统计模型计算结果都可以估计目标是否存在.<div class="comments-icon"></div></p>
<p class="comments-section">该算法的后置滤波以均方误差为准则对每一个频点计算一个增益值,就实际的语音场景,其动态范围一般较宽,所以在绝大多数场景每一个频点的绝大部分能量会来自同一个声源.所以就需要根据声源信息为每一个频点选择合适的增益.<div class="comments-icon"></div></p>
<p class="comments-section">在进入代码之前,还有几个数学公式:
矩阵归一化(范数归一化):<div class="comments-icon"></div></p>
<p class="comments-section">$$\left | R<em>{\beta} \right |</em>{\alpha} = {\mathbf V^HR<em>{\beta}\mathbf V, V=\arg max_W W^HR</em>{\alpha}W, WW^H = 1} \tag{11.1}$$<div class="comments-icon"></div></p>
<p class="comments-section">则归一化的$$\overline R_{\alpha}$$如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\overline R<em>\alpha = \frac{1}{\left | R</em>{\alpha} \right |<em>{\alpha}} R</em>{\alpha} \tag{11.2}<div class="comments-icon"></div></p>
<p>$$</p>
<p class="comments-section">设$$d_n$$是第n个目标信号,$$u_m$$是第m干扰信号,则对每一个时频点有一个对应的目标干扰组合$$(d_n^{'}, u_m^{'})$$,则增益的表达式可以写成下式:<div class="comments-icon"></div></p>
<p class="comments-section">$$g<em>i = \max \limits</em>{n^{'}} \min \limits_{m^{'}}g_in^{'}m^{'} \tag{11.3}$$
实际使用时,为了鲁棒性会将11.3调整成下式:<div class="comments-icon"></div></p>
<p class="comments-section">$$
g<em>i = 1 - \prod \limits</em>{n^{'}}\left [ 1- \prod \limits<em>{m^{'}}g</em>{n^{'}m^{'}} \right ]
\tag{11.4}<div class="comments-icon"></div></p>
<p>$$</p>
<p class="comments-section">设$$\Phi$$是目标声源$$\xi$$的估计值,则对于线性波束形成算法,有:<div class="comments-icon"></div></p>
<p class="comments-section">$$\Phi = W^HY \tag{11.5}$$
后置滤波操作将上述目标估计值$$\hat \xi$$乘以11.4中的增益,则有:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat \Phi = g_i \Phi = g_i W^HY \tag {11.6}$$
W由声源目标的空间位置和麦克风坐标求得,$$g_i$$通过最小均方误差求得:<div class="comments-icon"></div></p>
<p class="comments-section">$$g<em>i^* = \arg \max \limits_g [A_i[|w^Hh</em>{\xi} \xi -g\Phi|^2]] \tag{11.7}$$
将式子11.7展开后得:<div class="comments-icon"></div></p>
<p class="comments-section">$$g<em>i^* = \arg \max \limits_g [A_i[|w^Hh</em>{\xi} \xi -g\Phi|^2]] =
\arg \max \limits<em>g [A_i[g^2|\Phi|^2 - g w^Hh</em>{\xi}\xi \Phi^H - g h_{\xi}^Hw \xi^H \Phi]]
\tag{11.7}$$
令上式等于零,可以得到最优解:<div class="comments-icon"></div></p>
<p class="comments-section">$$g<em>i^* = \frac{E[A_i[|w^Hh</em>{\xi}|^2 |\xi|^2]]}{E[A<em>i[|\Phi|^2]]}
==\frac{A_i[|\xi|^2]w^H R</em>{\xi}w}{A<em>i|\xi|^2w^H R</em>{\xi}w + A<em>i[|\phi|^2]w^H R</em>{\xi}w}<div class="comments-icon"></div></p>
<p class="comments-section">=\frac{A<em>i[|\xi|^2]w^HR</em>{\xi}w}{w^HR_Mw} \tag{11.8}<div class="comments-icon"></div></p>
<p class="comments-section">$$
其中<div class="comments-icon"></div></p>
<p class="comments-section">$$R<em>M$$是观测到信号的协方差矩阵,$$R_M = A_i[yy^H]$$,$$R{\xi}$$是麦克风接收到的声源部分(目标信号加传递函数的影响)的协方差矩阵,$$\phi$$指示麦克风采集到的干扰和噪声部分.
上述式子中$$R</em>{\xi}$$，$$R<em>{\phi}$$和$$R_M$$是容易求得的，式11.8分母部分可以看成是波束形成器的输出功率,对其进行矩阵归一化(用到11.1和11.2),得:$$_w^H \overline R w =
\frac{A_i[|\phi|^2]||R</em>{\phi}||<em>{\phi}}{||R_M||_M}w^H \overline R</em>{\phi}w +
\frac{A<em>i[|\xi|^2]||R</em>{\xi}||<em>{\xi}}{||R_M||_M}w^H \overline R</em>{\xi}w \<div class="comments-icon"></div></p>
<p class="comments-section">=\frac{A<em>i[|\phi|^2]||R</em>{\phi}||<em>{M}}{||R_M||_M} \cdot \frac{||R</em>{\phi}||<em>{\phi}}{||R</em>{\phi}||<em>M} \cdot |||\overline R</em>{\phi}||_w \<div class="comments-icon"></div></p>
<p class="comments-section">=\frac{A<em>i[|\xi|^2]||R</em>{\xi}||<em>{M}}{||R_M||_M} \cdot \frac{||R</em>{\xi}||<em>{\xi}}{||R</em>{\xi}||<em>M} \cdot |||\overline R</em>{\xi}||_w \<div class="comments-icon"></div></p>
<p class="comments-section">=(1-\lambda)\cdot \frac{||R<em>{\phi}||</em>{\phi}}{||R<em>{\phi}||_M} \cdot |||\overline R</em>{\phi}||<em>w + \lambda \cdot \frac{||R</em>{\xi}||<em>{\xi}}{||R</em>{\xi}||<em>M} \cdot |||\overline R</em>{\xi}||_w \tag {11.9}<div class="comments-icon"></div></p>
<p class="comments-section">$$
其中$$\lambda$$是对$$A_i[|\xi|^2]$$的归一化:<div class="comments-icon"></div></p>
<p class="comments-section">$$\lambda = \frac{A<em>i[|\xi|^2]||R</em>{\xi}||_M}{||R_M||_M} \tag {11.10}<div class="comments-icon"></div></p>
<p>$$</p>
<p class="comments-section">从上式可以看出对波束形成的输出贡献最大的部分是期望的目标信号.
根据11.9,可以得到归一化的$$\lambda$$的解:<div class="comments-icon"></div></p>
<p class="comments-section">$$\lambda = \frac{ ||\overline R<em>M||_M - \frac{||R</em>{\phi}||<em>{\phi}}{||R</em>{\phi}||<em>M}||\overline R</em>{\phi}||<em>w}
{\frac{||R</em>{\xi}||<em>{\xi}}{||R</em>{\xi}||<em>M}||\overline R</em>{\xi}||<em>w - \frac{||R</em>{\phi}||<em>{\phi}}{||R</em>{\phi}||<em>M||\overline R</em>{\phi}||_w}}<div class="comments-icon"></div></p>
<p class="comments-section">=\frac{||\overline R<em>M||_w - \frac{||\overline R</em>{\phi}||<em>w}{||\overline R</em>{\phi}||<em>M}}{\frac{||\overline R</em>{\xi}||<em>w}{||\overline R</em>{\xi}||<em>M} - \frac{||\overline R</em>{\phi}||<em>w}{||\overline R</em>{\phi}||_M}}
\tag{11.11}<div class="comments-icon"></div></p>
<p class="comments-section">$$
可以按能量方式去理解11.11.这样的化,式子11.8可以变为:<div class="comments-icon"></div></p>
<p class="comments-section">$$g^* = \lambda \cdot \frac{||R<em>M||_M}{||R</em>{\xi}||<em>M} \cdot \frac{||R</em>{\xi}||<em>w}{||R_M||_w}\
= \frac{1-\frac{||\overline R</em>{\phi}||<em>w}{||\overline R_M||_M} \frac{1}{||\overline R_M||_w}}
{1- \frac{||R</em>{\phi}||<em>w}{||\overline R</em>{\phi}||<em>M} \frac{||\overline R</em>{\xi}||<em>M}{||\overline R</em>{\xi}||_w}}<div class="comments-icon"></div></p>
<p class="comments-section">\tag {11.12}<div class="comments-icon"></div></p>
<p>$$</p>
<p class="comments-section">11.12的分子和分母都有$$\frac{||\overline R<em>{\phi}||_w}{||\overline R</em>{\phi}||_M}$$,该项在波束指向方向时,值较小,但是其它方向时较大(通常小于1).为了增强鲁棒性,常使用下式:<div class="comments-icon"></div></p>
<p class="comments-section">$$g^* = \frac{1 - min{\alpha, \frac{||\overline R<em>{\phi}||_w}{||\overline R</em>{\phi}||<em>M} \frac{1}{||\overline R_M||_w}}}{1- min{\alpha, \frac{||\overline R</em>{\phi}||<em>w}{||\overline R</em>{\phi}||<em>M} \frac{||\overline R</em>{\xi}||<em>M}{||\overline R</em>{\xi}||_w} }} \tag{11.13}$$
其中$$\alpha$$是选定的常数因子,通常取值为0.999.
在使用delay-sum时,由于宽带信号的空域混跌问题,权重$$w$$在200Hz以下,以及混叠频率处需要额外处理,通常取均值做为整个频带的权重因子.<div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">空域混叠
对于时域有奈奎斯特定理:$$f<em>s = \frac{1}{T_s} \ge 2f</em>{max}$$,在空间上类似的有:$$f<em>{x_a} = \frac{1}{d} \ge 2 f</em>{xmax}$$.
根据麦克距离和声源方向可以计算混跌频率：<div class="comments-icon"></div></p>
<p class="comments-section">$$f_{Alias} = \frac{s}{d\cdot(1+cos(target))}$$
d是两麦克直线距离，<div class="comments-icon"></div></p>
</blockquote>
<h3 id="测试文件">测试文件</h3>
<pre><code class="lang-cpp">nonlinear_beamformer_test.cc

6 NonlinearBeamformer bf(array_geometry, array_geometry.size());
7 bf.Initialize(kChunkSizeMs, in_file.sample_rate());
8 while (in_file.ReadSamples(interleaved.size(),
&amp;interleaved[0]) == interleaved.size() ) {
9 FloatS16ToFloat(&amp;interleaved[0], interleaved.size(), &amp;interleaved[0]);
10 Deinterleave(&amp;interleaved[0], buf.num_frames(),
buf.num_channels(), buf.channels());
count++;
11 bf.AnalyzeChunk(buf);
12 bf.PostFilter(&amp;buf);
13 Interleave(buf.channels(), buf.num_frames(),
buf.num_channels(), &amp;interleaved[0]);
14 FloatToFloatS16(&amp;interleaved[0], interleaved.size(), &amp;interleaved[0]);
out_file.WriteSamples(&amp;interleaved[0], interleaved.size());
}
</code></pre>
<p class="comments-section">6是波束形成的实例化,需要把麦克风的坐标和数量放在参数里,7行的第一个参数是数据长度(8ms~32ms)之间都可以,第二个参数是采样率.9,10行是对读入的数据进行了重新排列.11行主要是用于计算11.13中的$$g^*$$,12行主要是对对其进行T-F相乘.
第6行创建的对象是整个算法的而核心:<div class="comments-icon"></div></p>
<pre><code class="lang-cpp">// Enhances sound sources coming directly in front of a uniform linear array
// and suppresses sound sources coming from all other directions. Operates on
// multichannel signals and produces single-channel output.
//
// The implemented nonlinear postfilter algorithm taken from "A Robust Nonlinear
// Beamforming Postprocessor" by Bastiaan Kleijn.
class NonlinearBeamformer : public LappedTransform::Callback {
public:
//半波数弧度值,是角分辨率的度量,webrtc默认设置成了20度.
static const float kHalfBeamWidthRadians;
//这里设置了目标的初始方向.
explicit NonlinearBeamformer(
const std::vector&lt;Point&gt;&amp; array_geometry,
size_t num_postfilter_channels = 1u,
SphericalPointf target_direction =
SphericalPointf(static_cast&lt;float&gt;(M_PI) / 2.f, 0.f, 1.f));
~NonlinearBeamformer() override;

// Sample rate corresponds to the lower band.
// Needs to be called before the NonlinearBeamformer can be used.
virtual void Initialize(int chunk_size_ms, int sample_rate_hz);

//根据初始化的参数,将时域信号分到频域,计算11.3的每个频点的g值
virtual void AnalyzeChunk(const ChannelBuffer&lt;float&gt;&amp; data);

//根据AnalyzeChunk计算到的g值,做后滤波,然后在变换到时域
virtual void PostFilter(ChannelBuffer&lt;float&gt;* data);
//设置目标的坐标的API(球坐标:依次是方位角,俯仰角,半径)
virtual void AimAt(const SphericalPointf&amp; target_direction);
//根据波束形成后的情况,判断目标是否存在
virtual bool IsInBeam(const SphericalPointf&amp; spherical_point);
virtual bool is_target_present();

protected:
//时频处理的核心函数
void ProcessAudioBlock(const complex&lt;float&gt;* const* input,
size_t num_input_channels,
size_t num_freq_bins,
size_t num_output_channels,
complex&lt;float&gt;* const* output) override;

private:
FRIEND_TEST_ALL_PREFIXES(NonlinearBeamformerTest,
InterfAnglesTakeAmbiguityIntoAccount);

typedef Matrix&lt;float&gt; MatrixF;
typedef ComplexMatrix&lt;float&gt; ComplexMatrixF;
typedef complex&lt;float&gt; complex_f;
//低频部分是200!~400,这段不是每个频点使用一个g,而是使用这个频段的平均值.
void InitLowFrequencyCorrectionRanges();
//根据阵列信息计算得到混跌频率,从混跌频率到二分之一采样率范围为高频部分,处理方式和200Hz~400Hz类似
void InitHighFrequencyCorrectionRanges();
//如果没有指定干扰方向的信息,则这里可以按照第一旁瓣接收到的信号做为干扰信号,也可以是目标方向6dB带宽之外的信号做为干扰信号.
void InitInterfAngles();
//目标方向的信号
void InitDelaySumMasks();
//计算目标信号的协方差矩阵
void InitTargetCovMats();
//计算散射协方差矩阵
void InitDiffuseCovMats();
//计算干扰方向的协方差矩阵
void InitInterfCovMats();
//归一化方法11.1
void NormalizeCovMats();
//g值计算核心函数
float CalculatePostfilterMask(const ComplexMatrixF&amp; interf_cov_mat,
float rpsiw,
float ratio_rxiw_rxim,
float rmxi_r);

// Prevents the postfilter masks from degenerating too quickly (a cause of
// musical noise).
void ApplyMaskTimeSmoothing();
void ApplyMaskFrequencySmoothing();

// The postfilter masks are unreliable at low frequencies. Calculates a better
// mask by averaging mid-low frequency values.
void ApplyLowFrequencyCorrection();

// Postfilter masks are also unreliable at high frequencies. Average mid-high
// frequency masks to calculate a single mask per block which can be applied
// in the time-domain. Further, we average these block-masks over a chunk,
// resulting in one postfilter mask per audio chunk. This allows us to skip
// both transforming and blocking the high-frequency signal.
void ApplyHighFrequencyCorrection();

// Compute the means needed for the above frequency correction.
float MaskRangeMean(size_t start_bin, size_t end_bin);

// Applies post-filter mask to |input| and store in |output|.
void ApplyPostFilter(const complex_f* input, complex_f* output);

void EstimateTargetPresence();

static const size_t kFftSize = 256;
static const size_t kNumFreqBins = kFftSize / 2 + 1;

// Deals with the fft transform and blocking.
size_t chunk_length_;
std::unique_ptr&lt;LappedTransform&gt; process_transform_;
std::unique_ptr&lt;PostFilterTransform&gt; postfilter_transform_;
float window_[kFftSize];

// Parameters exposed to the user.
const size_t num_input_channels_;
const size_t num_postfilter_channels_;
int sample_rate_hz_;

const std::vector&lt;Point&gt; array_geometry_;
// The normal direction of the array if it has one and it is in the xy-plane.
const rtc::Optional&lt;Point&gt; array_normal_;

// Minimum spacing between microphone pairs.
const float min_mic_spacing_;

// Calculated based on user-input and constants in the .cc file.
size_t low_mean_start_bin_;
size_t low_mean_end_bin_;
size_t high_mean_start_bin_;
size_t high_mean_end_bin_;

// Quickly varying mask updated every block.
float new_mask_[kNumFreqBins];
// Time smoothed mask.
float time_smooth_mask_[kNumFreqBins];
// Time and frequency smoothed mask.
float final_mask_[kNumFreqBins];

float target_angle_radians_;
// Angles of the interferer scenarios.
std::vector&lt;float&gt; interf_angles_radians_;
// The angle between the target and the interferer scenarios.
const float away_radians_;

// Array of length |kNumFreqBins|, Matrix of size |1| x |num_channels_|.
ComplexMatrixF delay_sum_masks_[kNumFreqBins];

// Arrays of length |kNumFreqBins|, Matrix of size |num_input_channels_| x
// |num_input_channels_|.
ComplexMatrixF target_cov_mats_[kNumFreqBins];
ComplexMatrixF uniform_cov_mat_[kNumFreqBins];
// Array of length |kNumFreqBins|, Matrix of size |num_input_channels_| x
// |num_input_channels_|. The vector has a size equal to the number of
// interferer scenarios.
std::vector&lt;std::unique_ptr&lt;ComplexMatrixF&gt;&gt; interf_cov_mats_[kNumFreqBins];

// Of length |kNumFreqBins|.
float wave_numbers_[kNumFreqBins];

// Preallocated for ProcessAudioBlock()
// Of length |kNumFreqBins|.
float rxiws_[kNumFreqBins];
// The vector has a size equal to the number of interferer scenarios.
std::vector&lt;float&gt; rpsiws_[kNumFreqBins];

// The microphone normalization factor.
ComplexMatrixF eig_m_;

// For processing the high-frequency input signal.
float high_pass_postfilter_mask_;
float old_high_pass_mask_;

// True when the target signal is present.
bool is_target_present_;
// Number of blocks after which the data is considered interference if the
// mask does not pass |kMaskSignalThreshold|.
size_t hold_target_blocks_;
// Number of blocks since the last mask that passed |kMaskSignalThreshold|.
size_t interference_blocks_count_;
};
</code></pre>
<h3 id="权重计算函数">权重计算函数</h3>
<pre><code class="lang-cpp">
<span class="hljs-keyword">void</span> NonlinearBeamformer::ProcessAudioBlock(<span class="hljs-keyword">const</span> complex_f* <span class="hljs-keyword">const</span>* input,
<span class="hljs-keyword">size_t</span> num_input_channels,
<span class="hljs-keyword">size_t</span> num_freq_bins,
<span class="hljs-keyword">size_t</span> num_output_channels,
complex_f* <span class="hljs-keyword">const</span>* output) {
RTC_CHECK_EQ(kNumFreqBins, num_freq_bins);
RTC_CHECK_EQ(num_input_channels_, num_input_channels);
RTC_CHECK_EQ(<span class="hljs-number">0</span>, num_output_channels);
<span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> 0</span>
<span class="hljs-keyword">for</span>(<span class="hljs-keyword">size_t</span> ch =<span class="hljs-number">0</span>; ch &lt;num_input_channels_; ++ch ){
<span class="hljs-keyword">for</span>(<span class="hljs-keyword">size_t</span> f_ix=<span class="hljs-number">0</span>; f_ix &lt; kNumFreqBins; ++ f_ix)
<span class="hljs-built_in">std</span>::<span class="hljs-built_in">cout</span> &lt;&lt; <span class="hljs-string">"fft out["</span> &lt;&lt;ch &lt;&lt;<span class="hljs-string">"]["</span> &lt;&lt; f_ix &lt;&lt; <span class="hljs-string">"]"</span>&lt;&lt; input[ch][f_ix] &lt;&lt; <span class="hljs-built_in">std</span>::<span class="hljs-built_in">endl</span>;
}

<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span>
<span class="hljs-built_in">std</span>::<span class="hljs-built_in">cout</span> &lt;&lt;<span class="hljs-string">"NonlinearBeamformer::ProcessAudioBlock"</span> &lt;&lt; <span class="hljs-built_in">std</span>::<span class="hljs-built_in">endl</span>;
<span class="hljs-comment">// Calculating the post-filter masks. Note that we need two for each</span>
<span class="hljs-comment">// frequency bin to account for the positive and negative interferer</span>
<span class="hljs-comment">// angle.</span>
<span class="hljs-comment">//对每个频点进行的操作</span>
<span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = low_mean_start_bin_; i &lt;= high_mean_end_bin_; ++i) {
<span class="hljs-comment">//计算输入信号的归一化矩阵,实际上是噪声+目标+干扰的结果</span>
eig_m_.CopyFromColumn(input, i, num_input_channels_);
<span class="hljs-keyword">float</span> eig_m_norm_factor = <span class="hljs-built_in">std</span>::<span class="hljs-built_in">sqrt</span>(SumSquares(eig_m_));
<span class="hljs-keyword">if</span> (eig_m_norm_factor != <span class="hljs-number">0.f</span>) {
eig_m_.Scale(<span class="hljs-number">1.f</span> / eig_m_norm_factor);
}
<span class="hljs-comment">//11.1中的范数归一化,由此计算得到(目标+噪声+干扰)的能量.</span>
<span class="hljs-keyword">float</span> rxim = Norm(target_cov_mats_[i], eig_m_);
<span class="hljs-keyword">float</span> ratio_rxiw_rxim = <span class="hljs-number">0.f</span>;
<span class="hljs-keyword">if</span> (rxim &gt; <span class="hljs-number">0.f</span>) {
ratio_rxiw_rxim = rxiws_[i] / rxim;
}
<span class="hljs-comment">//计算得到目标的能量</span>
complex_f rmw = <span class="hljs-built_in">abs</span>(ConjugateDotProduct(delay_sum_masks_[i], eig_m_));
rmw *= rmw;
<span class="hljs-keyword">float</span> rmw_r = rmw.real();
<span class="hljs-comment">//根据干扰和目标计算得到权重值</span>
new_mask_[i] = CalculatePostfilterMask(*interf_cov_mats_[i][<span class="hljs-number">0</span>],
rpsiws_[i][<span class="hljs-number">0</span>],
ratio_rxiw_rxim,
rmw_r);
<span class="hljs-comment">//对于面阵,或者有明确干扰源的情况下,还要进行计算</span>
<span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> j = <span class="hljs-number">1</span>; j &lt; interf_angles_radians_.size(); ++j) {
<span class="hljs-keyword">float</span> tmp_mask = CalculatePostfilterMask(*interf_cov_mats_[i][j],
rpsiws_[i][j],
ratio_rxiw_rxim,
rmw_r);
<span class="hljs-comment">//保守的权值跟新策略</span>
<span class="hljs-keyword">if</span> (tmp_mask &lt; new_mask_[i]) {
new_mask_[i] = tmp_mask;
}
}
}
<span class="hljs-comment">//在时域上对权值进行平滑</span>
ApplyMaskTimeSmoothing();
<span class="hljs-comment">//低频情况下使用平均代替单个频点</span>
ApplyLowFrequencyCorrection();
<span class="hljs-comment">//混叠频率的处理和低频情况下一致</span>
ApplyHighFrequencyCorrection();
<span class="hljs-comment">//频域上的平滑处理</span>
ApplyMaskFrequencySmoothing();
}
</code></pre>
<h3 id="权重相乘操作">权重相乘操作</h3>
<pre><code class="lang-cpp"><span class="hljs-keyword">void</span> PostFilterTransform::ProcessAudioBlock(<span class="hljs-keyword">const</span> <span class="hljs-keyword">complex</span>&lt;<span class="hljs-keyword">float</span>&gt;* <span class="hljs-keyword">const</span>* input,
<span class="hljs-keyword">size_t</span> num_input_channels,
<span class="hljs-keyword">size_t</span> num_freq_bins,
<span class="hljs-keyword">size_t</span> num_output_channels,
<span class="hljs-keyword">complex</span>&lt;<span class="hljs-keyword">float</span>&gt;* <span class="hljs-keyword">const</span>* output) {
RTC_DCHECK_EQ(num_freq_bins_, num_freq_bins);
RTC_DCHECK_EQ(num_input_channels, num_output_channels);
<span class="hljs-built_in">printf</span>(<span class="hljs-string">"PostFilterTransform::ProcessAudioBlock, \n"</span>);
<span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> ch = <span class="hljs-number">0</span>; ch &lt; num_input_channels; ++ch) {
   <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> f_ix = <span class="hljs-number">0</span>; f_ix &lt; num_freq_bins_;                  ++f_ix) {
     output[ch][f_ix] =kCompensationGain *    final_mask_[f_ix] * input[ch][f_ix];

   }
}
</code></pre>
<h2 id="本章小节">本章小节</h2>
<p class="comments-section">本章主要根据开源的波束算法进行剖析额算法的原理,算法的公式,和相关核心公式的代码实现,要了解算法 的精髓还是需要源码一行行的看的.该算法既有delay-sum经典的算法又有T-Fmasking的思想在里面了,弥补了各自的不足,dleay-sum是线性处理,而T-F masking是非线性处理,实际上在有些场景下线性是适用的,在有些场景下非线性是适用的(T-F masking只是一种特例),所以经常需要考虑将多个算法恰到好处的综合成一个,这个难度还是有的,一种新的处理方法正能实现这种综合功能而具有旺盛的生命力,就是基于深度学习的方法,下一章将浅析这一算法.<div class="comments-icon"></div></p>

<h1 id="第十二章-波束形成进阶">第十二章 波束形成进阶</h1>
<p class="comments-section">Last chapter a beamforming example based on T-F masking method already analysised. <div class="comments-icon"></div></p>
<p class="comments-section">上一章分析了t-F masking掩码的实例,并给了代码下载的github地址.<div class="comments-icon"></div></p>
<p class="comments-section">That method is based on the geometray of microphone array and DOA(usually base on SRP-PHATH algorithm) estimate. Several questions need to be step furtuer.<div class="comments-icon"></div></p>
<p class="comments-section">那个方法基于麦克风阵列坐标以及先验的声源风向(通常基于SRP-PHAT计算获得).这种方法是大多数基于波束形成的声源分离方法.有些地方还是值得推敲的.<div class="comments-icon"></div></p>
<ol>
<li>In masse production, if the array corrdinate error is about $$\pm 1mm$$, what's the influnce. </li>
</ol>
<ol>
<li><p class="comments-section">对于大型生产场景,如果麦克风坐标误差对波束形成的影响如何?<div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">DOA不准的时候,波束形成的结果是怎样的?(在其它算法中被称为鲁棒性,如MVDR中会使用对角阵的方式增加鲁棒性)<div class="comments-icon"></div></p>
</li>
</ol>
<p class="comments-section">如何将上面两个对波束形成的因数去除掉?基于盲源分离(PCA,主成份分析)是一种方法,但是这里要分析的依然还是基于波束形成的,但是波束指向的方向向量不再依赖于DOA结果,而是根据观测到的信号自主决定,这就是chime3挑战赛中的方法CGMM.如果细致跟过前面章节中VAD检测的的ML方法,那么这里的方法理解起来也不难.<div class="comments-icon"></div></p>
<h2 id="高斯混合模型">高斯混合模型</h2>
<h3 id="一维高斯分布">一维高斯分布</h3>
<p class="comments-section">$$p(x) = \sum \limits<em>{i=1}^{K}\phi_i N(x|\mu_i, \sigma_i), \sum \limits</em>{i=1}^K \phi_i=1 \tag{12.1}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$N(x|\mu_i, \sigma_i) = \frac{1}{\sigma_i \sqrt{2\pi}}exp(-\frac{(x-\mu_i)^2}{2\sigma_i^2}) \tag{12.2}$$<div class="comments-icon"></div></p>
<h3 id="多维高斯分布">多维高斯分布</h3>
<p class="comments-section">$$p(\vec x) = \sum \limits<em>{i=1}^K \phi_i N(\vec x| \vec \mu_i, \Sigma_i), \sum \limits</em>{i=1}^K \phi_i=1 \tag{12.3}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$N(\vec x| \vec \mu_i, \Sigma_i) = \frac{1}{\sqrt{(2\pi)^K|\Sigma_i|}}exp(- \frac{1}{2}(\vec x - \vec \mu_i)^T\Sigma_i^{-1}(\vec x - \vec \mu_i)) \tag{12.4}$$<div class="comments-icon"></div></p>
<h2 id="cgmm方法">CGMM方法</h2>
<p class="comments-section">CGMM(complex gaussian mixture model)是基于混合高斯模型的方法,和之前的波束形成方法相比,差异在于根据高斯混合模型求解指向目标方向的向量.这个方法的处理框图如下:
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/CGMM.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">CGMM波束形成架构<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">该波束形成方法包括波束形成,指向向量以及时频掩码三个模块,这三个模型的最终输出就是增强后的信号.<div class="comments-icon"></div></p>
<h3 id="阵列接收模型假设">阵列接收模型假设</h3>
<p class="comments-section">设$$k \in {1,2, \cdots, K}$$是声源目标的索引,$$m
 \in {1,2, \cdots, M}$$是麦克风的索引.在时域每个麦克风接收到的信号可以写做如下形式:<div class="comments-icon"></div></p>
<p class="comments-section"> $$y<em>m(t) = \sum \limits_k \sum \limits</em>{\tau}h_m^{(k)}(\tau)s^{(k)}(t-\tau) + n_m(t) \tag{12.5}$$
上式中, $$s^{(k)}(t)$$和$$n_m(t)$$分别表示第k个声源和以及第m个麦克采集到的噪声,$$h_i^{(k)}(\tau)$$是第k个声源对第m个麦克的脉冲相应函数.对12.5使用STFT变换后可得:<div class="comments-icon"></div></p>
<p class="comments-section">$$y_m(f,t)=\sum \limits_k h_m^{(k)}(f) s^{(k)}(f,t) +n_m(f,t) \tag{12.6}$$
向量表示法如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\begin{bmatrix}
 y_1\ y_2 \ \vdots \ y_m 
\end{bmatrix}= 
\begin{bmatrix}
 h_1^1 &amp;&amp; \cdots &amp;&amp; h_1^m\ 
 y_2^1 &amp;&amp; \cdots &amp;&amp; h_2^m \ 
 \vdots 
 \ y_m^1 &amp;&amp; \cdots  &amp;&amp; y_m^m
\end{bmatrix}<div class="comments-icon"></div></p>
<p class="comments-section">\begin{bmatrix}
 s^1 \ 
 s^2 \ 
 \vdots 
 \ s^m
\end{bmatrix} + 
\begin{bmatrix}
 n_1 \ 
 n_2 \ 
 \vdots 
 \ n_m
\end{bmatrix} = 
\mathbf {y}(f,t) = 
\sum \limits_k \mathbf r^{(k)}(f)s^{(k)}(f,t)+\mathbf n(f,t) \tag{12.7}<div class="comments-icon"></div></p>
<p>$$</p>
<h3 id="波束形成">波束形成</h3>
<p class="comments-section">波束形成方法采用了MVDR,该方法对频域信号乘以权重获得到原始信号的估计:<div class="comments-icon"></div></p>
<p class="comments-section">$$\hat s<em>{f,t}^{(k)} = \mathbf {w_f^{(k)}}^H \mathbf y</em>{f,t} \tag{12.8}$$<div class="comments-icon"></div></p>
<p class="comments-section"> $$\hat s_{f,t}^{(k)}$$是第k个目标信号的估计值,$$H$$是共轭转置(频域信号), 最小均方误差准则的限制条件是$$\mathbf w_f^{(k)H}\mathbf r_f^{(k)} = 1$$,则可以推导出MVDR波束形成的权重计算公式是:<div class="comments-icon"></div></p>
<p class="comments-section"> $$\mathbf w_f^{(k)} = \frac{R_f^{(y)-1} \mathbf r_f}{\mathbf r_f^{(k)H} R_f^{(y)-1}\mathbf r_f^{(k)}} \tag{12.9}$$
 其中$$R_f^{(y)}$$是根据观测信号得到的协方差矩阵:<div class="comments-icon"></div></p>
<p class="comments-section"> $$R<em>f^{(y)} = \frac{1}{T}\sum \limits_t  \mathbf y</em>{f,t} \mathbf y_{f,t}^H \tag{12.10}$$ 
 当然也可以使用其它波束形成算法,如多通道维纳滤波.<div class="comments-icon"></div></p>
<h3 id="指向向量">指向向量</h3>
<p class="comments-section">这个波束形成算法的关键在于指向向量的计算,该方法直接根据麦克风阵列的协方差矩阵求得,使用协方差矩阵的主特征向量做为波束指向向量,这非常类似盲源分离的PCA思想,协方差矩阵可以使用T-F masking获得.
设$$\lambda<em>{f,t}^{(k+n)}$$和$$\lambda</em>{f,t}^{(n)}$$是时频点$$(f,t)$$第k阶带噪信号和只有噪声信号的掩码(概率).
则带噪语音信号和只有噪声信号的协方差矩阵可以表示如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$R<em>f^{(k+n)}= \frac{1}{\Sigma_t \lambda</em>{f,t}^{(n)}}\sum \limits<em>t \lambda</em>{f,t}^{(k+n)}\mathbf y<em>{f,t} \mathbf y</em>{f,t}^H \tag{12.11}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$R<em>f^{(n)} = \frac{1}{\Sigma_t \lambda</em>{f,t}^{(n)}}\sum<em>t \lambda</em>{f,t}^{(n)}\mathbf y<em>{f,t}\mathbf y</em>{f,t}^H \tag{12.12}$$<div class="comments-icon"></div></p>
<p class="comments-section">假设噪声是不相关的,则估计的纯净语音信号由下式计算:<div class="comments-icon"></div></p>
<p class="comments-section">$$R_f^{(k)} = R_f^{(k+n)}-R_f^{(n)} \tag{12.13}$$<div class="comments-icon"></div></p>
<p class="comments-section">则根据12.13可知,第k个声源的指向向量可以通过对$$R_f^{(k)}$$使用特征向量分解法求得,即最大特征值对应的特征向量做为指向向量.<div class="comments-icon"></div></p>
<h3 id="时频掩码估计">时频掩码估计</h3>
<h4 id="复高斯混合模型">复高斯混合模型</h4>
<p class="comments-section">时频掩码基于复高斯混合模型,
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/gmm.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图12.2 高斯混合模型图<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">基于语音信号在时频域的稀疏性,多通道的语音生成模型可以使用$$K+1$$个的高斯分布来表示,每一个对应于一个带噪语音类别.(这里的分类和VAD里的不太一致的地方,在于,其将带噪语音信号类别的总数被聚类到K+1个),混合高斯模型的参数估计使用EM算法.
式$$16.7$$可以写为:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf y<em>{f,t} = \mathbf r_f^{(v)}s</em>{f,t}^{(v)}, d<em>{f,t}=v \tag{12.14}$$
其中,$$d</em>{f,t}$$是时频点$$(f,t)$$所属类别索引,$$v$$可以取$$k+n(k=1 \cdots K)或者n$$,分别表示第$$k$$个带噪语音信号和噪声信号.$$s<em>{f,t}^{(k+n)}$$表示的是第k个类表示的带噪语音信号.$$s</em>{f,t}^{(n)}$$是时频点$$(f,t)$$的噪声信号表示.这里假设带噪语音信号和噪声信号的指向向量是时不变的.
基于上述观测模型,可以定义生成模型,假设$$s_{f,t}^{(v)}$$复合复高斯分布:<div class="comments-icon"></div></p>
<p class="comments-section">$$s<em>{f,t}^{(v)} \sim N_c(0, \phi</em>{f,t}^{(v)}) \tag{12.15}$$
其中,$$\phi_{f,t}^{(v)}$$是方差, $$N_c(x|\mu, \Sigma) = \frac{1}{\pi \Sigma}exp - \frac{|x-\mu|^2}{\Sigma}$$.从式12.15和式12.14可得多通道高斯分布如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf y<em>{f,t}|d</em>{f,t} = \mathbf v \sim N<em>c(0, \phi</em>{f,t}^{(v)}\mathbf R_f{(v)}) \tag{12.16}$$,<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf R_f^{(v)}$$是$$\mathbf r_f^{(v)}\mathbf r_f^{(v)H}$$,式子12.16中的$$N_c(\mathbf x|\mu, \Sigma)=\frac{1}{|\pi \Sigma|}exp(-(\mathbf x -\mathbf \mu)^{H}\Sigma^{-1}(\mathbf x - \mu))$$,使用混合权重可以将式子12.16转为下式:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf y<em>{f,t} \sim \sum \limits_v \alpha_f^{(v)}N_c(0,\phi</em>{f,t}^{(v)}\mathbf R_f^{(v)}),\Sigma_v \alpha_f^{(v)}=1 \tag{12.17}$$
对$$\mathbf r_f^{(v)}$$的估计通过估计$$R_f^{(v)}$$获得.<div class="comments-icon"></div></p>
<h4 id="基于em算法的参数估计">基于EM算法的参数估计</h4>
<p class="comments-section">CGMM参数,如$$\alpha<em>f^{(v)}, \phi</em>{f,t}^{(v)},\mathbf R_f^{(v)}$$使用最大似然法估计(maximum likelihood,ML).对于上一小节提出的复高斯混合模型,其对数目标函数是:<div class="comments-icon"></div></p>
<p class="comments-section">$$\log p(\mathbf y|\Theta) = \sum \limits<em>{f,t} \log \sum \limits_v \alpha_f^{(v)}N_c(\mathbf y</em>{f,t}|0, \phi_{f,t}^{(v)}\mathbf R_f^{(v)}), \sum \limits_v \alpha_f^{(v)}=1\tag{12.18}$$
这里$$\Theta$$是CGMM的参数集.在每一次EM计算中的Q函数定义如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$Q(\Theta|\Theta^{'}) = E[\log p(\mathbf y | \Theta, \mathbf d)]<em>{\mathbf d} = \sum \limits</em>{f,t} \sum \limits<em>{v} \lambda</em>{f,t}^{(v)}\log \alpha<em>f^{(v)}N_c(\mathbf y</em>{f,t}|0, \phi_{f,t}^{(v)}\mathbf R_f^{(v)}) \tag{12.19}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$E[\cdot ]<em>x$$表示的是$$x$$的期望,$$\Theta^{'}$$是前一次的参数估计集合,$$\lambda</em>{f,t}^{(v)}(满足\Sigma<em>v \lambda</em>{f,t}^{(v)}=1)$$是$$d_{f,t}$$属于$$v$$的后验概率.在E步骤中后验概率的计算公式是:<div class="comments-icon"></div></p>
<p class="comments-section">$$\lambda<em>{f,t}^{(v)} \leftarrow \frac{\alpha_f^{(v)}p(\mathbf y</em>{f,t}|d<em>{f,t}=v, \Theta^{'})}{\sum_v \alpha_f^{(v)}p(\mathbf y</em>{f,t}|d<em>{f,t}=v,\Theta^{'})} \tag{12.19}$$
其中$$p(\mathbf y</em>{f,t}|d<em>{f,t}=v, \Theta^{'}) = N_c(\mathbf y</em>{f,t}|0, \phi_{f,t}^{(v)}\mathbf R_f^{(v)})$$.在M步骤,CGMM参数估计如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$\phi<em>{f,t}^{(v)} \leftarrow \frac{1}{M}tr(\mathbf y</em>{f,t} \mathbf y_{f,t}^H \mathbf R_f^{(v)-1}) \tag {12.20}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf R<em>f^{(v)} \leftarrow \frac{1}{\sum_t \lambda</em>{f,t}^{(v)}}\sum \limits<em>v \lambda</em>{f,t}^{(v)} \frac{1}{\phi<em>{f,t}^{(v)}} \mathbf y</em>{f,t} \mathbf y_{f,t}^H \tag{12.21}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$\alpha<em>f^{(v)} \leftarrow \frac{1}{T} \sum_t \lambda</em>{f,t}^{(v)} \tag{12.22}$$
在EM步骤收敛后,就可以认为T-F掩码就是对应的$$\lambda_{f,t}^{n}$$值.
对于单声源目标情况,E-M算法的初始设置,可以使用单位阵做为噪声的空域互相关矩阵,使用观测的信号所得的互相关矩阵做为带噪信号的互相关矩阵.多声源情况,可以对观测到的信号的互相关矩阵加满秩随机矩阵做为带噪信号的互相关矩阵.
对于matlab代码以及所实现的效果,可以参考
<a href="https://github.com/shichaog/CGMM_BF" target="_blank">CGMM 波束形成git网址</a><div class="comments-icon"></div></p>
<h4 id="排列问题">排列问题</h4>
<p class="comments-section">EM算法收敛后,需要对每个频点的带噪语音和噪声进行聚类,这被称为排列问题.这在盲源分离时也会遇到.<div class="comments-icon"></div></p>
<h2 id="基于cgmm方法的在线波束形成">基于CGMM方法的在线波束形成</h2>
<p class="comments-section">假设语音信号根据,以batch形式获得,每一个batch 按照时间序列依次被采集到,batch的索引$$l \in {1, \cdots, L}$$,$$B<em>l$$是第l个batch所包含的数据,对于第$$l$$个batch,$$\lambda</em>{f,t}^{(v)}$$和$$\phi<em>{f,t}^{(v)}$$根据式12.19和12.20求得,其中的$$\mathbf R</em>{f,l}^{(v)}$$则根据第$$l-1$$的$$\mathbf R_{f,l-1}^{(v)}$$迭代获得,将式12.21修改为下式:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf R<em>{f,l}^{(v)} \leftarrow \frac{\Lambda</em>{f,l-1}^{(v)} }{\Lambda<em>{f,l-1}^{(v)}+\sum</em>{t \in B<em>l} \lambda</em>{f,t}^{(v)}}\mathbf R<em>{f,l-1}^{(v)} +
\frac{1}{\Lambda</em>{f,l-1}^{(v)}+\sum<em>{t \in B_l}\lambda</em>{f,t}^{(v)}}\sum \limits{t \in B<em>l} \lambda</em>{f,t}^{(v)} \frac{1}{\phi<em>{f,t}^{(v)}} \mathbf y</em>{f,t} \mathbf y_{f,t}^H \tag{12.23}<div class="comments-icon"></div></p>
<p class="comments-section">$$
这里$$\Lambda<em>{f,l}^{(v)}$$是多有帧$$\lambda</em>{f,t}^{(v)}$$的和,其递归跟新方法是:<div class="comments-icon"></div></p>
<p class="comments-section">$$\Lambda<em>{f,l}^{(v)} \leftarrow \Lambda</em>{f,l-1}^{(v)} + \sum \limits<em>{t \in B_l} \lambda</em>{f,t}^{(v)} \tag{12.24}$$
使用序列方法估计的时频掩码$$\lambda_{f,t}^{(v)}$$,则实时波束形成如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$ R<em>{f,l}^{(v)} \leftarrow \frac{\Lambda</em>{f,l-1}^{(v)} }{\Lambda<em>{f,l-1}^{(v)}+\sum</em>{t \in B<em>l} \lambda</em>{f,t}^{(v)}} R<em>{f,l-1}^{(v)} +
\frac{1}{\Lambda</em>{f,l-1}^{(v)}+\sum<em>{t \in B_l}\lambda</em>{f,t}^{(v)}}\sum \limits<em>{t \in B_l} \lambda</em>{f,t}^{(v)} \mathbf y<em>{f,t} \mathbf y</em>{f,t}^H \tag{12.25}<div class="comments-icon"></div></p>
<p class="comments-section">$$
波束指向向量$$\mathbf r$$依然是根据$$\mathbf R$$计算获得.然后$$B_l$$内的带噪语音使用MVDR方法获得增强后的信号.带噪语音的协方差矩阵的跟新方式如下:<div class="comments-icon"></div></p>
<p class="comments-section">$$ R<em>{f,t}^{(y)} \leftarrow  \frac{T</em>{l-1}}{T<em>l}  R</em>{f, l-1}^{(y)}+ \frac{1}{T<em>l}\sum \limits</em>{t \in B<em>l}\mathbf y</em>{f,t}\mathbf y_{f,t}^H \tag{12.26}$$
其中$$T_l$$是到第$$l$$个batch为止观测到的帧总数.在实时计算算法流程是:<div class="comments-icon"></div></p>
<blockquote>
<p>初始化</p>
<p class="comments-section">$$R<em>{f,0}^{(y)} \leftarrow \mathbf 0, R</em>{f,0}^{(k+n)} \leftarrow \mathbf 0, 
R<em>{f,0}^{(n)} \leftarrow \mathbf 0,
\Lambda</em>{f,0}^{(k+n)} \leftarrow 0, 
\Lambda<em>{f,0}^{(n)} \leftarrow 0$$
使用前文提到的方法或者训练数据训练得到:$$\mathbf R</em>{f,0}^{k+n}$$和$$\mathbf R_{f,0}^{(n)}$$
for l=1 to L in all k and all f do<div class="comments-icon"></div></p>
<ol>
<li>根据12.19和12.20对$$t \in B<em>l$$计算$$\lambda</em>{f,t}^{(k+n)}, \lambda<em>{f,t}^{(n)}, \phi</em>{f,t}^{(k+n)}, \phi_{f,t}^{(n)} $$</li>
<li>根据12.23跟新$$\mathbf R<em>{f,l}^{(k+n)}$$和$$\mathbf R</em>{f,l}^{(n)}$$</li>
<li>根据12.25跟新$$R<em>{f,l}^{(k+n)}$$和$$R</em>{f,l}^{(n)}$$</li>
<li>根据12.13跟新$$R_{f,l}^{(k)}$$</li>
<li>提取$$R<em>{f,l}^{(y)}$$的特征向量做为波束形成的指向向量$$\mathbf r</em>{f,l}^{(k)}$$</li>
<li>根据12.26更新$$R_{f,l}^{(y)}$$</li>
<li>根据12.9更新$$\mathbf w_{f,l}^{(k)}$$</li>
<li>根据12.8做波束形成,得到语音的估计$$\hat s_{f,t}^{(k)}, t \in B_l$$</li>
<li>根据12.24更新$$\Lambda<em>{f,l}^{(k+n)}$$和$$\Lambda</em>{f,l}^{(n)}$$
end for </li>
</ol>
</blockquote>

<h1 id="第十三章-基于深度学习deep-learning的语音增强">第十三章 基于深度学习(deep learning)的语音增强</h1>
<p class="comments-section">深度学习方法正成为新的解决语音增强的方法,由于以物理模型为基础的方法都有诸多的前提假设,比如在去混响时,会假设语音符合高斯或者拉普拉斯分布,实际上这些假设和真实场景上还有差距,深度学习方法却可以在接近真实场景上更进一步.<div class="comments-icon"></div></p>
<p class="comments-section">在第六章的NS算法中提到,在判决是否是噪声时,使用到了频谱平坦度,谱减法,语音噪声高斯统计模型以及子带和频点处理相关技术.在深度学习方法中,统计模型不再是一开始认为将其设置成高斯模型,而是通过训练数据训练出来,同样频带的数量也是可以通过深度学习方法习得.<div class="comments-icon"></div></p>
<p class="comments-section">随输入特征选取的不同,网络的结构会有差异,有使用类似ASR的特征做为DL输入特征,也有使用幅度和相位做为输入特征的处理方法.<div class="comments-icon"></div></p>
<p class="comments-section">DL可以用于从第一章至此的任意一个章节,并且谷歌和亚马逊在他们的实现中,服务器测还采用了DL方法进行的语音增强.<div class="comments-icon"></div></p>
<h2 id="开发环境pycharm">开发环境pycharm</h2>
<pre><code>sudo add-apt-repository ppa:ubuntu-desktop/ubuntu-make
sudo apt-get update
sudo apt-get install ubuntu-make

umake ide pycharm
umake -r ide pycharm
</code></pre><p class="comments-section"><a href="https://www.tensorflow.org/" target="_blank">tensorflow官网链接</a>
__<div class="comments-icon"></div></p>
<p class="comments-section">本书是为ASR服务的,所以不得不提到前端语音增强和ASR分离带来影响问题,从前文可以看出NS/AEC/BF/DR/DOA/VAD等这些算法除了基于有限的统计模型的组合外,还有这些模型推导公式所使用的判决准则,如最小均方误差准则,最大声学似然比,不仅是这些判决准则不是以ASR为目标的,就连处理过程(如:NS的谱减法等,实际上就是没有关注SDR, signal distortion ratio)也不是以ASR为目标的,所以如果单单是使用深度学习的方法进行NS,或者单单做去混响操作,方法本身没有问题,但是判决准则同样不是以ASR的识别率为目标的,这样的化从各个模块来看是可以达到模块最优,但是以ASR这一全局的结果来看,这些局部最优组合起来未必能够达到全局最优,所以更好的方式是以ASR识别率做为检验的标准,声学模型用多通道的数据(当前训练数据相对少些)进行训练,这样语音增强算法和声学模型算法可以更好的融合(它们的目标是一致的:ASR识别率,模型之间在训练中可以进行反馈和调整,这样可以达到全局最优).所以章就以带识别情况为例说明深度学习在语音增强上面的一些应用思路,如果对语音识别不太熟悉,不妨先看第17章.<div class="comments-icon"></div></p>
<p class="comments-section"><a href="http://www.iqiyi.com/paopao/u/1427478235/#curid=10587668609_1d541fae605e002809555987e9aff3f5" target="_blank">CNN卷积运算过程例1</a>
<a href="http://www.iqiyi.com/paopao/u/1427478235/#curid=10587677409_987d1106ec5d6aa165ca3a86c2a362bc" target="_blank">CNN卷积运算过程例2</a>
<a href="http://blog.csdn.net/shichaog/article/details/72853665" target="_blank">RNN/LSTM/GRU简介</a><div class="comments-icon"></div></p>
<h2 id="beamformering-dnn">beamformering DNN</h2>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/Screenshot from 2017-12-15 15:41:55.png" alt=""></p>
<blockquote>
<p class="comments-section">图12.1 深度学习波束形成
注:摘自[Deep BEAMFORMING NETWORKS FOR MULTI-CHANNEL SPEECH RECOGNITION]<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">从图12.1可以看出,该特征提取模块和大多数基于DNN方法的ASR识别方法没有大的差异,bf的模块主要是训练波束形成的权重,对于输入的阵列信号,分成两个部分,一个部分是做STFT,每一个通道都获得复数的频谱$$z<em>{t,f,m}$$,t是时间戳,f是频率点,m是麦克风的通道号;另外一路是做GCC-PHAT,这常用于DOA算法,且类似delay-sum的方法也是先根据方向在做波束形成的,也就是波束形成的权重是和声源的方向有关的,这边的beamforming DNN模块的输入是GCC-PHAT值,这样获得的信息要比具体的声源目标位置要多,所以理论上权重也应该更优.mean pooling是对采集到的每句话做均值平均,这样会得到权重$$w</em>{f,m}$$,根据这个权重和STFT的结果相称后得到频域权重.
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/Screenshot from 2017-12-15 15:55:18.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图12.2 权重举例<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">还有一篇文章也是非常的有启发.文章指出WER有5%的提升.该文章基于的思想是DNN可以将特征提取和分类问题合二为一,这样的化可以使用原始数据输入这个网络,而非向上一篇文章需要现在特征提取,这样可以联合高效进行特征提取和分类.
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/12_2.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图12.2 多通道裸wav CLDNN架构
注:摘自[Multichannel Signal Processing with Deep Neural Networks for Automatic Speech Recognition]<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">该文章采用卷积LSTM深度网络声学模型,第一层通过短时多通道卷积滤波器将多通道数据映射成单通道数据,黄色的部分是空域滤波器,所用神经网络使用异步随机梯度下降法(asynchronous stochastic gradient descent ASGD)减小的交叉熵(cross-entropy CE)来进行训练.<div class="comments-icon"></div></p>
<p class="comments-section">该多通道CLDNN波束形成思想源于filter-sum方法,对每个麦克风使用FIR滤波器滤波后再相加,这一过程可以用下式表示:<div class="comments-icon"></div></p>
<p class="comments-section">$$y[n] = \sum \limits<em>{c=0}^{C-1} \sum \limits</em>{n=0}^{N-1}h_c[n] x_c[t-n-\tau_c] \tag{12.1}$$
其中n是滤波器阶数,c是麦克风通道数,$$h_c[n]$$是滤波器系数,$$x[t]$$是原始麦克风数据,$$y[n]$$是输出信号.<div class="comments-icon"></div></p>
<p class="comments-section">式12.1中$$\tau$$值常通过DOA获得的获得,$$h[n]$$通过MVDR
等准则获得,而在图12.2中获得的方式有所不同,图中将延迟和滤波器系数根据声学模型最优准则进行联合训练.<div class="comments-icon"></div></p>
<p class="comments-section">该模型使用一组p通道的滤波器获取延迟$$\tau$$,滤波器$$p \in {0,\cdots, P-1}$$的输出可以写成下式:<div class="comments-icon"></div></p>
<p class="comments-section">$$y^p[t] = \sum \limits<em>{c=0}^{C-1} \sum \limits</em>{n=0}^{N-1}h<em>c^p[n]x_c[t-n] = \sum \limits</em>{c=0}^{C-1}x<em>c[t]<em>h_c^p \tag{12.2}$$
这里的$$h_c^p[n]$$是延迟和FIR滤波器的综合.$$</em>$$是卷积运算.
公式12.2可以使用多通道卷积运算实现,图12.2中的$${ x_0[t],x_1[t], \cdots, x</em>{C-1}[t]}, t \in {1,\cdots, M}$$,每个通道数据$$x_c$$(数据长度是M)都和P个滤波器(滤波器阶数是N)组进行卷积$$h_c[n] = { h_c^1[n], h_c^2[n], \cdots, h_c^P[n]}$$
卷积结果输出是$$y_c[t]  \in R^{(M-N+1) \times P}$$<div class="comments-icon"></div></p>
<p class="comments-section">$$y_c[t]$$对c个通道按时域进行max pool后(丢失短时相位信息)得到输出$$y[t] \in R^{1 \times P}$$.然后进行非线性对数压缩获得输出$$z[l]$$.
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/Screenshot from 2017-12-15 17:45:02.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图12.3 滤波器系数和其空域波束响应<div class="comments-icon"></div></p>
</blockquote>
<h2 id="本章小节">本章小节</h2>
<p class="comments-section">至此本章把语音增强的大致方法理了一个脉络,但是还是有很多点没有阐述,比如对于BF以及VAD等也只是阐述了基本原理和一个实例,并没有遍尝绝大部分方法,也就没法给出方法之间的性能差异,再比如对于盲源分离问题,并没有给出实例,另外对于ASR云端服务,也没有阐述数据压缩的方法和压缩实例.
本章简介了一下深度学习在语音增强上的应用,实际上从上文的分析来看,对于ASR识别场景,语音增强更适合于混合如声学模型的训练网络,根据最终的ASR识别率来反馈调节各个具体的语音增强模块和声学模型模块.
所以本章大致介绍了语音增强在深度学习的网络结构,并着重分析了一篇声学模型和波束形成混合的一篇文章,这篇文章对于使用深度学习方法解决识别问题很有裨益.<div class="comments-icon"></div></p>
<p class="comments-section">这里匆匆结束语音增强相关的内容,在下一章将进入语音识别方面的内容,并且深度学习的方法也都放在ASR的内容进行讨论.<div class="comments-icon"></div></p>

<h1 id="第十四章-kaldi入门">第十四章 kaldi入门</h1>
<h2 id="代码下载和安装">代码下载和安装</h2>
<pre><code class="lang-sh"><span class="hljs-variable">$git</span> <span class="hljs-built_in">clone</span> https://github.com/kaldi-asr/kaldi
</code></pre>
<p class="comments-section">根据INSTALL文件，先完成tools文件夹下的编译和安装，然后在编译src目录下源码。<div class="comments-icon"></div></p>
<pre><code class="lang-sh"><span class="hljs-variable">$cd</span> kaldi/tools
<span class="hljs-variable">$extras</span>/check_dependencies.sh
<span class="hljs-variable">$make</span> -j4
</code></pre>
<p class="comments-section">然后切换到src目录<div class="comments-icon"></div></p>
<pre><code>$cd ../src
$./configure
$make depend
$make -j4
</code></pre><h2 id="kaldi依赖的工具">kaldi依赖的工具</h2>
<ol>
<li>OpenFst 加权有限自动状态转换器（Weighted Finite State Transducer）</li>
<li>ATLAS/CLAPACK标准的线性代数库
这两个在解码和线性代数(矩阵类)计算时会用到.<a href="http://publications.idiap.ch/downloads/reports/2011/Povey_Idiap-RR-04-2012.pdf" target="_blank">kaldi工具的概貌文档</a>,初步入门,强烈建议先读此文.<h2 id="kaldi特征提取">kaldi特征提取</h2>
<h3 id="mfcc">MFCC</h3>
compute-mfcc-feats.cc<pre><code>Create MFCC feature files.
Usage:  compute-mfcc-feats [options...] &lt;wav-rspecifier&gt; &lt;feats-wspecifier&gt;
</code></pre>其中参数rspecifier用于读取.wav文件，wspecifier用于写入得到的MFCC特征。典型应用中，特征将被写入到一个大的”archive”文件，同时会写入一个”scp”文件用于随机存取。该程序并未提取delta特征（add-delats.cc）.
其–channel参数用于选择立体声情况（–channel=0， –channel=1）.<pre><code>compute-mfcc-feats --config=conf/mfcc.conf \
scp:exp/make_mfcc/train/wav1.scp    \
ark:/data/mfcc/raw_mfcc_train.1.ark;
</code></pre>第一个参数”scp:…”用于读取exp/make_mfcc/train/wav1.scp指定的文件。第二个参数”ark:…”指示计算得到的特征写入归档文件/data/mfcc/raw_mfcc_train.1.ark。归档文件里的每一句是N（frames）× N（mfcc）的特征矩阵。
MFCC特征的计算是在对象MFCC中的compute方法完成的，计算过程如下：
1.遍历每一帧（通常25ms一帧，10ms滑动）
2.对每一帧
a.提取数据，添加可选扰动，预加重和去直流，加窗
b.计算该点的能量（使用对数能量，而非C0）
c.做FFT并计算功率谱
d.计算每个梅尔频点的能量，共计23个重叠的三角频点，中心频率根据梅尔频域均匀分布。
e.计算对数能量，做离散余弦变换，保留指定的系数个数
f.倒谱系数加权，确保系数处于合理的范围。</li>
</ol>
<p class="comments-section">三角梅尔频点的上下限由–low-freq和–high-freq决定，通常被分别设置成接近0和奈奎斯特频率。如对于16KHz语音–low-freq=20, –high-freq=7800。
可以使用copy-feats.cc将特征转换成其它格式。<div class="comments-icon"></div></p>
<h3 id="倒谱均值和方差归一化">倒谱均值和方差归一化</h3>
<p class="comments-section">该归一化通常是为了获得基于说话人或者基于说话语句的零均值，单位方差归一化特征倒谱。但是并不推荐使用这个方法，而是使用基于模型的均值和方差归一化，如Linear VTLN（LVTLN）。可以使用基于音素的小语言模型进行快速归一化。特征提取代码compute-mfcc-feats.cc/compute-plp-feats.cc同样提供了–substract-mean选项获得零均值特征。如果要获得基于说话人或者基于句子的均值和方差归一化特征，可以使用
compute-cmvn-states.cc或者apply-cmvn.cc程序。
compute-cmvn-stats.cc将会计算均值和方差需要的所有统计量，并将这些统计信息以矩阵的形式写入到table中。<div class="comments-icon"></div></p>
<pre><code>compute-cmvn-stats 
          --spk2utt=ark:data/train/train.1k/spk2utt \
          scp:data/train.1k/feats.scp \
          ark:exp/mono/cmvn.ark;
</code></pre><h2 id="kaldi声学模型">kaldi声学模型</h2>
<ul>
<li>支持标准的基于ML训练的模型<ul>
<li>线性变换，如LDA，HLDA，MLLT/STC</li>
<li>基于fMLLR，MLLR的说话人自适应</li>
<li>支持混合系统</li>
</ul>
</li>
<li>支持SGMMs<ul>
<li>基于fMLLR的说话人识别</li>
</ul>
</li>
<li>模型代码，可以容易的修改扩展</li>
</ul>
<h2 id="声学模型训练过程">声学模型训练过程</h2>
<h3 id="1获得语料集的音频集和对应的文字集">1.获得语料集的音频集和对应的文字集</h3>
<p class="comments-section">可以提供更精确的对齐，发音（句子）级别的起止时间，但这不是必须的。<div class="comments-icon"></div></p>
<h3 id="2将获得的文字集格式化">2.将获得的文字集格式化</h3>
<p class="comments-section">kaldi需要各种格式类型的，训练过程中将会用到每句话的起止时间，每句话的说话人ID，文字集中用到的所有单词和音素。<div class="comments-icon"></div></p>
<h3 id="3从音频文件提取声学特征">3.从音频文件提取声学特征</h3>
<p class="comments-section">MFCC或者PLP被传统方法广泛使用。对于NN方法有所差异。<div class="comments-icon"></div></p>
<h3 id="4单音素训练">4.单音素训练</h3>
<p class="comments-section">单音素训练不使用当前音素之前或者之后的上下文信息，三音素则使用当前音素，之前的音素和之后的音素。<div class="comments-icon"></div></p>
<h3 id="5基于gmmhmm的框架">5.基于GMM/HMM的框架</h3>
<p class="comments-section">1.将音频根据声学模型对齐
声学模型的参数在声学训练时获得，然而，这个过程可以使用训练和对齐的循环进行优化。这也称之为维特比（Viterbi）训练（包括前向-后向以及期望最大化密集计算过程），通过将音频和文字对齐，可以使用额外的训练算法提升和精细化参数模型。所以，每一个训练步骤会跟随一个对齐步骤。<div class="comments-icon"></div></p>
<p class="comments-section">2.训练三音素模型
单音素模型仅仅表示了单个音素的参数，但是音素是随着上下文变化的，三音素模型使用上下文的前后音素展现了音素的变化。
并不是所有的单音素组合都存在于提供的文字集中，总共有$pohems^3$个可能的三音素，但是训练集所包含的是一个有限的子集，并且出现的三音素组合也要有一定的次数以方便训练，音素决策树会将这些三音素聚类成更小的集合。<div class="comments-icon"></div></p>
<p class="comments-section">3.根据声学模型重新对齐音频&amp;重新训练三音素模型
重复上述1,2步骤，并加入额外更精细的三音素模型训练，通常包括delta+delta-delta训练， LDA-MLLT以及SAT。对齐算法包括说话人对齐和FMLLR。<div class="comments-icon"></div></p>
<p class="comments-section">4.训练算法
Delta和Delta-delta算法计算特征的一阶和二阶导数，或者是动态参数以补充MFCC特征。
LDA-MLLT（Linear Discriminant Analysis – Maximum Likelihood Linear Transform）， LDA根据降维特征向量建立HMM状态。MLLT根据LDA降维后的特征空间获得每一个说话人的唯一变换。MLLT实际上是说话人的归一化。
SAT（Speaker Adaptive Training）。SAT同样对说话人和噪声的归一化。<div class="comments-icon"></div></p>
<p class="comments-section">5.对齐算法
实际的对齐操作是一样的，不同文集使用不同的声学模型。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20171104095237898.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图13.1 kalid中高斯声学模型<div class="comments-icon"></div></p>
</blockquote>
<h2 id="gmm">GMM</h2>
<ul>
<li>高效似然估计</li>
</ul>
<p class="comments-section">$$\log f_{Gauss}(\mathbf x)=\mathbf K_1(\mathbf \sum)-\frac{1}{2}(\mathbf{X}-\mathbf{ \mu})^T\mathbf \sum^{-1}(\mathbf X- \mathbf \mu)
=\mathbf K_1(\mathbf \sum)-\frac{1}{2}[\mathbf X^T\sum^{-1}\mathbf X-2\mu^T\sum^{-1}\mathbf{X}+\mu^T\mathbf{\sum}^{-1}\mu]
=\mathbf K_2(\mu, \mathbf \sum)+\mu^T\mathbf \sum^{-1}\mathbf X-\mathbf x^T \mathbf \sum^{-1}\mathbf {X} \tag{13.1}<div class="comments-icon"></div></p>
<p>$$</p>
<ul>
<li>概率计算经过两个矩阵向量的相乘获得，可以使用优化的BLAS函数实现。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20171104100757260.png" alt=""><blockquote>
<p class="comments-section">图13.2 概率矩阵的计算<div class="comments-icon"></div></p>
</blockquote>
</li>
</ul>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20171104124713313.png" alt=""></p>
<blockquote>
<p class="comments-section">图13.3 kaldi中解码部分接口<div class="comments-icon"></div></p>
</blockquote>
<pre><code class="lang-cpp"><span class="hljs-keyword">class</span> DecodableInterface{
<span class="hljs-keyword">public</span>:
<span class="hljs-comment">//Returns the log likelihood(negated in the decoder).</span>
<span class="hljs-function"><span class="hljs-keyword">virtual</span> BaseFloat <span class="hljs-title">LogLikelihood</span><span class="hljs-params">(int32 frame, int32 index)</span></span>=<span class="hljs-number">0</span>;

<span class="hljs-comment">//Frames are one-based.</span>
<span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">bool</span> <span class="hljs-title">IsLastFrame</span><span class="hljs-params">(int32 frame)</span></span>=<span class="hljs-number">0</span>;

<span class="hljs-comment">//Indices are one-based(compatibility with OpenFst).</span>
<span class="hljs-function"><span class="hljs-keyword">virtual</span> int32 <span class="hljs-title">NumIndices</span><span class="hljs-params">()</span></span>=<span class="hljs-number">0</span>;

<span class="hljs-keyword">virtual</span> ~DecodableInterface(){}
}
</code></pre>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20171104125107589.png" alt=""></p>
<blockquote>
<p class="comments-section">图13.4 声学模型训练
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20171104125202004.png" alt="">
图13.5 fMLLR声学模型训练
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20171104125322558.png" alt="">
图13.6 对角高斯声学模型<div class="comments-icon"></div></p>
</blockquote>
<h2 id="单音素训练">单音素训练</h2>
<h3 id="初始化单音素模型">初始化单音素模型</h3>
<p class="comments-section">-gmm-init-mono.cc
该程序有两个输入和两个输出。做为输入，需要描述声学模型的HMM音素结构的拓扑文件（如data/lang/topo）和高斯混合模型中每个分量的维度。
如:<div class="comments-icon"></div></p>
<pre><code>gmm-init-mono data/lang/topo \
              39 \
              exp/mono/0.mdl \
              exp/mono/tree;
</code></pre><p class="comments-section">在这个例子中tree是语境决策树。<div class="comments-icon"></div></p>
<p class="comments-section">-compile-train-graphs.cc
假设我们已经得到了决策树和模型。接下来的命令创建训练集对应的HCLG图归档文件。该程序为每一个训练语句编译FST。<div class="comments-icon"></div></p>
<pre><code>compile-train-graphs  exp/mono/tree \
                      exp/mono/0.mdl \
                      data/L.fst \
                      ark:data/train.tra \
                      ark:exp/mono/graphs.fsts;
</code></pre><p class="comments-section">train.tra是训练集的索引文件，该文件每一行的第一个字段是说话人ID。该程序的输出是graphs.fsts；其为train.tra中的每句话建立一个二进制格式的FST。该FST和HCLG对应，差异在于没有转移概率不在里面。这是因为该图将在训练中被多次用到且转移概率也将会发生变化，所以后面才会加上转移概率。但是归档的FST中包含了silence的概率（编码到L.fst）。解码图就是$$HCLG=H \circ C \circ L \circ G$$。
1.H包括了HMM定义；输出符号是上下文无关的音素，输入符号是一系列状态转变ID（是概率id和其它信息的编码）。
2.C代表的是上下文依赖关系。其输出代表音素的符号，输入是代表上下文相关的音素符号；
3.L是字典（Lexicon）；输出是单词，输入是一系列音素。
4.G表示的是Grammar；是语音模型的有限状态机。<div class="comments-icon"></div></p>
<p class="comments-section">由于训练时候不要进行符号消歧义，所以图创建要比测试时简单，训练和测试使用HCLG形式上是一样的，不同在于，训练时G仅仅包括和训练文集相关的一个线性接收器。
同样希望HCLG具有随机性，传统方法使用“push-weights”方法。<div class="comments-icon"></div></p>
<h3 id="训练单音素模型">训练单音素模型</h3>
<p class="comments-section">-align-equal-compiled.cc
给定声学模型/图rspecifier/特征rspecifier。这个程序会返回用于对齐的wspecifier。这是EM算法中的E步骤.
EM算法可以见
<a href="http://blog.csdn.net/shichaog/article/details/78415473" target="_blank">EM算法原理链接</a>
<a href="https://github.com/shichaog/em4gmm" target="_blank">EM算法源码链接</a><div class="comments-icon"></div></p>
<pre><code>align-equal-compiled 0.mdl \
                     graphs.fsts \
                     scp:train.scp \
                     ark:equal.ali;
</code></pre><p class="comments-section">如果要查看对齐结果，可以使用show-alignments.cc程序查看。
-gmm-acc-stats-ali.cc
这个程序有三个输入：1.编译好的声学模型（0.mdl）；2.训练用的音频文件特征（MFCC，train.scp）3.先前计算的隐藏状态的对齐信息。输出文件是GMM训练用的状态集（0.acc）。<div class="comments-icon"></div></p>
<pre><code>gmm-acc-stats-ali 0.mdl \
                  scp:train.scp \
                  ark:equal.ali \
                  0.acc
</code></pre><p class="comments-section">-gmm-test.cc
这是EM算法的M步骤，给定1.声学模型； 2.GMM训练状态集，这个程序将会输出新的声学模型（经过ML估计更新）。<div class="comments-icon"></div></p>
<pre><code>gmm-test --min-gaussian-occupancy=3 \
         --mix-up=250 \
         exp/mono/0.mdl \
         exp/mono/0.acc \
         exp/mono/1.mdl;
</code></pre><p class="comments-section">参数<code>--mix-up</code>指定了新的混合高斯模型成分的数量。
当训练数据量小时，<code>--min-gaussian-occupancy</code>需要指定以处理少见的音素。<div class="comments-icon"></div></p>
<h3 id="音素和数据对齐">音素和数据对齐</h3>
<p class="comments-section">-gmm-align-compiled.cc
给定1.声学模型；2.图的rspecifier；3.特征的rspecifier。该程序返回对齐的wspecifier。这是EM算法中的E步骤。对齐是指HMM状态和提取的语音特征向量关系。每一个HMM状态有一个高斯分布输出，对齐后的特征向量会被用于高斯参数更新（$\mu$和$\sum$）.<div class="comments-icon"></div></p>
<pre><code>gmm-align-compiled 1.mdl \
                    ark:graphs.fsts \
                    scp:train.scp \
                    ark:1.ali;
</code></pre><h2 id="三音素训练">三音素训练</h2>
<h3 id="上下文相关音素决策数构建">上下文相关音素决策数构建</h3>
<p class="comments-section">CART（Clustering and Regression Tree）.
-acc-tree-stats.cc
该程序有三个输入参数1.声学模型，2.声学特征的rspecifier，3.前一次对齐的rspecifier。返回值是树累积量。
该程序不仅能处理单音素对齐，也能处理基于上下文关系的对齐（如三音素），构建树需要的统计量被以BuildTreeStatsType类型写入磁盘，函数AccumulateTreeStats()接收P和N。命令行程序会将P和N缺省设置成3和1，但是可以使用<code>--context-with</code>和<code>--central-position</code>选项进行更改。acc-tree-stats.cc接收一系列上下文无关音素（如，silence），这可以减少统计量的数目。<div class="comments-icon"></div></p>
<pre><code>acc-tree-stats final.mdl \
               scp:train.scp \
               ark:JOB.ali   \
               JOB.treeacc;
</code></pre><p class="comments-section">-sum-tree-statics.cc
该程序为音素树构建求和统计量，该程序输入多个*.treeacc文件，输出单个累积后统计量（如treeacc）。<div class="comments-icon"></div></p>
<pre><code>sum-tree-stats  treeacc \
                phonesets.int \
                questions.int;
</code></pre><p class="comments-section">-compile-questions.cc
该程序的输入是1.HMM拓扑（如，topo），2.音素列表（如，questions.int），返回EventMap（如phonesets.qst）中“key”对应问题的C++对象表示的音素列表。<div class="comments-icon"></div></p>
<pre><code>compile-questions data/lang/topo \
                  exp/triphones/questions.int \
                  exp/triphones/questions.qst;
</code></pre><p class="comments-section">-build-tree.cc
当统计量累积后可以使用build-tree.cc构建树。其有三个输入参数：1.累积的树统计量（treeacc），2.问题配置（questions.qst）,3.根文件（roots.int）。
树统计量使用程序<code>acc-tree-stats.cc</code>求得,问题配置使用程序<code>compile-questions.cc</code>求得，cluster-phones.cc求得音素问题拓扑列表。
build-tree建立了一系列决策树，最大叶子节点数（如2000）是概率数量，在分割之后，对每一颗树会做后聚类。共享的叶子的作用域在单棵树内。<div class="comments-icon"></div></p>
<pre><code>build-tree treeacc \
           roots.int \
           questions.qst \
           topo    \
           tree;
</code></pre><p class="comments-section">可以使用程序<code>draw-tree.cc</code>查看决策树。<div class="comments-icon"></div></p>
<pre><code>draw-tree data/lang/phones.txt \
          exp/mono/tree | \
          dot -Tps -Gsize=8,10.5 | \
          ps2pdf - ~/tree.pdf
</code></pre><p class="comments-section">-gmm-init-model.cc
根据1.决策树（tree），2.累积树状态（treeacc），3.HMM拓扑（topo）来初始化GMM声学模型（1.mdl）。<div class="comments-icon"></div></p>
<pre><code>gmm-init-model tree \
               treeacc \
               topo   \
               1.mdl;
</code></pre><p class="comments-section">-gmm-mixup.cc
根据1.高斯声学模型（1.mdl），2.每个状态转变ID出现次数，做高斯合并操作，返回的高斯声学模型（2.mdl）的分量数会增加。<div class="comments-icon"></div></p>
<pre><code>gmm-mixup --mix-up=$numgauss \
          1.mdl  \
          1.occs   \
          2.mdl
</code></pre><p class="comments-section">-convert-ali.cc
根据1.旧的GMM模型（monophones_aligned/final.mdl），2.新的GMM模型（triphones_del/2.mdl），3.新的决策树（triphones_del/tree），4.就的对齐的rspecifier（monophones_aligned/ali.<em>.gz），该程序将返回新的对齐（triphones_del/ali.</em>.gz）。<div class="comments-icon"></div></p>
<pre><code>convert-ali monophones_aligned/final.mdl \
            triphones_del/2.mdl    \
            triphones_del/tree     \
            monophones_aligned/ali.*.gz  \
            triphones_del/ali.*.gz
</code></pre><p class="comments-section">-compile-train-graphs.cc
给定输入1.决策树（tree），2.声学模型（2.mdl），3.字典的有限状态转换器（L.fst），4.训练文本集的rspecifier（text）将输出训练图的wspecifier（fsts.*.gz）。<div class="comments-icon"></div></p>
<pre><code>compile-train-graphs tree \
                     1.mdl \
                     L.fst \
                     text  \
                     fsts.*.gz;
</code></pre><h2 id="wfst关键点">WFST关键点</h2>
<p class="comments-section">1.determinization
2.minimization
3.composition
4.equivalent
5.epsilon-free
6.functional
7.on-demand algorithm
8.weight-pushing
9.epsilon removal<div class="comments-icon"></div></p>
<h2 id="hmm关键点">HMM关键点</h2>
<p class="comments-section">1.Markov Chain
2.Hidden Markov Model
3.Forward-backward algorithm
4.Viterbi algorithm
5.E-M for mixture of Gaussians<div class="comments-icon"></div></p>
<h2 id="lfst-发音字典fst">L.fst 发音字典FST</h2>
<p class="comments-section">L将单音素序列映射到单词。
文件L.fst是根据音素符号序列获得单词序列的有限状态转换器。<div class="comments-icon"></div></p>
<h2 id="聚类机制">聚类机制</h2>
<p class="comments-section">类GaussClusterable（高斯统计量）继承了纯虚类Clusterable。未来会加入从Clusterable类继承来的可以聚类对象的处理。Clusterable 存在的目的是使用通用聚类算法。
Clusterable的核心思想是将统计量相加，然后评测目标函数，两个可以聚类对象的距离是指两个对象各自目标函数和他们相加之后目标函数带来的影响。
Clusterable类相加的实例有混合高斯统计量的高斯模型，离散观测量的计数值。
获得Clusterable*对象的实例如下：<div class="comments-icon"></div></p>
<pre><code>Vector&lt;BaseFloat&gt; x_stats(10), x2_stats(10);
BaseFloat count = 100.0, var_floor = 0.01;
// initialize x_stats and x2_stats e.g. as
// x_stats = 100 * mu_i, x2_stats = 100 * (mu_i*mu_i + sigma^2_i)
Clusterable *cl = new GaussClusterable(x_stats, x2_stats, var_floor, count);
</code></pre><h3 id="聚类算法">聚类算法</h3>
<p class="comments-section">聚类函数如下：<div class="comments-icon"></div></p>
<ul>
<li>ClusterBottomUp</li>
<li>ClusterBottomUpCompartmentalized</li>
<li>RefineClusters </li>
<li>ClusterKMeans</li>
<li>TreeCluster</li>
<li>ClusterTopDown
常用到的数据类型是：</li>
</ul>
<pre><code>std::vector&lt;Clusterable*&gt; to_be_clustered;
</code></pre><h3 id="k-means及其类似算法接口">K-means及其类似算法接口</h3>
<p class="comments-section">聚类代码调用实例如下：<div class="comments-icon"></div></p>
<pre><code class="lang-cpp"><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Clusterable*&gt; to_be_clustered;
<span class="hljs-comment">// initialize "to_be_clustered" somehow ...</span>
<span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Clusterable*&gt; clusters;
int32 num_clust = <span class="hljs-number">10</span>; <span class="hljs-comment">// requesting 10 clusters</span>
ClusterKMeansOptions opts; <span class="hljs-comment">// all default.</span>
<span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;int32&gt; assignments;
ClusterKMeans(to_be_clustered, num_clust, &amp;clusters, &amp;assignments, opts);
</code></pre>
<p class="comments-section">ClusterBottomUp() 和ClusterTopDown()和ClusterKMeans()的调用方法类似，如果聚类的数量较大，ClusterTopDown()比ClusterKMeans()更高效。<div class="comments-icon"></div></p>
<h2 id="hmm拓扑">HMM拓扑</h2>
<p class="comments-section">使用c++的HmmTopology来描述音素的HMM拓扑。其描述的一个实例（3-state Bakis模型）如下：<div class="comments-icon"></div></p>
<pre><code>&lt;Topology&gt;
 &lt;TopologyEntry&gt;
 &lt;ForPhones&gt; 1 2 3 4 5 6 7 8 &lt;/ForPhones&gt;
 &lt;State&gt; 0 &lt;PdfClass&gt; 0
 &lt;Transition&gt; 0 0.5
 &lt;Transition&gt; 1 0.5
 &lt;/State&gt;
 &lt;State&gt; 1 &lt;PdfClass&gt; 1
 &lt;Transition&gt; 1 0.5
 &lt;Transition&gt; 2 0.5
 &lt;/State&gt;
 &lt;State&gt; 2 &lt;PdfClass&gt; 2
 &lt;Transition&gt; 2 0.5
 &lt;Transition&gt; 3 0.5
 &lt;/State&gt;
 &lt;State&gt; 3
 &lt;/State&gt;
 &lt;/TopologyEntry&gt;
 &lt;/Topology&gt;
</code></pre><p class="comments-section">在这个实例中只有一个<code>TopologyEntry</code>，其包括了音素1~8（所以这个例子总共8个音素，这些音素共享相同的拓扑）。有三个发射状态，每个状态包括一个自循环和发射到其它状态的概率。还有最后一个非发射状态（状态3，没有<pdfclass>入口）。kaldi把状态0作为初始状态，最后一个状态作为作为终止状态（无发射状态，其概率等于1）。<code>HmmTopology</code> 对象中的概率用于初始化训练。训练的概率是上下文相关的<code>HMM</code>并且存储在<code>TransitionModel</code>对象。<code>TransitionModel</code>以<code>c++</code>类成员的方式存储<code>HmmTopology</code>对象。<code>HmmTopology</code>的转换概率通常除了初始化<code>TransitionModel</code>对象其它地方并不会被用到。</pdfclass><div class="comments-icon"></div></p>
<h3 id="pdf-class">Pdf-class</h3>
<p class="comments-section"><code>Pdf-class</code>是和对象<code>HmmTopology</code>有关的一个对象。<code>HmmTopology</code>为每一个音素指定了一个HMM模型，每一个有编号的状态有两个变量<code>forward_pdf_class</code>和<code>self_loop_pdf_class</code>。<code>self_loop_pdf_class</code>是转换到状态自身的概率，缺省值是和<code>forward_pdf_class</code>一样的。但是两者的概率也可以不一样。
音素的HMM状态通常从0开始，连续的（1,2，。。。），这是为了图构建的方便。<div class="comments-icon"></div></p>
<h3 id="状态转换模型（transitionmodel对象）">状态转换模型（TransitionModel对象）</h3>
<p class="comments-section"><code>TransitionModel</code>对象存储了音素的HMM拓扑对应的转变概率和信息。构建图的代码根据<code>TransitionModel</code>和<code>ContextDependencyInterface</code>对象来获得拓扑结构和状态转换概率。<div class="comments-icon"></div></p>
<h4 id="状态转化概率建模">状态转化概率建模</h4>
<p class="comments-section">状态转换的概率是和上下文相关的HMM状态相关的，其依赖如下5项内容（5元组）：<div class="comments-icon"></div></p>
<ul>
<li>音素<ul>
<li>源HMM状态（<code>HmmTopology</code>对象解析，通常是0,1,2...）</li>
<li>前向概率（<code>forward-pdf-id</code>，）</li>
<li>自循环概率（<code>self-loop-pdf-id</code>）</li>
<li><code>HmmTopology</code>对象的状态索引
后四项可以看成是目标HMM状态编码成<code>HmmTopology</code>对象。<h4 id="transition-ids"><code>transition-ids</code></h4>
<code>TrainsitionModel</code>对象在初始化时建立了音素和整数之间的映射关系，此外还有转换标识符 （transition identifiers）<code>transition-ids</code>，转换索引（transition indexes），转换状态（transition states）这些量。引入这些量为了完全使用基于FST的训练方法。<h4 id="transitionmodel使用的整型标识符"><code>TransitionModel</code>使用的整型标识符</h4>
</li>
<li>音素（从1开始）：可以从OpenFst符号表转换成音频的名字，并不要求音素是连续标号的。</li>
<li>hmm状态（从0开始）：用于索引<code>HmmTopology::TopologyEntry</code>对象。</li>
<li>概率或者pdf-ids（从0开始）：源于决策树聚类后结果，通常一个ASR系统有数以千计的pdf-id.</li>
<li><code>transition-state</code>（从1开始）：<code>TransitionModel</code>定义。每一个可能的三元组（音素，hmm状态，概率）映射到一个独一无二的转换状态。</li>
<li><code>transition-index</code>（从0开始）：是对<code>HmmTopology::HmmState</code>的索引。</li>
<li><code>transition-id</code>（从1开始）：是状态转换模型的转换概率。二元组（<code>transition-state</code>，<code>transition-index</code>）和<code>transition-id</code>可以互相映射。</li>
</ul>
</li>
</ul>
<h3 id="转换模型（transition-model）训练">转换模型（transition model）训练</h3>
<p class="comments-section">用于训练和测试的FST将<code>transition-id</code>做为输入label。在训练过程中使用维特比解码获得输入transition-id序列（每一个都是一个特征向量），函数<code>Transition::Update()</code>对每个<code>transition-state</code>做最大似然估计。<div class="comments-icon"></div></p>
<h3 id="对齐">对齐</h3>
<p class="comments-section">和的语句长度一样的包含一系列<code>transition-ids</code>的vector<int32>向量描述了对齐关系。<code>transition-ids</code>序列从解码器得到。对齐用于维特比训练和测试时自适应。由于<code>transition-ids</code>编码了音素信息，可以通过工具<code>SplitToPhones()</code>和<code>ali-to-phones.cc</code>根据对齐取出音素序列。
通常kaldi中需要处理由句子索引的对齐集合，这通常使用表的方式来实现。
函数<code>ConvertAlignment()</code>（命令行是<code>convert-ali</code>）将对齐从一个状态转变模型转换到另一个模型。</int32><div class="comments-icon"></div></p>
<h3 id="状态层次后验概率">状态层次后验概率</h3>
<p class="comments-section">状态级后验概率是“对齐”概念的扩展，区别在于“对齐”概念上每帧对应一个状态转变ID，而状态级后验概率每帧的状态转变ID的数量没有限制，且每个状态ID都有一个权重对应。通常按如下结构存储：<div class="comments-icon"></div></p>
<pre><code>typedef std::vector&lt;std::vector&lt;std::pair&lt;int32, BaseFloat&gt; &gt; &gt; Posterior;
</code></pre><p class="comments-section">如果使用<code>Posterior</code>创建了一个名为post的对象，则<code>post.size()</code>将等于句子帧数，<code>post[i]</code>存储的是<code>(transition-id, posterior)</code>信息。
当前程序中，只有两个方法创建<code>posteriors</code>。<div class="comments-icon"></div></p>
<ul>
<li>使用<code>ali-to-post</code>程序将对齐转换成后延概率。</li>
<li>使用<code>weight-silence-post</code>修改后验概率。
当加入lattice是，也有工具从Lattice生成后验概率。</li>
</ul>
<h3 id="高斯层次后验概率">高斯层次后验概率</h3>
<p class="comments-section">表示高斯层次的后验概率类型如下：<div class="comments-icon"></div></p>
<pre><code>typedef std::vector&lt;std::vector&lt;std::pair&lt;int32, Vector&lt;BaseFloat&gt; &gt; &gt; &gt; GauPost;
</code></pre><p class="comments-section">其状态是使用向量浮点数来表示的。向量的size和高斯量的数目是一样的。<code>post-to-gpost</code>将<code>Posterior</code>结构转换成<code>GauPost</code>结构。使用模型和特征计算高斯层次的后验概率。<div class="comments-icon"></div></p>
<h2 id="hmms转成fsts">HMMs转成FSTs</h2>
<h3 id="gethtransducer">GetHTransducer()</h3>
<pre><code>fst::VectorFst&lt;fst::StdArc&gt;*
GetHTransducer (const std::vector&lt;std::vector&lt;int32&gt; &gt; &amp;ilabel_info,
                const ContextDependencyInterface &amp;ctx_dep,
                const TransitionModel &amp;trans_model,
                const HTransducerConfig &amp;config,
                std::vector&lt;int32&gt; *disambig_syms_left);
</code></pre><p class="comments-section">该函数返回输入是<code>transition-ids</code>，输出是上下文相关音素的FST。FST具有初始和终止状态，转换出FST的状态变换将输出音素符号。通常转出FST状态会转入一个表示3状态HMM的结构体中，然后跳到起始状态。<div class="comments-icon"></div></p>
<h3 id="htransducerconfig配置类">HTransducerConfig配置类</h3>
<p class="comments-section"><code>HTransducerConfig</code>控制着<code>GetHTransducer</code>的行为。<div class="comments-icon"></div></p>
<ul>
<li>变量<code>trans_prob_scale</code>是状态转变缩放因子。当转变概率添加到图里时，会乘以缩放因子。命令行工具是<code>transition-scale</code>。<h3 id="gethmmasfst">GetHmmAsFst()</h3>
函数<code>GetHmmAsFst()</code>输入是一段音素，返回的是状态机最终状态时得到的<code>transition-ids</code>序列。</li>
</ul>
<h3 id="addselfloops">AddSelfLoops()</h3>
<p class="comments-section">是向图中添加自循环。添加自循环的意义是可以进行状态重新调整，而不加的意义在于决策过程可以更高效。<div class="comments-icon"></div></p>
<h3 id="fst添加状态转变概率">FST添加状态转变概率</h3>
<p class="comments-section">函数<code>AddTransitionProbs()</code>向FST添加概率。这样可以在无概率时就可以创建图了。<div class="comments-icon"></div></p>
<h2 id="yesno实例">yes/no实例</h2>
<p class="comments-section">这个例子可以和第十五章基于tensorflow的yes/no实例进行对比.
<a href="https://github.com/shichaog/tensorflow-android-speech-kws" target="_blank">tensorflow yes/no实例</a><div class="comments-icon"></div></p>
<h3 id="数据描述">数据描述</h3>
<p class="comments-section">如果kaldi/egs/yesno/s5目录下没有waves_yesno.tar.gz文件,则要下载该文件.
<a href="http://www.openslr.org/resources/1/waves_yesno.tar.gz" target="_blank">http://www.openslr.org/resources/1/waves_yesno.tar.gz</a>
解压后,waves_yesno文件夹下的文件如下.<div class="comments-icon"></div></p>
<pre><code>0_0_0_0_1_1_1_1.wav  0_0_1_1_0_1_1_0.wav  
...
1_1_1_0_1_0_1_1.wav
</code></pre><p class="comments-section">总共60个wav文件,采样率都是8k,wav文件里每一个单词要么"ken"要么"lo"("yes"和"no")的发音,所以每个文件有8个发音,文件命名中的1代表yes发音,0代表no的发音.<div class="comments-icon"></div></p>
<h3 id="数据预处理">数据预处理</h3>
<h4 id="wav文件预处理">wav文件预处理</h4>
<pre><code>local/prepare_data.sh waves_yesno
local/prepare_dict.sh
utils/prepare_lang.sh --position-dependent-phones false data/local/dict "&lt;SIL&gt;" data/local/lang data/lang
local/prepare_lm.sh
</code></pre><ul>
<li>生成wavelist文件<pre><code>ls -1 $waves_dir &gt; data/local/waves_all.list
</code></pre>上述shell命令就是把waves_yeno目录下的文件名全部保存到waves_all.list中.</li>
</ul>
<pre><code>local/create_yesno_waves_test_train.pl waves_all.list waves.test waves.train
</code></pre><ul>
<li>生成waves.test和waves.train
将waves_all.list中的60个wav文件,分成两拨，各30个，分别放在waves.test和waves.train中．
如waves.train文件内容如下：<pre><code>0_0_0_0_1_1_1_1.wav
</code></pre></li>
<li><p class="comments-section">生成test_yesno_wav.scp和train_yesno_wav.scp
根据waves.test 和waves.train又会生成test_yesno_wav.scp和train_yesno_wav.scp两个文件．
这两个文件内容排列格式如下<div class="comments-icon"></div></p>
<pre><code>&lt;file_id&gt;　&lt;wave filename with path OR command to get wave file&gt;
如：
0_0_0_0_1_1_1_1 waves_yesno/0_0_0_0_1_1_1_1.wav
</code></pre><p class="comments-section">其中由于训练的scp文件如下：<div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">生成train_yesno.txt和test_yesno.txt
这两个文件存放的是发音id和对应的文本．<div class="comments-icon"></div></p>
<pre><code>&lt;utt_id&gt;&lt;transcript&gt;
如：
0_0_1_1_1_1_0_0 NO NO YES YES YES YES NO NO
</code></pre></li>
<li>生成utt2spk和spk2utt
这个两个文件分别是发音和人对应关系，以及人和其发音id的对应关系．由于只有一个人的发音，所以这里都用global来表示发音．
```
utt2spk
<utt_id><speaker_id>
0_0_1_0_1_0_1_1 global</speaker_id></utt_id></li>
</ul>
<p class="comments-section">utt2spk<div class="comments-icon"></div></p>
<p><speaker_id> <all_hier_utterences></all_hier_utterences></speaker_id></p>
<pre><code>此外还可能会有如下文件（这个例子没有用到）：

- segments
包括每个录音的发音分段/对齐信息
只有在一个文件包括多个发音时需要
- reco2file_and_channel
双声道录音情况使用到
- spk2gender
将说话人和其性别建立映射关系，用于声道长度归一化．
以上生成的文件经过辅助操作均在：
</code></pre><p class="comments-section">data/train_yesno/
data/test_yesno/<div class="comments-icon"></div></p>
<pre><code>目录结构如下:
</code></pre><p class="comments-section">data
├───train_yesno
│   ├───text
│   ├───utt2spk
│   ├───spk2utt
│   └───wav.scp
└───test_yesno
    ├───text
    ├───utt2spk
    ├───spk2utt
    └───wav.scp<div class="comments-icon"></div></p>
<pre><code>####字典准备
构建语言学知识－词汇和发音词典．需要用到steps和utils目录下的工具。这可以通过修改该目录下的path.sh文件进行更新。
首先创建词典目录
</code></pre><p class="comments-section">mkdir -p data/local/dict<div class="comments-icon"></div></p>
<pre><code>这个简单的例子只有两个单词：YES和NO，为简单起见，这里假设这两个单词都只有一个发音：Y和N。这个例子直接拷贝了相关的文件，非语言学的发音，被定义为SIL。
</code></pre><p class="comments-section">data/local/dict/lexicon.txt<div class="comments-icon"></div></p>
<p class="comments-section"><sil> SIL
YES Y
NO N</sil><div class="comments-icon"></div></p>
<pre><code>- lexicon.txt，完整的词位-发音对
- lexicon_words.txt，单词-发音对
- silence_phones.txt， 非语言学发音
- nonsilence_phones.txt，语言学发音
- optional_silence.txt ，备选非语言发音
最后还要把字典转换成kaldi可以接受的数据结构-FST（finit state transducer）。这一转换使用如下命令：
</code></pre><p class="comments-section">utils/prepare_lang.sh --position-dependent-phones false data/local/dict "<sil>" data/local/lang data/lang</sil><div class="comments-icon"></div></p>
<pre><code>由于语料有限，所以将位置相关的发音disable。这个命令的各行意义如下：
</code></pre><p class="comments-section">utils/prepare_lang.sh --position-dependent-phones false <raw_dict_path> <oov> <temp_dir> <output_dir></output_dir></temp_dir></oov></raw_dict_path><div class="comments-icon"></div></p>
<pre><code>OOV存放的是词汇表以外的词，这里就是静音词（非语言学发声意义的词）
发音字典是二进制的OpenFst 格式，可以使用如下命令查看：
</code></pre><p class="comments-section">gsc@X250:~/kaldi/egs/yesno/s5$ ~/kaldi/tools/openfst-1.6.2/bin/fstprint --isymbols=data/lang/phones.txt --osymbols=data/lang/words.txt data/lang/L.fst 
0    1    <eps>    <eps>    0.693147182
0    1    SIL    <eps>    0.693147182
1    1    SIL    <sil>
1    1    N    NO    0.693147182
1    2    N    NO    0.693147182
1    1    Y    YES    0.693147182
1    2    Y    YES    0.693147182
1
2    1    SIL    <eps></eps></sil></eps></eps></eps><div class="comments-icon"></div></p>
<pre><code>
####语言学模型
这里使用的是一元文法语言模型，同样要转换成FST以便kaldi接受。该语言模型原始文件是data/local/lm_tg.arpa，生成好的FST格式的。是字符串和整型值之间的映射关系，kaldi里使用整型值。
</code></pre><p class="comments-section">gsc@X250:~/kaldi/egs/yesno/s5$ head -5 data/lang/phones.txt <div class="comments-icon"></div></p>
<p class="comments-section"><eps> 0
SIL 1
Y 2
N 3
#0 4
gsc@X250:~/kaldi/egs/yesno/s5$ head -5 data/lang/words.txt </eps><div class="comments-icon"></div></p>
<p><eps> 0</eps></p>
<p class="comments-section"><sil> 1
NO 2
YES 3
#0 4</sil><div class="comments-icon"></div></p>
<pre><code>可以使用如下命令查看生成音素的树形结构：
###phone 树
</code></pre><p class="comments-section"> ~/kaldi/src/bin/draw-tree data/lang/phones.txt exp/mono0a/tree | dot -Tps -Gsize=8,10.5 | ps2pdf - ./tree.pdf<div class="comments-icon"></div></p>
<pre><code>![](/assets/20170625165516481.png)
&gt;图13.1 音素树

LM（language model）在data/lang_test_tg/。
</code></pre><p class="comments-section">local/prepare_lm.sh<div class="comments-icon"></div></p>
<pre><code>
####查看拓扑结构
![](/assets/20170625165814453.png)
在&lt;ForPhone&gt; &lt;/ForPhones&gt;之间的数字，1表示silcense，2,3分别表示Y和N，这从拓扑图里也可以看出来。
指定了三个状态从左到右的HMM以及默认的转变概率。为silence赋予5个状态。
![](/assets/20170825112908582.png)
&gt;图13.2 模型

0.mdl的内容如上：
####转移模型
</code></pre><p><transitionmodel> </transitionmodel></p>
<p><topology> </topology></p>
<p><topologyentry> </topologyentry></p>
<p class="comments-section"><forphones> 
2 3 
</forphones> <div class="comments-icon"></div></p>
<p class="comments-section"><state> 0 <pdfclass> 0 <transition> 0 0.75 <transition> 1 0.25 </transition></transition></pdfclass></state> <div class="comments-icon"></div></p>
<p class="comments-section"><state> 1 <pdfclass> 1 <transition> 1 0.75 <transition> 2 0.25 </transition></transition></pdfclass></state> <div class="comments-icon"></div></p>
<p class="comments-section"><state> 2 <pdfclass> 2 <transition> 2 0.75 <transition> 3 0.25 </transition></transition></pdfclass></state> <div class="comments-icon"></div></p>
<p class="comments-section"><state> 3 </state> 
&lt;/TopologyEntry&gt; <div class="comments-icon"></div></p>
<p><topologyentry> </topologyentry></p>
<p class="comments-section"><forphones> 
1 
</forphones> <div class="comments-icon"></div></p>
<p class="comments-section"><state> 0 <pdfclass> 0 <transition> 0 0.25 <transition> 1 0.25 <transition> 2 0.25 <transition> 3 0.25 </transition></transition></transition></transition></pdfclass></state> <div class="comments-icon"></div></p>
<p class="comments-section"><state> 1 <pdfclass> 1 <transition> 1 0.25 <transition> 2 0.25 <transition> 3 0.25 <transition> 4 0.25 </transition></transition></transition></transition></pdfclass></state> <div class="comments-icon"></div></p>
<p class="comments-section"><state> 2 <pdfclass> 2 <transition> 1 0.25 <transition> 2 0.25 <transition> 3 0.25 <transition> 4 0.25 </transition></transition></transition></transition></pdfclass></state> <div class="comments-icon"></div></p>
<p class="comments-section"><state> 3 <pdfclass> 3 <transition> 1 0.25 <transition> 2 0.25 <transition> 3 0.25 <transition> 4 0.25 </transition></transition></transition></transition></pdfclass></state> <div class="comments-icon"></div></p>
<p class="comments-section"><state> 4 <pdfclass> 4 <transition> 4 0.75 <transition> 5 0.25 </transition></transition></pdfclass></state> <div class="comments-icon"></div></p>
<p class="comments-section"><state> 5 </state> 
&lt;/TopologyEntry&gt; 
&lt;/Topology&gt;<div class="comments-icon"></div></p>
<pre><code>
####音素 hmm状态
</code></pre><p class="comments-section"><triples> 11 
1 0 0 
1 1 1 
1 2 2 
1 3 3 
1 4 4 
2 0 5 
2 1 6 
2 2 7 
3 0 8 
3 1 9 
3 2 10 
</triples><div class="comments-icon"></div></p>
<pre><code>####高斯模型
如下的20+1个log概率对应于11个phone(0-10)。
</code></pre><p class="comments-section"><logprobs> 
 [ 0 -1.386294 ... ]
</logprobs><div class="comments-icon"></div></p>
<pre><code>接下来是高斯模型的维度39维（没有能量），对角GMM参数总共11个。
</code></pre><p class="comments-section"><dimension> 39 <numpdfs> 11 <diaggmm> </diaggmm></numpdfs></dimension><div class="comments-icon"></div></p>
<pre><code>在接下来就是对角高斯参数的均值方差权重等参数：
</code></pre><p class="comments-section"><gconsts>  [ -79.98567 ]</gconsts><div class="comments-icon"></div></p>
<p class="comments-section"><weights>  [ 1 ]</weights><div class="comments-icon"></div></p>
<p class="comments-section"><means_invvars>  [
  0.001624335 ...]</means_invvars><div class="comments-icon"></div></p>
<p class="comments-section"><inv_vars>  [
  0.006809053 ... ]</inv_vars><div class="comments-icon"></div></p>
<pre><code>####编译训练图
![](/assets/20170625170744561.png)
为每一个训练的发音编译FST，为训练的发句编码HMM结构。

####kaldi 中表的概念
&gt;表是字符索引-对象的集合，有两种对象存储于磁盘
&gt;“scp”（script）机制：.scp文件从key（字串）映射到文件名或者pipe
&gt;“ark”（archive）机制：数据存储在一个文件中。
Kaldi 中表
一个表存在两种形式："archive"和"script file"，他们的区别是archive实际上存储了数据，而script文件内容指向实际数据存储的索引。
从表中读取索引数据的程序被称为"rspecifier"，向表中写入字串的程序被称为"wspecifier"。
| rspecifier        | meaning           |
| ------------- |:-------------:| 
| ark:-      | 从标准输入读取到的数据做为archive | 
| scp:foo.scp      | foo.scp文件指向了去哪里找数据      |
冒号后的内容是wxfilename 或者rxfilename，它们是pipe或者标准输入输出都可以。
表只包括一种类型的对象（如，浮点矩阵）
respecifier和wspecifier可以包括一些选项：
-   在respecifier中，ark,s,cs:- ，表示当从标准输入读操作时，我们期望key是排序过的(s)，并且可以确定它们将会按排序过的顺序读取，(cs)意思是我们知道程序将按照排序过的方式对其进行访问（如何条件不成立，程序会crash），这是得Kaldi不要太多内存下可以模拟随机访问。
* 对于数据源不是很大，并且结果和排序无关的情形时，rspecifier可以忽略s,cs。
* scp,p:foo.scp ,p表示如果scp索引的文件存在不存在的情况，程序不crash（prevent of crash）。
* 对于写，选项t表示文本模式。

script文件格式是，`&lt;key&gt; &lt;rspecifier|wspecifier&gt;`如`utt1 /foo/bar/utt1.mat`

从命令行传递的参数指明如何读写表（scp,ark）。对于指示如何读表的字串称为“rspecifier”,而对写是"wspecifier"。
写表的实例如下：
| wspecifier        | 意义           |
| ------------- |:-------------:|
| ark:foo.ark      | 写入归档文件foo.ark |
| scp:foo.scp      | 使用映射关系写入foo.scp     | 
| ark:- | 将归档信息写入stdout      | 
| ark,t:\|gzip -c &gt; foo.gz      | 将文本格式的归档写入foo.gz |
| ark，t:-      | 将文本格式的归档写入 stdout    | 
| srk,scp:foo.ark, foo.scp | 写归档和scp文件      | 
读表：
| rspecifier        | 意义           |
| ------------- |:-------------:|
| ark:foo.ark      | 读取归档文件foo.ark |
| scp:foo.scp      | 使用映射关系读取foo.scp     | 
| scp,p:foo.scp      | 使用映射关系读取foo.scp,p：如果文件不存在，不报错     | 
| ark:- | 从标准输入读取归档      | 
| ark：gunzip -c foo.gz\|      | 从foo.gz读取归档信息 |
| ark，s,cs:-      | 从标准输入读取归档后排序    | 


###特征提取和训练
####特征提取，这里是做mfcc。
</code></pre><p class="comments-section">steps/make_mfcc.sh --nj <n> <data_dir> <log_dir> <mfcc_dir> 
--nj <n>是处理器单元数</n></mfcc_dir></log_dir></data_dir></n><div class="comments-icon"></div></p>
<p class="comments-section"><data_dir>训练语料所在目录</data_dir><div class="comments-icon"></div></p>
<p class="comments-section"><log_dir>这个目录下记录了make_mfcc的执行log</log_dir><div class="comments-icon"></div></p>
<p class="comments-section"><mfcc_dir>是mfcc特征输出目录</mfcc_dir><div class="comments-icon"></div></p>
<pre><code>

</code></pre><p class="comments-section">for x in train_yesno test_yesno; do
 steps/make_mfcc.sh --nj 1 data/$x exp/make_mfcc/$x mfcc
 steps/compute_cmvn_stats.sh data/$x exp/make_mfcc/$x mfcc
 utils/fix_data_dir.sh data/$x
done<div class="comments-icon"></div></p>
<pre><code>该脚本主要执行的命令是：
</code></pre><p class="comments-section">gsc@X250:~/kaldi/egs/yesno/s5$ head -3 exp/make_mfcc/train_yesno/make_mfcc_train_yesno.1.log <div class="comments-icon"></div></p>
<h1 id="compute-mfcc-feats---verbose2---configconfmfccconf-scppexpmakemfcctrainyesnowavtrainyesno1scp-ark---copy-feats---compresstrue-ark--arkscphomegsckaldiegsyesnos5mfccrawmfcctrainyesno1arkhomegsckaldiegsyesnos5mfccrawmfcctrainyesno1scp">compute-mfcc-feats --verbose=2 --config=conf/mfcc.conf scp,p:exp/make_mfcc/train_yesno/wav_train_yesno.1.scp ark:- | copy-feats --compress=true ark:- ark,scp:/home/gsc/kaldi/egs/yesno/s5/mfcc/raw_mfcc_train_yesno.1.ark,/home/gsc/kaldi/egs/yesno/s5/mfcc/raw_mfcc_train_yesno.1.scp</h1>
<p class="comments-section">copy-feats --compress=true ark:- ark,scp:/home/gsc/kaldi/egs/yesno/s5/mfcc/raw_mfcc_train_yesno.1.ark,/home/gsc/kaldi/egs/yesno/s5/mfcc/raw_mfcc_train_yesno.1.scp
compute-mfcc-feats --verbose=2 --config=conf/mfcc.conf scp,p:exp/make_mfcc/train_yesno/wav_train_yesno.1.scp ark:-<div class="comments-icon"></div></p>
<pre><code>archive文件存放的是每个发音对应的特征矩阵（帧数X13大小）。
第一个参数$$scp:...$$指示在[dir]/wav1.scp里罗列的文件。
![](/assets/20170824163327615.png)
通常在做NN训练时，提取的是40维度，包括能量和上面的一阶差分和二阶差分。
</code></pre><p class="comments-section">~/kaldi/src/featbin/copy-feats ark:raw_mfcc_train_yesno.1.ark ark:- |~/kaldi/src/featbin/add-deltas ark:- ark,t:- | head<div class="comments-icon"></div></p>
<pre><code>
然后归一化导谱特征系数
</code></pre><p class="comments-section">steps/compute_cmvn_stats.sh data/$x exp/make_mfcc/$x mfcc<div class="comments-icon"></div></p>
<pre><code>生成的文件最终在mfcc目录下：
</code></pre><p class="comments-section">cmvn_test_yesno.ark<br>cmvn_train_yesno.ark<br>raw_mfcc_test_yesno.1.ark<br>raw_mfcc_train_yesno.1.ark
cmvn_test_yesno.scp<br>cmvn_train_yesno.scp<br>raw_mfcc_test_yesno.1.scp<br>raw_mfcc_train_yesno.1.scp<div class="comments-icon"></div></p>
<pre><code>详细各个命令意义，参考kaldi官网文档http://kaldi-asr.org/doc/tools.html
###单音节训练
</code></pre><p class="comments-section">steps/train_mono.sh --nj <n> --cmd <main_cmd> <data_dir> <lang_dir> <output_dir>
--cmd <main_cmd>，如果使用本机资源，使用utils/run.pl。</main_cmd></output_dir></lang_dir></data_dir></main_cmd></n><div class="comments-icon"></div></p>
<p class="comments-section">steps/train_mono.sh --nj 1 --cmd "$train_cmd" \
  --totgauss 400 \
  data/train_yesno data/lang exp/mono0a<div class="comments-icon"></div></p>
<pre><code>这将生成语言模型的FST，
使用如下命令可以查看输出：
</code></pre><p class="comments-section">fstcopy 'ark:gunzip -c exp/mono0a/fsts.1.gz|' ark,t:- | head -n 20<div class="comments-icon"></div></p>
<pre><code>![](/assets/20170615113310845.png)

其每一列是（Q-from, Q-to, S-in, S-out, Cost）
###解码和测试
####图解码
首先测试文件也是按此生成。
然后构建全连接的FST。
</code></pre><p class="comments-section">utils/mkgraph.sh data/lang_test_tg exp/mono0a exp/mono0a/graph_tgpr<div class="comments-icon"></div></p>
<pre><code>解码
</code></pre><h3 id="decoding">Decoding</h3>
<p class="comments-section">steps/decode.sh --nj 1 --cmd "$decode_cmd" \
    exp/mono0a/graph_tgpr data/test_yesno exp/mono0a/decode_test_yesno<div class="comments-icon"></div></p>
<pre><code>这将会生成`lat.1.gz`，该文件包括发音格。`exp/mono/decode_test_yesno/wer_X`并且也计算了词错误率。`exp/mono/decode_test_yesno/scoring/X.tra`是文本。X是语言模型权重LMWT。当然也可以在调用`score.sh`添加参数`--min_lmwt 和 --max_lmwt`进行修改。

###结果查看
</code></pre><p class="comments-section">for x in exp/<em>/decode</em>; do [ -d $x ] &amp;&amp; grep WER $x/wer_* | utils/best_wer.sh; done
<code>``
如果对单词级别的对齐信息感兴趣，可以参考</code>steps/get_ctm.sh`<div class="comments-icon"></div></p>

<h1 id="kaldi-中文asr实例">kaldi 中文ASR实例</h1>
<p class="comments-section">该实例基于thchs30开源数据集,且基于高斯统计模型,旨在了解训练过程和搭建在线识别系统的过程.<div class="comments-icon"></div></p>
<h2 id="在线中文识别">在线中文识别</h2>
<h3 id="原始数据下载">原始数据下载</h3>
<p class="comments-section"><a href="http://www.openslr.org/18/" target="_blank">thchs30数据集下载链接</a>
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170623194224423.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图14.1 数据集下载页面<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">总共三个tgz文件：
data_thchs30.tgz [6.4G] ( speech data and transcripts )
test-noise.tgz [1.9G] ( standard 0db noisy test data )
resource.tgz [24M] ( supplementary resources, incl. lexicon for training data, noise samples )
下载后随便解压到哪个目录文件夹下。
我解压的目录路径是：<div class="comments-icon"></div></p>
<pre><code>/media/gsc/kaldi_data/thchs30-openslr
</code></pre><h3 id="训练生成模型">训练生成模型</h3>
<p class="comments-section">该example在我的电脑上的目录是：<div class="comments-icon"></div></p>
<pre><code>/home/gsc/kaldi/egs/thchs30/s5
</code></pre><p class="comments-section">PC编译的cmd.sh按如下方式更改以使用pc进行编译：
至少4核4G内存。<div class="comments-icon"></div></p>
<pre><code>export train_cmd=run.pl
export decode_cmd="run.pl --mem 4G"
export mkgraph_cmd="run.pl --mem 8G"
export cuda_cmd="run.pl --gpu 1"
#export train_cmd=queue.pl
#export decode_cmd="queue.pl --mem 4G"
#export mkgraph_cmd="queue.pl --mem 8G"
#export cuda_cmd="queue.pl --gpu 1"
</code></pre><p class="comments-section">修改run.sh文件：
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170623194750055.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图14.2 run.sh修改<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">然后执行run.sh，由于编译时间较长，可以参看run.sh文件跟踪过程；该文件进行了不同声学模型的训练。最简单的就是单音素模型了。
可以在任何时间停止，但是至少mono训练结束后停止，因为下面在线识别依赖至少一个模型。<div class="comments-icon"></div></p>
<h3 id="训练过程">训练过程</h3>
<p class="comments-section">run.sh的剖析和上一章(13章)的yes/no类似,可参考上一章.<div class="comments-icon"></div></p>
<h3 id="安装portaudio">安装portaudio</h3>
<pre><code>gsc@X250:~/kaldi/tools$ ./install_portaudio.sh 
./src/下 , make ext
</code></pre><h3 id="在线识别">在线识别</h3>
<ul>
<li><p class="comments-section">创建相关文件
从voxforge把online_demo拷贝到thchs30下，和s5同级，online_demo建online-data和work两个文件夹。online-data下建audio和models，audio放要识别的wav，models建tri1，讲s5下/exp/下的tri1下的final.mdl和35.mdl拷贝过去，把s5下的exp下的tri1下的graph_word里面的words.txt和HCLG.fst也拷过去。<div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">修改脚本
修改online_demo 下的run.sh<div class="comments-icon"></div></p>
<ul>
<li><p>注释
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170623195434419.png" alt=""></p>
<blockquote>
<p class="comments-section">图14.3 run.sh脚本修改<div class="comments-icon"></div></p>
</blockquote>
</li>
<li><p class="comments-section">修改模型类型<div class="comments-icon"></div></p>
<pre><code>ac_model_type=tri2b_mmi 改成ac_model_type=tri1
</code></pre></li>
<li><p class="comments-section">更改命令行
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170623195715652.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图14.4 注释掉tri2b模式时的命令调用<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">-运行run.sh –test_mode live在线识别
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170623195903361.png" alt="">    <div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图14.5 在线识别运行过程<div class="comments-icon"></div></p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p class="comments-section">单音素识别结果如上，基本不准，但是对“为什么”识别很好.          <div class="comments-icon"></div></p>
<h2 id="其它统计模型">其它统计模型</h2>
<p class="comments-section">运行tri2(tri3,tri4同理)：把s5下的exp下的tri2b下的12.mat考到models的tri2b下，把final.mat考过来，再拷贝其他相应的文件，修改，
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170623200026065.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图14.6 更改模型<div class="comments-icon"></div></p>
</blockquote>
<pre><code>online-gmm-decode-faster --rt-min=0.5 --rt-max=0.7 --max-active=4000 \
        --beam=12.0 --acoustic-scale=0.0769 --left-context=3 --right-context=3 $ac_model/final.mdl $ac_model/HCLG.fst \
        $ac_model/words.txt '1:2:3:4:5' $trans_matrix;;
</code></pre><p class="comments-section">不截屏结果了，显示比前面一个好，且如果听过wav，就会发现wav文件里高频词汇，识别的结果相对准一些。<div class="comments-icon"></div></p>
<h2 id="dnn模型">dnn模型</h2>
<p class="comments-section">运行dnn：首先要将nnet1转成nnet2，如何转换，上面的文章里有，再贴一下链接：
<a href="http://kaldi-asr.org/doc/dnn1.html#dnn1_conversion_to_dnn2" target="_blank">dnn网络转换 nnet1-&gt;nnet2</a>
<a href="https://sourceforge.net/p/kaldi/discussion/1355348/thread/1ff78ec8/" target="_blank">网络转换case</a><div class="comments-icon"></div></p>

<h1 id="tensorflow入门">tensorflow入门</h1>
<h2 id="下载和安装">下载和安装</h2>
<p class="comments-section">ubuntu <div class="comments-icon"></div></p>
<pre><code>sudo apt-get -y install -y libpng12-dev libfreetype6 python-numpy python-scipy ipython python-matplotlib build-essential cmake pkg-config libtiff5-dev libjpeg-dev libjasper-dev libgtk2.0-dev libavcodec-dev libavformat-dev libswscale-dev libv4l-dev swig zip python-sklearn python-wheel
</code></pre><ol>
<li>安装pip以及virtualenv<pre><code>$sudo apt-get install python-pip python-dev python-virtualenv
</code></pre></li>
</ol>
<p class="comments-section">2.创建virtualenv<div class="comments-icon"></div></p>
<pre><code>$virtualenv --system-site-packages
</code></pre><p class="comments-section">3.使能virtual环境<div class="comments-icon"></div></p>
<pre><code> $source ~/tensorflow/bin/activate
</code></pre><p class="comments-section">升级到1.2<div class="comments-icon"></div></p>
<pre><code>python2.7
(envtensorflow)gsc@X250:~/envtensorflow/lib$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.0-cp27-none-linux_x86_64.whl

python3(全局)
gsc@X250:~/envtensorflow/lib$ sudo pip3 install --upgrade  https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.0-cp34-cp34m-linux_x86_64.whl
[sudo] password for gsc: 
Downloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.0-cp34-cp34m-linux_x86_64.whl
  Downloading tensorflow-1.2.0-cp34-cp34m-linux_x86_64.whl (34.5MB): 34.5MB downloaded
</code></pre><p class="comments-section">将出现如下提示符合：<div class="comments-icon"></div></p>
<p class="comments-section">(tensorflow)$ <div class="comments-icon"></div></p>
<p class="comments-section">4.安装tensorflow<div class="comments-icon"></div></p>
<p class="comments-section"> (tensorflow)$ pip install --upgrade tensorflow      # for Python 2.7
 (tensorflow)$ pip3 install --upgrade tensorflow     # for Python 3.n<div class="comments-icon"></div></p>
<h2 id="kws唤醒词识别">KWS(唤醒词识别)</h2>
<h3 id="训练模型">训练模型:</h3>
<p class="comments-section">代码可以在github上下载:
<a href="">kws code base</a><div class="comments-icon"></div></p>
<p class="comments-section">谷歌官网是个不错的起点
<a href="https://www.tensorflow.org/tutorials/audio_recognition" target="_blank">英文原版</a><div class="comments-icon"></div></p>
<pre><code>python tensorflow/examples/speech_commands/train.py ----wanted_words=house
</code></pre><p class="comments-section">下载完成之后,运行会有如下log:<div class="comments-icon"></div></p>
<pre><code>I0730 16:53:44.766740   55030 train.py:176] Training from step: 1
I0730 16:53:47.289078   55030 train.py:217] Step #1: rate 0.001000, accuracy 7.0%, cross entropy 2.611571
</code></pre><p class="comments-section">freeze pb文件:<div class="comments-icon"></div></p>
<pre><code>python tensorflow/examples/speech_commands/freeze.py --start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 --out_file=/tmp/my_frozen_graph.pb ----wanted_words=house
</code></pre><p class="comments-section">After pb file generated, move to assets directory of android, android directory is an android studio project. The file used by my project is conv_actions_house_labels.txt and my_house_frozen_graph.pb.<div class="comments-icon"></div></p>
<p class="comments-section">如果需要了解tensorflow RNN训练细节见
<a href="http://blog.csdn.net/shichaog/article/details/72910410" target="_blank">tensorflow RNN实例</a><div class="comments-icon"></div></p>
<h3 id="android-apk安装">Android apk安装</h3>
<p class="comments-section">上述code base已经有apk安装文件.
adb install 命令可以进行安装.<div class="comments-icon"></div></p>
<h3 id="android-tensorflow-api">Android tensorflow API</h3>
<pre><code class="lang-java"><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> String LABEL_FILENAME = <span class="hljs-string">"file:///android_asset/conv_actions_house_labels.txt"</span>;
  <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> String MODEL_FILENAME = <span class="hljs-string">"file:///android_asset/my_house_frozen_graph.pb"</span>;
  <span class="hljs-keyword">private</span> TensorFlowInferenceInterface inferenceInterface;

    <span class="hljs-comment">// Load the TensorFlow model.</span>
    inferenceInterface = <span class="hljs-keyword">new</span> TensorFlowInferenceInterface(getAssets(), MODEL_FILENAME);

      <span class="hljs-comment">// Run the model.</span>
      inferenceInterface.feed(SAMPLE_RATE_NAME, sampleRateList);
      inferenceInterface.feed(INPUT_DATA_NAME, floatInputBuffer, RECORDING_LENGTH, <span class="hljs-number">1</span>);
      inferenceInterface.run(outputScoresNames);
      inferenceInterface.fetch(OUTPUT_SCORES_NAME, outputScores);
</code></pre>
<h3 id="模型创建和加载">模型创建和加载</h3>
<pre><code>public TensorFlowInferenceInterface(AssetManager var1, String var2) {
        this.prepareNativeRuntime();
        this.modelName = var2;
        this.g = new Graph();
        this.sess = new Session(this.g);
        this.runner = this.sess.runner();
        boolean var3 = var2.startsWith("file:///android_asset/");
        Object var4 = null;

        try {
            String var5 = var3?var2.split("file:///android_asset/")[1]:var2;
            var4 = var1.open(var5);
        } catch (IOException var9) {
            if(var3) {
                throw new RuntimeException("Failed to load model from \'" + var2 + "\'", var9);
            }

            try {
                var4 = new FileInputStream(var2);
            } catch (IOException var8) {
                throw new RuntimeException("Failed to load model from \'" + var2 + "\'", var9);
            }
        }

        try {
            this.loadGraph((InputStream)var4, this.g);
            ((InputStream)var4).close();
            Log.i("TensorFlowInferenceInterface", "Successfully loaded model from \'" + var2 + "\'");
        } catch (IOException var7) {
            throw new RuntimeException("Failed to load model from \'" + var2 + "\'", var7);
        }
    }
</code></pre><h3 id="c语言接口api">C语言接口API</h3>
<pre><code> #include &lt;stdio.h&gt;                                                                        
#include &lt;stdlib.h&gt;                                                                       
#include &lt;tensorflow/c/c_api.h&gt;                                                           

TF_Buffer* read_file(const char* file);                                                   

void free_buffer(void* data, size_t length) {                                             
        free(data);                                                                       
}                                                                                         

int main() {                                                                              
  // Graph definition from unzipped https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip
  // which is used in the Go, Java and Android examples                                   
  TF_Buffer* graph_def = read_file("tensorflow_inception_graph.pb");                      
  TF_Graph* graph = TF_NewGraph();

  // Import graph_def into graph                                                          
  TF_Status* status = TF_NewStatus();                                                     
  TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();                         
  TF_GraphImportGraphDef(graph, graph_def, opts, status);
  TF_DeleteImportGraphDefOptions(opts);
  if (TF_GetCode(status) != TF_OK) {
          fprintf(stderr, "ERROR: Unable to import graph %s", TF_Message(status));        
          return 1;
  }       
  fprintf(stdout, "Successfully imported graph");                                         
  TF_DeleteStatus(status);
  TF_DeleteBuffer(graph_def);                                                             

  // Use the graph                                                                        
  TF_DeleteGraph(graph);                                                                  
  return 0;
} 

TF_Buffer* read_file(const char* file) {                                                  
  FILE *f = fopen(file, "rb");
  fseek(f, 0, SEEK_END);
  long fsize = ftell(f);                                                                  
  fseek(f, 0, SEEK_SET);  //same as rewind(f);                                            

  void* data = malloc(fsize);                                                             
  fread(data, fsize, 1, f);
  fclose(f);

  TF_Buffer* buf = TF_NewBuffer();                                                        
  buf-&gt;data = data;
  buf-&gt;length = fsize;                                                                    
  buf-&gt;data_deallocator = free_buffer;                                                    
  return buf;
}
</code></pre><h2 id="tensorflow-模型文件">tensorflow 模型文件</h2>
<p class="comments-section">tensorflow生成的模型文件主要有三个：<div class="comments-icon"></div></p>
<pre><code>.meta, .index和.data
</code></pre><p class="comments-section">分成三个文件的原因是tensorflow将计算图结构和变量值存储在不同的文件里。<code>.meta</code>文件描述的是计算图结构。
freeze_graph.py脚本从一个GraphDef(<code>.pb或者.pbtxt</code>)文件和checkpoint(<code>.meta, .index, .data</code>)文件生成“frozen”【所谓的frozen就是将训练中的变量，转换成模型部署需要的常量过程，这个常量的保存形式是google的protocol buffers格式】的模型。<code>protocol buffer</code>在memory和运算速度上比XML有优势。
frozen的过程大致如下：
1.建立和训练模型tf.Graph，记为g_1;
2.使用<code>Session.run()</code>接口，以numpy数组的方式获取checkpoint的变量值；
3.在新的<code>tf.Graph</code>中，使用<code>tf.constant()</code>为该新的graph创建常量值，常量值的来源是第二部创建的值。
4.使用<code>tf.import_graph_def()</code>将g_1的节点拷贝到g_2中，使用input_map参数将g_1中的参数存放到<code>tf.constant()</code>张量中。
5.使用<code>g_2.as_graph_def()</code>获得图的<code>protocol buffer</code>的表示方法。<div class="comments-icon"></div></p>

<h2 id="唤醒词">唤醒词</h2>
<p class="comments-section">唤醒词主要用在设备端，以解决安全，网络带宽占用以及不必要的服务器访问，有几个方面是唤醒词需要考虑的。<div class="comments-icon"></div></p>
<ul>
<li>低功耗，手表，IoT等场景</li>
<li>低成本，memory/cpu</li>
<li>使用场景：1.外界环境； 2.远场， 3.训练语料<h2 id="sensory方法">sensory方法</h2>
目前未查到公开的算法架构，其在树莓派上公开的一个数组如下：
(16KHz, 16bit, ARMv7 with NEON)</li>
</ul>
<p class="comments-section"><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/sensory_kws.png" alt="">
在2018年5月其又推出不需要硬件加速的唤醒技术。<div class="comments-icon"></div></p>
<h2 id="apple">Apple</h2>
<p class="comments-section">其在2017年10月发了篇文章讲述其在Applewatch和iPhone上的实现，2018年苹果也出了音响，是远场的，唤醒以及音效都挺好，文章讲述的是hey，Siri的识别技术，那时的技术基于DNN方法，将时域语音转到特征域，然后对特征进行模型的前向计算以获得概率分布，最终确定唤醒的概率。<div class="comments-icon"></div></p>
<p class="comments-section">采样率也是采用16KHz，对语音进行分段做STFT，段长约10ms。大约200ms时长的语音被送入声学模型中，DNN计算后语音特征被映射到预先分类好的声学特征类中，大约有20个类别。DNN模型主要包括的就是矩阵乘法（对于无浮点或者功耗考虑是可以进行量化的）和非线性运算（也有的模型还需要采用normalization），最后一层的非线性常常采用softmax进行分类（mcu级别的也可以对其进行简化）。<div class="comments-icon"></div></p>
<p class="comments-section">Hey Siri的识别模型如下：
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/apple_kws.png" alt=""><div class="comments-icon"></div></p>
<p class="comments-section">其hiddenlayer的层数通常采用五层，神经元的size取相同的，根据不同的硬件配置选择32,128或者192.该模型最后映射的是声学模型计算的发音概率，这里的发音不是音素级别的，也不是指单词级别的，而是介于音素和单词之间的多个音素的结合体（是音节或者比音节更大一些），在汉语中基本上一个汉子就是一个音节，音节有声母和韵母组成，汉语无音调音节有400多个，带语调的有1300多个。为了便于识别，Apple选的词siri（['si:ri]）的前后元音都在里面。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/apple_kws_2.png" alt="">
个人觉得 这个选择非常务实，非常为用户考虑。<div class="comments-icon"></div></p>
<p class="comments-section">而且选择了两个谐振较为明显的前后元音。那么基于音节粒度的识别会存在一个问题，200ms没法把完整的”HeySiri”的特征全部囊括进去，但是这里softmax的输出却是指明有没有唤醒，这通过增加RNN层（黄色的两层）来实现的，这样就保留的序列的时间关联性。RNN层的实现如下：<div class="comments-icon"></div></p>
<p class="comments-section">$$F<em>{i,t} = max{s_i} + F</em>{i,t-1} + F<em>{i-1,t-1} + q</em>{i,t} \tag{16.101}$$
其中：$$F<em>{i,t}$$是声学模型累计得分值，$$q</em>{i,t}$$是声学模型输出（音节的对数得分），$$s_i$$是处在状态i的代价，$$m_i$$是从状态$$i$$转移的代价。$$s_i$$和$$m_i$$是根据训练集中数据分段时长确定的（和HMM的状态转移类似）。
公式的实现如上图。手机上的完整实现如下：
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/apple_3.png" alt=""><div class="comments-icon"></div></p>
<h2 id="google">google</h2>
<p class="comments-section">唤醒词“okgoogle”，据说是根据《SMALL-FOOTPRINTKEYWORD SPOTTING USING DEEP NEURALNETWORKS》这篇文章，不过google出了好几篇文章，并且为了推广他们的tensorflow，自带了kws的例子，其基于词级别的识别。识别架构如下：
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/google_kws_1.png" alt="">
为了减少功耗，这里多了一个VAD模块，将13维的PLP特征及其一阶和二阶delta值输入训练过的有30个分量（正交基）的对角协方差GMM模型，该模型的输出就是是否是语音，然后之后会外接一个滑动处理的状态机。<div class="comments-icon"></div></p>
<p class="comments-section">当VAD模块检测到是语音的时候，对25ms的语音数据计算40维的log-fbank，滑动窗长选择了10ms。<div class="comments-icon"></div></p>
<p class="comments-section">其对后验概率的处理以及平滑策略有些差异。<div class="comments-icon"></div></p>
<h2 id="百度">百度</h2>
<p class="comments-section">基于CRNN架构，<div class="comments-icon"></div></p>
<h2 id="其它网络模型">其它网络模型</h2>
<p class="comments-section">由于CNN能够处理更细节的时频信息；所以在很多场景中都会遇到使用CNN的场景，但是CNN有个比较大的缺点就是计算量大，所以会使用depth-wise之类的变种以降低运算量，在实际使用中也有将多个模型混合在一起使用，比如CNN+RNN,DNN+RNN,RNN有LSTM和GRU结构。除了上面的模型还有基于CTC的kws识别网络。相关的paper都有。我绝对对于入门者而言，不应该问“到底那种网络结构好”这类的问题。<div class="comments-icon"></div></p>
<h2 id="nn方法的个人理解">NN方法的个人理解</h2>
<p class="comments-section">在上述网络结构图中，出现的最多的就是圆圈，这些圆圈基本就是矩阵乘加以及非线性运算。这些圈的背后意义我的理解是：正交基。<div class="comments-icon"></div></p>
<p class="comments-section">太阳光照射的物理反射和自身颜色一样的光而吸收（不会完全吸收）其它颜色的光，如果要问一个辣椒（假设只有红色，黄色和绿色）是什么颜色的，那么就可以用x,y,z三个正交基来分别表示红色，黄色和绿色，我们可以采集辣椒反射的太阳光谱，分别求反射光谱属于x,y,z三个坐标轴上的值，可以知道对于红色辣椒，其得到的x分量肯定大于y，z方向上的分量。对于其它颜色的辣椒类似。则一个辣椒的颜色就可以使用具有三个正交基解决。这是可以把网络的输入看成是光谱的特征，圈圈表示的网络就是在提取其在三个正交基x，y，z上的分量，最后的非线性输入就是惩罚和奖励各个正交基上的分量。<div class="comments-icon"></div></p>
<p class="comments-section">对于七色的物理颜色识别问题，则可以使用具有七个正交基来解决。对于使用音节做为识别单位的ASR系统可以使用1300（中文约有1300个带声调的音节）个正交基来识别。所以所有的问题都可以划分为找到正确的正交基数量，并且让其正交。<div class="comments-icon"></div></p>
<p class="comments-section">回到网络模型中的圈圈，这些圈圈之间可以是正交的，层于层之间的圈圈也可以正交的，所以可以知道对于一个每层具有64个圈圈（神经元），共三层的这种层级的结构可达的正交基的数量是：<div class="comments-icon"></div></p>
<p class="comments-section">64<em>64×64(1层和2层)这么多个正交基，在网络模型训练时会采用非线性隔开层与层之间的强联系，所以正交基的数量约有64</em>64*2这么多，这么一个小小的模型就有这么多正交基已经非常不错了。<div class="comments-icon"></div></p>
<p class="comments-section">现在的正交基获取方式是使用Adam，SGD随机梯度方法+训练数据的方法，由于数据并不能涵盖显现实生活中的所有场景（所有正交基），也有方法弥补SGD方法的不足，如dropout，normalization等，但是这使得得到的正交基大大折扣。<div class="comments-icon"></div></p>
<p class="comments-section">针对语音场景弥补正交基不足现在采用的方法主要是前端语音增强，外加追求覆盖使用场景的语料，以及模型改进，语音增强主要就是抑制噪声，混响以及自身音源的影响，追求覆盖使用场景的语料一方面是录一方面是针对使用场景的人为加噪声。如何在模型上改进也是值得尝试的，如早期使用DNN方法，后来使用CNN+DNN就是人为减小了获取正交基的难度，模型的改进牵涉到特征的选取，时频特征现在是基本都会采用的特征，那么时域到频域的窗长该如何选择，窗长滑动该如何选取，加噪声的影响，使用MFCC还是log-fbank。<div class="comments-icon"></div></p>

<h1 id="语音识别概述">语音识别概述</h1>
<p class="comments-section">语音识别问题就是模式分类问题。<br>一个基本的语音识别系统如下图，实线是识别过程，虚线是模型训练部分（声学模型，字典和语言模型）。<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170924184552589.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图16.1语音识别系统组成<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">语音识别是把语音声波转换成文字。人耳的听觉范围约50Hz~20KHz,人发声的频率范围约85Hz~8KHz,当前语音识别系统绝大多数采样16kHz采样率,16bits位宽进行识别.这就是一个宽带(可以含的信息量大)信号处理.原始语音数据量还是很大的,<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170530113715961.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图16.2 语音时域和频域模型图<div class="comments-icon"></div></p>
</blockquote>
<h3 id="识别器特征选取">识别器特征选取</h3>
<p class="comments-section">一个字持续了200ms~300ms(3200~4800点),这个数据量还是比较大的,我们期望减小数据处理量能够小一些,这就需要从上面(3200~4800)数据点提取出能正确识别的语音信息,这就要求,降维后的信息能够正确表示语音,且背景噪声以及说话语速等影响化.<br>为了实现降维通常会采用STFT(短时傅里叶变换),之所以短时是因为对原始语音进行了加窗,因为语音可以看成是短时(10~30ms)平稳信号,当前大多数语音识别系统都是按25ms加窗(400采样点)做512点(512维)FFT。用傅里叶变换将声波变换成频谱和幅度。<br>但是FFT之后的数据维度依然很高,通常这是会log-mel特征(40+维,根据人耳对频点的灵敏性)做为神经网络的输入特征(维度从512将到了40+维).log-mel特征有些文献称为logfbank(log filter bank, 从图16.3可以看到很多l滤波器组).<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/Screenshot from 2017-12-16 14:37:46.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图16.3 mel特征频率选择.<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">但是logfbank对于传统的高斯混合统计模型而言,维度还是多了,在做<a href="http://blog.csdn.net/shichaog/article/details/78415473" target="_blank">EM</a>时不宜调节,所以又对og fbank做了梅尔导谱系数(MFCC, Mel Frequency Cepstral Coefficients),后进行DCT变换得到(13+维)特征,并不直接取DCT变换后的个频谱分量，而是采用和“梅尔”缩放一样的粒度对DCT后的频谱取三角窗平均；通常也会采取预加重技术抵消加窗带来的影响。对信号加噪。高斯混合模型通常使用13维的MFCC系数(也有使用PLP, 相对频谱变换-感知线性预测， perceptual linear prediction做为特征的),也有DNN也使用MFCC做为特征输入(但是现在很少了).为了不丢失动态性,通常还会对MFCC做一阶和二阶微分获得总共39为的特征(有可能也会加上pitch,能量等)做为最终特征.<div class="comments-icon"></div></p>
<p class="comments-section">总结来说:<br>对于ASR，常用的特征是FBANK，MFCCs以及PLP特征。<div class="comments-icon"></div></p>
<ul>
<li>特征应该包括足够的信息以区分音素（ 好的时间分辨率10ms，好的频率分辨率20~40ms)</li>
<li>独立于基频$$F_0$$和其谐波</li>
<li>对不同的说话人要有鲁棒性</li>
<li>对噪声和通道失真要有鲁棒性</li>
<li>具有好的模型匹配特征（特征维度尽量低，对于GMM还要求特征之间独立，对于NN方法则无此要求）
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170924191724893.png" alt=""><blockquote>
<p class="comments-section">图16.4语音特征提取<div class="comments-icon"></div></p>
</blockquote>
</li>
</ul>
<p class="comments-section">预加重模块增加了高频语音信号的幅度，预加重公式如下：  <div class="comments-icon"></div></p>
<p class="comments-section">$$x'[t_d]=x[t_d]-\alpha x[t_d-1], 0.95&lt; \alpha &lt; 0.99 \tag {16.1}$$<br>预加重可见第八章.<div class="comments-icon"></div></p>
<p class="comments-section">语音信号是非稳态信号，但是信号处理的算法通常认为信号是稳态的，通常加窗以获得短时平稳信号：  <div class="comments-icon"></div></p>
<p class="comments-section">$$x[n]=w[n]s[n] $$即$$x_t[n]=w[n]x'[t_d+n] \tag{16.2}$$<br>为了减小截断带来的影响，通常使用hanning或者hamming窗  <div class="comments-icon"></div></p>
<p class="comments-section">$$w[n]=(1-\alpha)-\alpha cos(\frac{2\pi n}{L-1}) \tag{16.3}$$<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170924192926984.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图16.5 STFT<div class="comments-icon"></div></p>
</blockquote>
<h3 id="音素">音素</h3>
<p class="comments-section">有了特征之后就是如何使用这些特征,对于"中"这个字,可将其区分/ZH/ /ON/ /G/这三个发音单元,这样这样如果前一节的特征能够按顺序匹配上这三个音素,那么可以认为说话人极有可能说了这个单词,在音素层级的识别时,会将上述的音素映射成HMM状态(映射成0,1,...的数字序列),使用有限状态转换器去做匹配的动作.这在第十三章kaldi入门是可以看到这一过程的.当然后面也有基于三音素,字符层级和单词级别(罕见词难处理)甚至是句子级别的识别,这取决于数据集和任务以及可用资源.<div class="comments-icon"></div></p>
<ul>
<li>上下文依赖的音素聚类</li>
</ul>
<p class="comments-section">当使用上下文依赖的关系时,可以提高识别率,但是带来音素组合比较多,如果有42个音素,采用三状态的组合,那么每次要计算$$3 \times 42^3$$概率的计算量太大,实际中采用聚类的方式,将相近和相似的组合合并成一个组合,以减小决策树的规模.<div class="comments-icon"></div></p>
<h3 id="语音识别概率模型">语音识别概率模型</h3>
<p class="comments-section">将音素以及音素序列用离散的类来模拟。语音识别的目标是预测正确的类序列。如果$z$表示从声波提取的特征向量序列，那么语音识别系统可以根据最优分类方程来工作：  <div class="comments-icon"></div></p>
<p class="comments-section">$$\hat w =\operatorname*{argmax}\limits_{w \in W}P(w|z) \tag{16.4}$$<br>实际上$$\hat w$$使用贝叶斯准则来计算该值。  <div class="comments-icon"></div></p>
<p class="comments-section">$$\hat w =\operatorname*{argmax}\limits_{w \in W}\frac{P(Z|w)P(w)}{P(Z)} \tag{16.5}$$<br>其中$$P(Z|w)$$是声学似然（声学打分），代表了词$$w$$被说了的情况下，语音序列$$Z$$出现的概率。$$p(w)$$是语音打分，是语音序列出现的先验概率，其计算依赖于语言模型，在忽略语音序列出现概率的情况下，上式可以简化为：  <div class="comments-icon"></div></p>
<p class="comments-section">$$\hat w =\operatorname*{argmax}\limits_{w \in W}{P(Z|w)P(w)} \tag{16.6}$$<br>这样语音识别可以分为两个主要步骤，特征提取和解码。<br>ASR主要包括四个部分：信号处理和特征提取，声学模型（AM，acoustic model），语言模型（LM，language model）和解码搜索（hypothesis search）。<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170528090407011.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图16.6连续语音识别<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">总结来说语音识别过程首先是将麦克风采集到的数据进行特征提取,然后根据声学模型和发音字典使用决策树搜索输入特征序列对应的字或词,最后根据语言模型(如"中心"和"忠心"在不同语义中使用)来确定输入特征对应的字或者词.<div class="comments-icon"></div></p>
<h2 id="声学模型">声学模型</h2>
<p class="comments-section">声学模型使用GMM-HMM（混合高斯-隐马尔科夫模型，Gaussian mixture model-HMM），训练该模型的准则有早期的最大似然准则（ML，maximum likelihood），中期的序列判别训练法（sequence hierarchical model），以及目前广泛使用的给予deep learning的特征学习法：深度神经元网络（Deep Neural Network DNN）。<div class="comments-icon"></div></p>
<h3 id="gmm模型">GMM模型</h3>
<p class="comments-section">用在说话人识别，语音特性降噪以及语音识别方面。<br>若随机变量$$X$$服从均值为$$\mu$$,，方差为$$\sigma$$的概率分布，则其概率密度函数是：  <div class="comments-icon"></div></p>
<p class="comments-section">$$f(x)=\frac{1}{\sqrt{2 \pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \tag{16.7}$$<br>则称$$x$$服从高斯分布（正态分布）。记作：$$X\sim N(\mu,\sigma^2)$$<br>正态随机向量$$\mathbf{X}=(x_1,x_2,...,x_D)^T$$的高斯分布是：  <div class="comments-icon"></div></p>
<p class="comments-section">$$f(\mathbf{x})=\frac{1}{\sqrt[D]{2\pi}\sqrt{|{\sum}|}}exp{[-\frac{1}{2}\frac{(\mathbf{x}-\mathbf{\mu})^T]}{\sum(\mathbf{x-\mu})}} \tag{16.8}$$<br>记作：$$X\sim N(\mu \in R^D,\sum \in R^{D×D})$$，其中$$\sum$$是$$D×D$$维协方差矩阵，$$|\sum |$$是$$\sum$$的行列式，$$\sum=E{(X-\mu)(X-\mu)}$$。<br>一个连续标量$X$的混合高斯分布的概率密度函数：  <div class="comments-icon"></div></p>
<p class="comments-section">$$f(X)=\sum<em>{m=1}^M\frac{c_m}{\sqrt{2\pi\sigma_m}}e^{-\frac{1}{2}(\frac{x-\mu_m}{\sigma_m})}=\sum</em>{m=1}^Mc<em>mN(x;\mu_m,\sigma_m^2),(-\infty &lt; x &lt; +\infty; \sigma_m &gt; 0; c_m &gt; 0) \tag{16.9}$$<br>混合权重的累加和等于一，即$$\sum</em>{m=1}^Mc<em>m=1.$$和单高斯分布相比，上式是一个具有多个峰值分布（混合高斯分布），体现在Ｍ&gt;1。混合高斯分布随机变量$x$的期望是$$E(x)=\sum</em>{m=1}^Mc_m\mu_m$$<br>多元混合高斯分布的联合概率密度函数是：  <div class="comments-icon"></div></p>
<p class="comments-section">$$f(\mathbf{x})=\sum \limits<em>{m=1}^M\frac{c_m}{\sqrt[D]{2\pi}{\sqrt{|\sum_m|}}}e^{-\frac{1}{2}(\mathbf{x-\mu_m})^T\sum_m^{-1}(\mathbf{x-\mu})}=\sum</em>{m=1}^Mc_mN(x;\mu_m,\sum_m),(c_m&gt;0) \tag{16.10}$$<div class="comments-icon"></div></p>
<h3 id="参数估计">参数估计</h3>
<p class="comments-section">对于多元混合高斯分布，参数变量$\Theta={c_m,\mu_m,\sum_m}$,这里参数估计的目标是选择合适的参数以使混合高斯模型符合建立的语音模型.<br>使用最大似然估计法估计混合高斯分布的参数：  <div class="comments-icon"></div></p>
<p class="comments-section">$$c_m^{(j+1)}=\frac{1}{N}\sum_{t=1}^Nh_m^{(j)}(t)$$  <div class="comments-icon"></div></p>
<p class="comments-section">$$\mu_m^{(j+1)}=\frac{\sum<em>{t=1}^Nh_m^{(j)}(t)\mathbf{X}^{(t)}}{\sum</em>{t=1}^Nh_m^{j}(t)} \tag{16.9}$$  <div class="comments-icon"></div></p>
<p class="comments-section">$$\sum_m^{(j+1)}=\frac{\sum<em>{t=1}^Nh_m^{(j)}[\mathbf{x}^t-\mu_m^j][\mathbf{x}^t-\mu_m^j]^T}{\sum</em>{t=1}^Nh_m^{(j)}(t)} \tag{16.10}$$<br>后验概率$h$的计算如下：  <div class="comments-icon"></div></p>
<p class="comments-section">$$h_m^j(t)=\frac{c_m^{(j)}N\mathbf(X^t;\mu_m^{j},\sum_m^j)}{\sum_{i=1}^nc_i^jN(\mathbf{x^t;\mu_i^j,\sum_i^j})}\tag{16.11}$$<br>基于当前（第ｊ次）的参数估计，$x^t$的条件概率取决于每一个采样。<br>ＧＭＭ模型适合用来对语音特征建模，而现实世界中组成的字的音节所包含的语音特征是有顺序概念在里面的，这时使用ＨＭＭ来表示其次序特征。<br>GMM模型不能有效的对呈非线性或者近似线性的数据进行建模。<div class="comments-icon"></div></p>
<h2 id="隐马尔科夫模型hmmhidden-markov-model">隐马尔科夫模型HMM(hidden markov model)</h2>
<p class="comments-section">HMM,的核心就是状态的概念，状态本身是离散的随机变量，用于描述随机过程。<div class="comments-icon"></div></p>
<h3 id="马尔科夫链">马尔科夫链</h3>
<p class="comments-section">设马尔科夫链的状态空间是$$q_t\in {s^{(j)},j=1,2,...,N}$$，一个马尔科夫链$$\mathbf{q}_1^T=q_1,q_2,...,q_T$$,可被转移概率完全表示，定义如下：  <div class="comments-icon"></div></p>
<p class="comments-section">$$p(q<em>t=s^{(j)}|q</em>{t-1}=s(i))\doteq p_{ij}(t),i,j=1,2,...,N$$<br>如果转移概率和时间无关，则得到齐次马尔科夫链，其矩阵表示方式如下：  <div class="comments-icon"></div></p>
<p class="comments-section">$$A=\begin{bmatrix}
    p<em>{11}&amp;p</em>{12}&amp;p<em>{13}&amp;...\
    p</em>{21}&amp;p<em>{22}&amp;p</em>{23}&amp;...\
    p<em>{31}&amp;p</em>{32}&amp;p<em>{33}&amp;...
    \end{bmatrix},\sum p</em>{ij}=1 \tag{16.12}$$<br>其观察概率分布$P(o<em>tt|s^{(i)}),i=1,2,...,N$，观察向量$o_t$是离散的，每个状态对应的概率分布用来描述观察${v_1,v_2,...,v_N}$的概率：  
</em>$$b<em>i(k)=P(o_t=v_k|q_t=i),i=1,2,...,N$$</em><br>在语音识别中，使用HMM的概率密度函数来描述观察向量$o_t \in R^D$的概率分布，其概率密度函数在语音识别中选择GMM的概率密度函数：  <div class="comments-icon"></div></p>
<p class="comments-section">$$b_i(o_t)=\sum<em>{m=1}^M\frac{c_im}{(2\pi)^{D/2}|\sum_i|^{1/2}}exp[-\frac{1}{2}(o_t-\mu</em>{i,m})^T\sum_{i,m}^{-1}(o_t-\mu_i,m)] \tag{16.13} $$<br>隐马尔科夫模型是统计模型，其被用来描述一个含有隐含位置参数的马尔科夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来进一步的分析。例如模式识别。<div class="comments-icon"></div></p>
<h3 id="隐马尔科夫模型">隐马尔科夫模型</h3>
<p class="comments-section">其是序列的概率模型，在每一个时刻都有一个状态与之对应。计算$p(sequence|model)$包括以对指数状态序列求和。可以使用动态规划递归求解，模型参数训练的目标是最大化训练数据集的概率。<br>其涉及两个重要的算法<div class="comments-icon"></div></p>
<ul>
<li>前向后向算法
递归计算状态概率，在模型训练时使用。</li>
<li>维特比算法
对于给定的字符序列，查找到最有可能的HMM状态序列。
早期基于HMM的语言模型使用向量量化（Vector Quantization）将语音特征映射到一个符号（通常有256个符号），每一个发音由三个马尔科夫状态表示，也就是三音素模型。</li>
</ul>
<h3 id="hmm参数学习－baum-welch法">HMM参数学习－Baum-Welch法</h3>
<p class="comments-section">定义“完整的数据”为$$\mathbf{y}=\mathbf{{o,h}}$$,其中是$$o$$观测值(如语音特征)。$$h$$是隐藏随机变量（如非观测的HMM状态序列），这里要解决的是对未知模型参数$$\theta$$的估计，这通过最大化对数似然度$$logp(o|\theta)$$可以求得，然而直接求解不易。可转换为如下公式求$$\theta$$的估计：  <div class="comments-icon"></div></p>
<p class="comments-section">$$Q(\theta|\theta<em>0)=E</em>{h|o}[\log p(\mathbf{y};\theta)|\mathbf{o};\theta_0]=E[\log p(\mathbf{o},\mathbf{h};\theta)|\mathbf{o};\theta_0] \tag{16.14}$$<div class="comments-icon"></div></p>
<p class="comments-section">其中$$\theta_0$$是前一次的估计。则上式离散情况下的期望值如下：  <div class="comments-icon"></div></p>
<p class="comments-section">$$Q(\theta|theta_0)=\sum_hp(\mathbf{h}|\mathbf{o})\log p(\mathbf{y}:\theta) \tag{16.15}$$<br>为了计算的方便，将数据集改为$$\mathbf{y}=[\mathbf{o}_1^T,\mathbf{q}_1^T]$$,$$\mathbf{o}$$依然是观测序列，$$\mathbf{h}$$是观测状态序列，$$mathbf{q}$$是马尔科夫链状态序列，BaTum-Welch算法中需要在Ｅ步骤中计算得到如下的条件期望值，或成为辅助函数$$Q(\theta|\theta_0)$$:  <div class="comments-icon"></div></p>
<p class="comments-section">$$Q(\theta|\theta_0)=E[\log p(\mathbf{o_1^T,q_1^T|\theta})\mathbf{o_1^T,\theta_0}]\tag{16.16}$$<br>这里期望通过隐藏状态序列$$\mathbf{q_1^T}$$确定得到。<div class="comments-icon"></div></p>
<h3 id="维特比算法">维特比算法</h3>
<p class="comments-section">在给定观察序列$$\mathbf{o}<em>1^T=\mathbf{o}_1,\mathbf{o}_2,...,\mathbf{o}_T $$的情况下，如何高效的找到最优的HMM状态序列。动态规划算法用于解决这类$$\mathbf{T}$$阶路劲最优化的问题被称为维特比（Viterbi）算法。对于转移状态$$a</em>{ij}$$给定的HMM，设状态输出概率分布为$$b_i(\mathbf{o_t})$$，令$$\delta_i(t)$$表示部分观察序列$$\mathbf{o}_1^t$$到达时间$$t$$，同时相应的HMM状态序列在该时间处在状态$$i$$时的联合似然度的最大值：  <div class="comments-icon"></div></p>
<p class="comments-section">$$\delta<em>i(t)=max</em>{q<em>1,q_2,...,q</em>{t-1}}P(\mathbf{o}_1^t,q_1^{t-1},q_t=i) \tag{16.17}$$<br>对于最终阶段$$t=T$$，有最优函数$$\delta_i^T$$，这通过计算所有$$t\le{T-1}$$的阶段来得到。当前处理$$t+1$$阶段的局部最优似然度，可以使用下面的函数等式来进行递归得到：  <div class="comments-icon"></div></p>
<p class="comments-section">$$\delta<em>j^{t=1}=max_i\delta_i(t)a</em>{ij}b<em>j(\mathbf{o}</em>{t+1}) \tag{16.18}$$<br>在语音建模和相关语音识别应用中一个最有趣且特别的问题就是声学特征序列的长度可变性。<div class="comments-icon"></div></p>
<h3 id="hmm识别器">HMM识别器</h3>
<p class="comments-section">单词序列$$\mathbf{W}(w_1,w_2,...,w_k)$$被分解为基音序列。在已知单词序列$$\mathbf{W}$$下观察到特征序列$$\mathbf{Y}$$的概率$$p(\mathbf{Y}|\mathbf{W})$$按如下公式计算：  <div class="comments-icon"></div></p>
<p class="comments-section">$$P(Y|W)=\sum_QP(y|Q)P(Q|W) \tag{16.19}$$  <div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf{Q}$$是单词发音序列$$Q_1,...,Q_k$$，每一个序列有事基音的序列$$Q_k=q_1^{(k)},q_2^({k)}...,$$,则有：  <div class="comments-icon"></div></p>
<p class="comments-section">$$P(Q|W)=\prod_{k=1}^KP(Q_k|w_k) \tag{16.20}$$<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170529091334757.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图16.8基于HMM的音素模型<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">如上图所示，基音$$q$$由隐马尔科夫密度表示，状态转移参数是$${a_{ij}}$$,观察分布是$${b_j()}$$，其通常是混合高斯分布：  <div class="comments-icon"></div></p>
<p class="comments-section">$$b<em>j(\mathbf{y})=\sum</em>{m=1}^Mc<em>{jm}N(\mathbf{y};\mu</em>{jm},\sum<em>{jm}) \tag{16.21}$$<br>其中$$N$$是均值为$$\mu</em>{jm}$$,方差为$$\sum_{jm}$$,约10到20维的联合高斯分布。由于声学向量$$\mathbf{y}$$维度较高，协方差矩阵通常限制为对角阵。状态进入和退出是非发散。$$\mathbf{Q}$$是基音序列的线性组合，声学似然如下：  <div class="comments-icon"></div></p>
<p class="comments-section">$$p(\mathbf{Y}|\mathbf{Q})=\sum_Xp(\mathbf{X,Y|Q}) \tag{16.22}$$<br>其中$$\mathbf{X}=x(0),...,x(T)$$是混合模型的状态序列。  <div class="comments-icon"></div></p>
<p class="comments-section">$$p(\mathbf{X,Y|Q})=a<em>{x(0),x(1)}\prod</em>{t=1}^Tb<em>x(t)(y_t)a</em>{x(t),x(t+1)} \tag{16.23}$$<br>声学模型参数$${a_{ij}}$$和$${b_j()}$$可以使用期望最大化的方式从语料库中训练得到。<br>由于发音通常是上下文相关的，如food和cool，通常使用三音子模型，以实现上下文相关法。如果有N个基音。那么将有$$N^3$$个可能的三音子。可以使用映射集群的方式缩减规模。<br>逻辑到物理模型集群通常是对状态层次的集聚而非模型层级的集群，每个状态所属的集群通过决策树确定。每个音素$$q$$的状态位置有一个二进制决策树与之相关。每一个音素模型有三个状态，树的每个节点都是语义的判断。将由$$q$$得到的逻辑模型音素$$q$$的状态$$i$$的集群。以最大化训练数据集的最终状态集概率为准则设置各个节点的判断条件。<div class="comments-icon"></div></p>
<h2 id="语言模型">语言模型</h2>
<p class="comments-section">语言模型计算单词序列的概率$$p(w_1,w_2,...,w_3)$$,传统语言模型当前词的概率依赖前n个单词，这通常由马尔科夫过程描述。  <div class="comments-icon"></div></p>
<p class="comments-section">$$p(w<em>1,...,w_m)=\prod</em>{i=1}^{m}p(w<em>i|w_1,...,w</em>{i-1})\approx \prod<em>{i=1}^{m}p(w_i|w</em>{i-(n-1)},...,w_{i-1}) \tag{16.24}$$<div class="comments-icon"></div></p>
<h3 id="n-gram语言模型">N-gram语言模型</h3>
<p class="comments-section">一个单词序列$$W=w_1,...,w_k$$的概率由以下公式表示：  <div class="comments-icon"></div></p>
<p class="comments-section">$$p(W)=\prod<em>{k=1}^Kp(w_k|w</em>{k-1},...,w_1) \tag{16.25}$$<br>对于大词汇量的识别问题，第$N$个单词的概率只依赖于前$N-1$个。  <div class="comments-icon"></div></p>
<p class="comments-section">$$p(W)=\prod<em>{k=1}^Kp(w_k|w</em>{k-1},w<em>{k-2},...,w</em>{k-N+1}) 
\tag{16.26}$$<br>通常N取2~4。通过计算训练数据集中N-gram出现的次数来形成最大似然概率。例如：  <div class="comments-icon"></div></p>
<p class="comments-section">$$C(w<em>{k-2}w</em>{k-1}w<em>k)$$是$$w</em>{k-2}w<em>{k-1}w_k$$三个词出现的次数，$$C(w</em>{k-2}w<em>{k-1})$$是$$w</em>{k-2}w_{k-1}$$出现的概率，则：  <div class="comments-icon"></div></p>
<p class="comments-section">$$p(w<em>k|w</em>{k-1}w<em>{k-2})\approx \frac{C(w</em>{k-2}w<em>{k-1}w_k)}{C(w</em>{k-2}w_{k-1})} \tag{16.27}$$<br>这种统计方式存在一个数据稀疏性问题。这通过结合非技术概率模型解决。  <div class="comments-icon"></div></p>
<p class="comments-section">$$p(w<em>k|w</em>{k-1},w<em>{k-2})=\frac{C(w</em>{k-2}{w<em>{k-1}w_k})}{C(w</em>{k-2}w{k-1})} \tag{16.28}$$<br>一元和二元语法模型的概率基于训练文集中单词出现的次数来统计。  <div class="comments-icon"></div></p>
<p class="comments-section">$$p(w_2|w_1)=\frac{count(w_1,w_2)}{count(w_1)} \tag{16.29}$$  <div class="comments-icon"></div></p>
<p class="comments-section">$$p(w<em>3|w_1,w_2)=\frac{count(w_1,w_2,w_3)}{count(w_1,w_2)}，如果c&gt;c';
=d\frac{count(w_1,w_2,w_3)}{count(w_1,w_2)}，如果0&lt;C&lt;C';
=\alpha(w</em>{k-1,w<em>{k-2}})p(w_k|w</em>{k-1})，其它 \tag{16.30}$$<br>其中$$C$$是计数门限，$$d$$是不连续系数，$$\alpha$$是归一化常数。<br>如果语音模型完全符合HMM模型（基于对角协方差多元高斯混合分布概率模型）假设的统计特性病切训练数据是充足的，那么就最小方差和零偏场景，最大似然准则解是最优解。可以从两个方面弥补非理想性，一个是参数估计策略，一个是模型。也有很多方法从这两个方面提升性能。<div class="comments-icon"></div></p>
<h3 id="归一化">归一化</h3>
<p class="comments-section">归一化的目的是减小环境和说话人物理特性差异的影响。由于前端特征源于对数频谱，特征值均值归一化见笑了通道的差异影响。倒谱方差归一化缩放每一个特征系数以获得单位方差，这减小了加性噪声的影响。<br>声道长度变化将导致共振峰频率近似线性变换，所以在前端特征提取时考虑线性缩放滤波器中心频率以获得近乎一致的共振峰频率，这一过程被称为VTLN(vocal-track-length normalization)。VTLN需要解决缩放函数定义和针对每个说话人的缩放函数参数估计。缩放函数可以采用分段线性函数（针对男声和女声所含信息不同）。<br>另外，如果训练语音数据集不能完全覆盖测试集中的说话人和说话场景，则语音识别将会产生错误，这类问题可以通过自适应的方法进行求解。<div class="comments-icon"></div></p>
<h2 id="加权有限状态转换机的语音识别">加权有限状态转换机的语音识别</h2>
<p class="comments-section">这是传统的语音识别方法，包括HMM模型，文本相关模型，发音字典，统计语法，单词和音素格。<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20170530081728605.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">传统ASR流程<div class="comments-icon"></div></p>
</blockquote>
<h3 id="加权有限自动机">加权有限自动机</h3>
<p class="comments-section">有限自动机定义为一个五元组：  <div class="comments-icon"></div></p>
<p class="comments-section">$$A=(Q,\sum,E,q_0,F)$$<br>其中$$Q$$是状态集合，$$\sum$$是输入符号集合，$$E$$为转移（边）集，其接收一个状态和输入符号，输出一个目的状态或者空。$$q_0 \in Q$$是初始状态，$$F \subset Q$$是最终状态集或者接受状态集。<div class="comments-icon"></div></p>
<h3 id="权值的半环理论">权值的半环理论</h3>
<p class="comments-section">语音识别时，不仅仅想要知道某个字串是否能够被接受，还要知道字串在语音中出现的概率。一个半环为一个五元组.<div class="comments-icon"></div></p>
<p class="comments-section">语音识别由早期的声学模型(单音素,三音素)+HMM+NGRAM组成识别系统,现在声学模型可以使用深度学习模型替代了,早期的声学模型基于DNN模型,后来由于语音其实是有上下文依赖关系存在的,深度学习单元又有LSTM出现,GRU实现类似的功能,但是计算量更少,训练过程中发现有些在一个长句子中,有些音素在根据发音字典解码成字占的比重比较大,又推出了Attention机制,不过呢,音素级别的训练需要大量的标注数据,所以呢,最早到2006年,就有文章提出来到字符和单词级别的识别,这就是端到端识别把从原始语音数据提取的特征编码成识别网络需要的编码格式,然后对这个编码进行前向计算,计算完了之后,计算完了之后在进行解码,解码的输出就是需要的字符或者单词.到2014年端到端识别又进行了改进,如加入了Attention机制.
ctc(connectionist temporal classification),不需要对齐
2014,2015两年,百度发表了deep speech,和deep speech2也是基于端到端的识别.<div class="comments-icon"></div></p>
<h2 id="深度学习模型">深度学习模型</h2>
<ul>
<li>RNN (Robinson and Fallside, 1991)</li>
<li>LSTM (Graves el al.2013)</li>
<li>Deep LSTM-P Sak et al.(2014)</li>
<li>CLDNN (Sainath et al. 2015a)</li>
<li>GRU DeepSpeech (Amodei at el. 2015)</li>
<li><p class="comments-section">BLSTM(i-vector normalization) 2016<div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">CTC<br>sequence discriminative training<br>sequence2sequence<br>Watch Listen,<div class="comments-icon"></div></p>
<p class="comments-section">Raw waveform speech recognition<br>CLDNN<br>WaveNet-sytle<div class="comments-icon"></div></p>
<p class="comments-section">CNN最早于98年提出来用于图像识别上,在语音识别上,CNN方法 可以利用时域和频域两个维度的相关性和信号时移不变性,通常CNN的层数不超过十几层,其后再加RNN和一层全连接层
~2014年
深度学习成功用在提升ASR识别率上,RNN<div class="comments-icon"></div></p>
</li>
</ul>
<p class="comments-section">~2015年<div class="comments-icon"></div></p>
<p class="comments-section">~2017(井喷)<div class="comments-icon"></div></p>
<ul>
<li><p class="comments-section">基于多通道的深度学习处理方法<div class="comments-icon"></div></p>
<p class="comments-section">(个人觉得未来三年内会被广泛采用,也就是NN芯片出来两代的时间)<a href="https://pdfs.semanticscholar.org/b9fc/cd8bee6e6998b87b4efc671dbcee45917282.pdf?_ga=2.162545140.93942331.1493904208-1691509212.1493904208" target="_blank">paper1</a>,<a href="ieeexplore.ieee.org/stamp/stamp.jsp">paper2</a><div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">混合型网络<div class="comments-icon"></div></p>
<p class="comments-section">如ResNet和LSTM结合<a href="https://arxiv.org/pdf/1701.03360.pdf" target="_blank">pdf</a>,ResNet和卷积CTC结合<a href="https://arxiv.org/pdf/1702.07793.pdf" target="_blank">pdf</a><div class="comments-icon"></div></p>
</li>
<li><p>端到端</p>
<p class="comments-section">声学模型不仅仅是音素级别的了,可以是直接转换成字符或者单词.对于ASR,端到端模型常常采用CTC做为代价函数.腾讯的RCNN-CTC模型可以在WSJ和腾讯测试集WER比大部分现行的模型要低<a href="https://arxiv.org/pdf/1702.07793.pdf" target="_blank">pdf</a>,<div class="comments-icon"></div></p>
</li>
<li><p class="comments-section">减少训练时间<div class="comments-icon"></div></p>
<p class="comments-section">当前在训练参数时,采用的方法基本都是随机梯度法,要减小训练时间,总体上思路有两类,一类是充分发掘硬件的资源优势,一类是优化算法,减小计算量或者提高并发性,二者也不是完全割裂的;<div class="comments-icon"></div></p>
<p class="comments-section">对于充分发掘硬件资源,受限于板卡和主机之间通信延迟和batch size大小,基于cache特性减小训练时间也是一个方法,减小数据访问的开销,嵌入式端的本地识别和信号增强也有这个思路;<a href="https://www.microsoft.com/en-us/research/publication/cache-based-recurrent-neural-network-language-model-inference-for-first-pass-speech-recognition/" target="_blank">Cache Based Recurrent Neural Network Language Model Inference for First Pass Speech Recognition 2014</a><div class="comments-icon"></div></p>
<p class="comments-section">对于算法层次,最基本的层次是网络模型的变更,如RNN到LSTM以及到GRU等.<div class="comments-icon"></div></p>
<p class="comments-section">硬件和算法资源相结合,一个是更另一个是更改loss函数,loss函数更改可以带来batch size的增加,这样可以充分利用服务器资源的同时也不会或者带来的损失很小.<a href="https://arxiv.org/pdf/1708.03888.pdf" target="_blank">LARGE BATCH TRAINING OF CONVOLUTIONAL NETWORKS 2017</a><div class="comments-icon"></div></p>
<p class="comments-section">*训练和使用场景相结合<div class="comments-icon"></div></p>
<ul>
<li>输入模型</li>
</ul>
<p class="comments-section">输入的语音信号通常采用能量归一化,提取谱特征,对数压缩,以及特征维度的均值和方差归一化,对数谱在频带之间可以获得很高的动态范围,这也意味这一些频带在压缩中信息会丢失掉,<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45911.pdf" target="_blank">PCEN 2016</a>层较上述方法更优.<div class="comments-icon"></div></p>
</li>
</ul>
<p class="comments-section">  <a href="https://arxiv.org/pdf/1705.04400.pdf" target="_blank">Reducing Bias in Production Speech Models 2017</a>.<div class="comments-icon"></div></p>
<h3 id="端到端语音识别">端到端语音识别</h3>
<h1 id="算法工程化">算法工程化</h1>
<p class="comments-section">算法工程实现是沟通算法和芯片的桥梁，算法评估通过MATLAB或者c/c++,工程化针对具体的硬件平台选择合适的处理器，嵌入式场景的特点是：<div class="comments-icon"></div></p>
<pre><code>- 其处理器性能受限（主频，缓存，带宽）
- 成本敏感
- 实时性要求高
- 功耗敏感
</code></pre><p class="comments-section">服务器端的场景特点是：<div class="comments-icon"></div></p>
<pre><code>- 机房发热量大
- 训练耗时长（多卡，多机）
- 花费较大
</code></pre><p class="comments-section">两种场景都要求优化算法，服务器端由于可以多机多卡，可以做并发处理，这是比嵌入式场景多出来的可以选择的方案。<div class="comments-icon"></div></p>
<p class="comments-section">说到算法优化，不可避免的会遇到编程代码，代码优化也需要考虑代码的可移植性，可维护性。不论是个人还是团队，建议使用google的代码风格，<a href="https://google.github.io/styleguide/cppguide.html" target="_blank">google的代码风格</a>代码即注释。<div class="comments-icon"></div></p>
<p class="comments-section">算法的优化从最底层的代码层级到算法层级。代码层级偏重于最大攫取硬件资源，算法层级侧重于在算法性能不变的前提下，算法层次上减少运算资源和memory的消耗，当然代码层级优化和算法层级优化也是相互关联的，可以互相促进的。<div class="comments-icon"></div></p>
<h2 id="芯片架构">芯片架构</h2>
<p class="comments-section">VLIW(very long instruction word,超长指令字),利用指令并行的指令架构，而传统的CPU通常只允许cpu顺序执行指令。VLIW处理器允许程序并发同时执行不同的指令。如可以在一个指令中同时完成单精度乘法，加法以及累加等操作。如DSP常常采用这种架构；
SIMD（single instruction multiple data）,单指令多数据，采用一个控制器控制多个核，对同一组中的数据每一个分别执行相同的操作从而实现空间上的并行性的技术。sse或者neon都是这种架构。<div class="comments-icon"></div></p>
<h3 id="处理器类别">处理器类别</h3>
<p class="comments-section">1.Analog Devices(SHARC, Blackfin, SigmaDSP)
2.TI(c55, c67x,c66x)
3.ARM
cortex-M4/M7; cortex-A8/A9/A15/A53/etc
4.Intel x86/x64
5.软IP
Tensilica，CEVA以及ARC<div class="comments-icon"></div></p>
<h3 id="系统设计考虑的问题">系统设计考虑的问题</h3>
<p class="comments-section">1.外设（接口，麦克，DMA...）
2.memory(片上RAM、FLASH)， 外部memory
3.功耗，尺寸<div class="comments-icon"></div></p>
<h2 id="cache">cache</h2>
<p class="comments-section">cache是非常非常重要的一个硬件，代码层级很多的优化就发生在cache上。
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/18_1.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图18.1 memory层级架构 <div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">图18.1中，从底向上容量越来越小，单位比特价格越来越高，对于手机类处理器通常只有三级cache，对于蓝牙耳机之类的dsp只有一级cache。cache的使用主要是局部性（空域和时域）原理。空域局部性原理指当前被存取的地址，其地址附近有接下来需要的数据。时间局部性原理指当前被存取的地址，很可能不久又要被存取。<div class="comments-icon"></div></p>
<pre><code>- 尽量编写cache友好性代码
- 一次存取一个cache line
- 减少程序不可预测分之跳转，减少流水被打断
- 对于数组
------
1 |  2 
3 |  4

在memory中会被存为1,2，3,4，所以按行存取数据有益于充分利用cache局部性特性

通常CPU的cache line一组为32或者64字节，通常有64组/128组，一次加载数据一个组的cache line。
</code></pre><p class="comments-section">当做64点FFT复数时，则128个复数加载次数：
如果cache line对齐，最少主存取四次就行，
如果cache line 不对其，数据连续，则要取8次;<div class="comments-icon"></div></p>
<p class="comments-section">如何cache line对其，alignment=要对其的字节数，bytes是要申请的空间大小<div class="comments-icon"></div></p>
<pre><code> struct AlignedPtr* AllocAlignedPointer(int alignment, int bytes) {
    struct AlignedPtr* aligned_ptr;
    unsigned long raw_address;
   aligned_ptr = (struct AlignedPtr*) malloc(sizeof(*aligned_ptr));
   aligned_ptr-&gt;raw_pointer_ = malloc(bytes + alignment);
    raw_address = (unsigned long) aligned_ptr-&gt;raw_pointer_;
    raw_address = (raw_address + alignment - 1) &amp; ~(alignment - 1);
   aligned_ptr-&gt;aligned_pointer_ = (void*) raw_address;
    return aligned_ptr;
  }
</code></pre><p class="comments-section">也可以使用编译器选项优化：<div class="comments-icon"></div></p>
<pre><code>Int myarray[16] __attribute__((aligned(64)));
</code></pre><h2 id="代码层级优化">代码层级优化</h2>
<ul>
<li><p class="comments-section">vector/list <div class="comments-icon"></div></p>
<p class="comments-section">vector顺序存储成员，list使用pointer指向前后两个元素。list对于频繁增删元素场景较合适，vector用于需要快速访问对象较为合适，<div class="comments-icon"></div></p>
</li>
</ul>
<p class="comments-section">List:<div class="comments-icon"></div></p>
<ul>
<li>Each element takes 2 integers to point previous and next elements, so most commonly, that's 8 bytes more for each elements in your list</li>
<li>Insert is linear in time:O(n)</li>
<li>Remove is a constant operation:O(1)</li>
<li>Access the x element is linear in time:O(n)</li>
</ul>
<p class="comments-section">Vectors:<div class="comments-icon"></div></p>
<ul>
<li>Needs less memory(no pointers to other elements, it's a simple math algorithm)</li>
<li>Remove is linear in time:O(n)</li>
<li>Access the x elements is constant:O(1)</li>
<li>Insert can be in linear time, but most commonly, it's a constant operation:O(1)</li>
</ul>
<ul>
<li>将循环用于函数内部，减少函数调用次数</li>
<li>使用初始化(Color c(black))而非赋值（color c; c=black;）的方式初始化对象</li>
<li>尽可能使用switch 代替if...else if...else if...</li>
<li>If possible, use shift operations(&lt;&lt;,&gt;&gt;) of integer multiplication and division.</li>
<li>对于类对象操作优先使用 +=, -=, *=, and /=.</li>
<li>对于基本的数据类型操作优先使用 +, -, *, and /.</li>
<li>对于类对象，使用前缀操作符号(++obj)而不是后置操作符号（obj++）</li>
<li>对于后置操作符，对象会被临时拷贝一份</li>
<li>++<ul>
<li>对于类对象，使用前缀操作符号(++obj)而不是后置操作符号（obj++）</li>
<li>对于后置操作符，对象会被临时拷贝一份</li>
<li>x= array[i++] is more efficient than x =array[++i]；
Two clock cycles needed for calculating address first</li>
<li>a = ++b is more efficient than a = b++；
Compiler will recognize that the values of a and b are the same for the fisrt situation.</li>
</ul>
</li>
<li>避免无效数据初始化，如果需要使用memset()函数</li>
<li>Array optimize<pre><code>for(int i=0; i&lt;n; i++) nArray[i]=nSomeValue; 
A better one is:
for(int* ptrInt = nArray; ptrInt&lt; nArray+n; ptrInt++) *ptrInt=nSomeValue;
</code></pre></li>
<li>最好不要在Ｃ++中使用malloc。如果分配识别，malloc返回NULL, new将抛出std::bad_alloc。稍后使用NULL指针的行为是未定义的。</li>
<li><p class="comments-section">&amp;(reference) vs *(pointer)<div class="comments-icon"></div></p>
<p class="comments-section">对于算法中使用的数据结构推荐使用指针*，对于函数参数和返回值推荐使用引用 &amp;<div class="comments-icon"></div></p>
</li>
<li>Branch prediction(instruction pipeline)</li>
</ul>
<p class="comments-section">likely/unlikely
 likely，是通知编译器if (true)被执行的概率比较高；
unlikely，是通知编译器if(false)被执行的概率比较高；<div class="comments-icon"></div></p>
<pre><code>        if (unlikely(value)){ 
                 //do thing1
           }else{
                // do thing2
           }
</code></pre><h3 id="inline的使用">inline的使用</h3>
<p class="comments-section">对于小于十行的代码可以考虑使用inline化，inline带来性能提升的前提是代码里循环要少，尽量没有，这样可以避免函数调用带来的入栈出栈开销，这在ARM上是常见的，在有些架构（DSP）上，有些有stack window 寄存器以避免栈操作开销；对于较大的函数体如果使用inline的化，可能导致整个代码的size比较大，进而反而导致性能上反而有所下降。<div class="comments-icon"></div></p>
<h3 id="restrict关键词">restrict关键词</h3>
<p class="comments-section"><strong>restrict</strong>关键词可以帮助编译器对代码进行优化，restrict 关键词可以让编译器推导出buffer的overlap与否而进行代码优化<div class="comments-icon"></div></p>
<h3 id="write-back-vs-write-through">write-back vs write-through</h3>
<p class="comments-section">write-through always write to cache and system memory
write-back write to cache. Only when cache lines is replaced, write to system memory.
在多核共享cache时，类似DSP等这种架构时需要注意write-back方式可能带来cache不一致的问题。<div class="comments-icon"></div></p>
<h2 id="neon">NEON</h2>
<p class="comments-section">neon汇编优化力度大，灵活性不好v7，v8要重写一遍，intrinsic通用性好，灵活性比汇编差。
对我ARM Neon汇编帮助巨大(项目中实际用到了)的博文是:
<a href="https://community.arm.com/processors/b/blog/posts/coding-for-neon---part-1-load-and-stores" target="_blank">Coding for NEON - Part 1: Load and Stores</a>
<a href="https://community.arm.com/processors/b/blog/posts/coding-for-neon---part-2-dealing-with-leftovers" target="_blank">Coding for NEON - Part 2: Dealing With Leftovers</a>
<a href="https://community.arm.com/processors/b/blog/posts/coding-for-neon---part-3-matrix-multiplication" target="_blank">Coding for NEON - Part 3: Matrix Multiplication</a>
<a href="https://community.arm.com/processors/b/blog/posts/coding-for-neon---part-4-shifting-left-and-right" target="_blank">Coding for NEON - Part 4: Shifting Left and Right</a>
<a href="https://community.arm.com/processors/b/blog/posts/coding-for-neon---part-5-rearranging-vectors" target="_blank">Coding for NEON - Part 5: Rearranging Vectors</a><div class="comments-icon"></div></p>
<h2 id="多线程">多线程</h2>
<h2 id="开源算法库">开源算法库</h2>
<p class="comments-section">有很多的开源库
<a href="https://github.com/ARM-software/ComputeLibrary" target="_blank">Compute library</a> 源于MIT，支持了信号处理和图像一些算法，对于GPU部分，使用中遇到了安德鲁GPU上API兼容性问题，NEON部分没有遇到。
<a href="http://www.fftw.org/" target="_blank">fftw</a> fft库
<a href="https://www.khronos.org/openmaxdl" target="_blank">openmax</a>
<a href="https://github.com/projectNe10/Ne10" target="_blank">Ne10</a><div class="comments-icon"></div></p>
<h2 id="代码分析">代码分析</h2>
<ul>
<li>Disassemble<ul>
<li>Compile source with -g</li>
<li>Use objdump -d -S</li>
</ul>
</li>
<li>Dump ELF data<ul>
<li>Readelf/objdump</li>
</ul>
</li>
<li>strings<ul>
<li>Display printable strings in file</li>
</ul>
</li>
<li>Nm<ul>
<li>List sybols from objects/binaries</li>
</ul>
</li>
<li>Size<ul>
<li>Display size of sections in binary/objects</li>
</ul>
</li>
<li>Addr2line<ul>
<li>Convert address into linenumber:filename</li>
</ul>
</li>
</ul>
<h2 id="code-profiling--tools">code profiling -tools</h2>
<ul>
<li>Simpleperf</li>
<li>Dumpsys meminfo</li>
<li>ps -ef f</li>
<li>Valgrind(memory leak)</li>
<li>mpstat</li>
<li>free</li>
<li>oprofile(multithread)</li>
<li>Top/perf top</li>
<li>gprof -pg</li>
<li>iostat/iperf</li>
<li>strace</li>
<li>cat /sys/pidXX/status</li>
</ul>
<h2 id="处理器">处理器</h2>
<h2 id="cortexm处理器特点">cortexM处理器特点</h2>
<p class="comments-section"><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/cortex_m.png" alt="">
2美金以下<div class="comments-icon"></div></p>
<p class="comments-section">TCM(Tightly-Coupled Memory)是一或者多块和CPU非常近的memory，cpu对齐的访问是单周期的。TCM常用于存储性能关键的数据和代码，如中断处理函数，实时任务需要的数据等。在有些芯片上又可以配置成通用的memory，这主要通过寻址解码方式实现（将TCM映射到特定的地址）。对于没有用到的TCM的情况，可以将其用于cache，这可以提高性能。<div class="comments-icon"></div></p>
<h2 id="cortexm优化">cortexM优化</h2>
<p class="comments-section">c语言编程，loop展开，尽量寄存器复用以减少数据存取；M4和M7不同；尽量使用CMSIS库
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/cortex_m_2.png" alt=""><div class="comments-icon"></div></p>
<h2 id="cortexa优化">CortexA优化</h2>
<p class="comments-section">intrinsic c特性；loop展开；寄存器复用；使用arm compute library
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20180510143826511.png" alt=""><div class="comments-icon"></div></p>
<h2 id="fir-benchmark">FIR benchmark</h2>
<p class="comments-section">256点FIR数据表，数值越小，性能越好
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/20180510143926754.png" alt=""><div class="comments-icon"></div></p>
<h2 id="fft-benchmark">FFT benchmark</h2>
<p class="comments-section">复数，无位反转
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/2018051014413833.png" alt=""><div class="comments-icon"></div></p>
<h2 id="线性代数计算库">线性代数计算库</h2>
<p class="comments-section">写算法的实现，离不开各种矩阵以及线性代数的运算，包括矩阵求逆，矩阵分解，SVD以及特征值，特征向量等；<div class="comments-icon"></div></p>
<p class="comments-section">尤其是复高斯模型或者在频域里做处理时用到复数矩阵相关运算；APP公司官网的加速链接中就有BLAS相关的API。<div class="comments-icon"></div></p>
<p class="comments-section">这里gsl是开源的计算库，该库的最新文档链接如下：<a href="https://www.gnu.org/software/gsl/doc/html/index.html。" target="_blank">https://www.gnu.org/software/gsl/doc/html/index.html。</a><div class="comments-icon"></div></p>
<p class="comments-section">music方法使用特征空间法估计信号（自关矩阵）的成分。自相关矩阵的特征值对应的不同特征向量是正交的（现在的深度学习方法也是在找若干组正交基的过程）。特征值最大的对应于该信号在该特征向量上的投影值比较大，也就是包含的信息量要比其它特征向量空间中的大，这在数据压缩领域有应用。<div class="comments-icon"></div></p>
<p class="comments-section">使用gsl实现MUSIC算法的实现代码片段如下：<div class="comments-icon"></div></p>
<pre><code>
#include &lt;stdio.h&gt;
#include &lt;math.h&gt;
#include "m_pd.h"

#include &lt;gsl/gsl_eigen.h&gt;
#include &lt;gsl/gsl_matrix.h&gt;
#include &lt;gsl/gsl_vector.h&gt;
#include &lt;gsl/gsl_complex.h&gt;
#include &lt;gsl/gsl_complex_math.h&gt;
#include &lt;gsl/gsl_blas.h&gt;

#define NOOFITERATIONS 1000
/* music~ : music-code for MUSIC Algorithm
commands for compiling with gcc:
&gt; gcc -Wall -c music~.c
&gt; ld -export-dynamic -shared -o music~.pd_linux music.o -lc -lm -lgsl -lgslcblas
*/
/* ------------------------ music~ ----------------------------- */

static t_class *music_tilde_class;

typedef struct _music_tilde
{
  t_object x_obj;
  t_outlet *x_outlet_value;
  t_outlet *x_outlet_index;
  gsl_matrix_complex *x_cmin;        //input signals
  gsl_matrix_complex *x_cov;        //covariance matrix
  gsl_vector *x_eval;            //eigenvalues
  gsl_matrix_complex *x_evec;        //eigenvectors
  gsl_matrix_complex *x_a;        //array response
  gsl_matrix_complex *x_c;        //
  gsl_matrix_complex *x_c2;
  gsl_eigen_hermv_workspace *x_w;    //calculating eigenvalues and -vectors
  int      x_count;            //sample counter

  t_float  x_msi;            //dummy
} t_music_tilde;

//Function for calculating the array response, which is stored in x-&gt;x_a matrix
static void array_response(t_music_tilde *x, double angle){
  double f = 1000;
  double lambda = 343/f;
  double d = 0.04;
  int i, no = 8;

  for (i=0; i&lt;no; i++) {
    if (angle &lt; 0){
      gsl_matrix_complex_set(x-&gt;x_a, (no-i-1), 0, gsl_complex_polar(1.0, (-(double)i*2*M_PI*d*sin(angle)/lambda)));
    }
    else {
      gsl_matrix_complex_set(x-&gt;x_a, i, 0,  gsl_complex_polar(1.0, (-(double)i*2*M_PI*d*sin(angle)/lambda)));
    }
  }
}

static t_int *music_tilde_perform(t_int *w)
{
  t_music_tilde *x = (t_music_tilde *) w[1];
  t_float *inarray[] = {(t_float *)w[2], (t_float *)w[3], (t_float *)w[4], (t_float *)w[5], (t_float *)w[6], (t_float *)w[7], (t_float *)w[8], (t_float *)w[9], (t_float *)w[10], (t_float *)w[11], (t_float *)w[12], (t_float *)w[13], (t_float *)w[14], (t_float *)w[15], (t_float *)w[16], (t_float *)w[17]};
  int n = (int)(w[18]);
  int i, j, p, nosources;
  int m=0;    //actual sample from 0 to n-1
  double f1, f2, theta_rad;
  gsl_matrix_complex_view nvec;
  gsl_vector *mdl = gsl_vector_alloc(8);
  const gsl_complex alpha = gsl_complex_rect((1/NOOFITERATIONS), (1/NOOFITERATIONS));
  const gsl_complex beta = gsl_complex_rect(0, 0);

  while (n--) {
    if (x-&gt;x_count == NOOFITERATIONS) {    // calculate eigenvalues and reset after a certain no of iterations
//normalize the covariance matrix
      gsl_matrix_complex_scale(x-&gt;x_cov, alpha);
//gsl command for calculating eigenvalues
      gsl_eigen_hermv(x-&gt;x_cov, x-&gt;x_eval, x-&gt;x_evec, x-&gt;x_w);
//print out the result
   //   post("cov[1][1]:%f cov[1][2]:%f cov[2][1]:%f cov[2][2]:%f eval[1]:%f eval[2]:%f", gsl_matrix_get(x-&gt;x_cov, 0, 0), gsl_matrix_get(x-&gt;x_cov, 0, 1), gsl_matrix_get(x-&gt;x_cov, 1, 0), gsl_matrix_get(x-&gt;x_cov, 1, 1), gsl_vector_get(x-&gt;x_eval, 0), gsl_vector_get(x-&gt;x_eval, 1));
//MDL Algorithm for estimating no of sources
      gsl_vector_reverse(x-&gt;x_eval);
      p = (int)x-&gt;x_eval-&gt;size;
      for (i = 0; i &lt; p; i++){
        f1=1.0;
        f2=0.0;
        for (j = i; j &lt; p; j++){
          f1*=pow(gsl_vector_get(x-&gt;x_eval, j), (1/(double)(p-i)));
          f2+=gsl_vector_get(x-&gt;x_eval, j);
        }
        f2=f2/(p-i);
        gsl_vector_set(mdl, i, (-log(pow((f1/f2), ((p-i)*NOOFITERATIONS)))+0.5*i*(2*p-i)*NOOFITERATIONS));
      }
      nosources = gsl_vector_min_index(mdl);
//Building nvec matrix
      nvec = gsl_matrix_complex_submatrix(x-&gt;x_evec, 0, 0, 8, (p-nosources));
      x-&gt;x_c = gsl_matrix_complex_alloc(1, (p-nosources));
//estimating the pmu function
      j=0;
      for (i=-90; i&lt;=90; i++) {
        theta_rad=i*M_PI/180;
        array_response(x, theta_rad);
        gsl_blas_zgemm(CblasConjTrans, CblasNoTrans, alpha, x-&gt;x_a, &amp;nvec.matrix, beta, x-&gt;x_c);
        gsl_blas_zgemm(CblasNoTrans, CblasConjTrans, alpha, x-&gt;x_c, x-&gt;x_c, beta, x-&gt;x_c2);
        outlet_float(x-&gt;x_outlet_value, GSL_REAL(gsl_matrix_complex_get(x-&gt;x_c2, 1, 1)));
        outlet_float(x-&gt;x_outlet_index, j);
        j++;
      }
//reset the temporary variables
      gsl_matrix_complex_set_zero(x-&gt;x_cov);
      x-&gt;x_count = 0;
    }
// aranging 16 inputs to a 8x1 complex matrix
    for (i=0; i&lt;8; i++) {
      gsl_matrix_complex_set(x-&gt;x_cmin, i, 0, gsl_complex_rect((double)inarray[i*2][m], (double)inarray[i*2+1][m]));
    }
// estimating the covariance matrix from 8 complex input signals
    gsl_blas_zgemm(CblasNoTrans, CblasConjTrans, alpha, x-&gt;x_cmin, x-&gt;x_cmin, alpha, x-&gt;x_cov);
// increment counters
    m++;
    x-&gt;x_count++;
  }

  return (w+19);
}

static void music_tilde_dsp(t_music_tilde *x, t_signal **sp)
{
  dsp_add(music_tilde_perform, 18, x, sp[0]-&gt;s_vec, sp[1]-&gt;s_vec, sp[2]-&gt;s_vec, sp[3]-&gt;s_vec, sp[4]-&gt;s_vec, sp[5]-&gt;s_vec, sp[6]-&gt;s_vec, sp[7]-&gt;s_vec, sp[8]-&gt;s_vec, sp[9]-&gt;s_vec, sp[10]-&gt;s_vec, sp[11]-&gt;s_vec, sp[12]-&gt;s_vec, sp[13]-&gt;s_vec, sp[14]-&gt;s_vec, sp[15]-&gt;s_vec, sp[0]-&gt;s_n);
}



static void *music_tilde_new()
{
  t_music_tilde *x = (t_music_tilde *)pd_new(music_tilde_class);
  int k;
  for (k=2; k&lt;=16; k++) {
    inlet_new(&amp;x-&gt;x_obj, &amp;x-&gt;x_obj.ob_pd, &amp;s_signal, &amp;s_signal);
  }
  x-&gt;x_outlet_value = outlet_new(&amp;x-&gt;x_obj, &amp;s_float);
  x-&gt;x_outlet_index = outlet_new(&amp;x-&gt;x_obj, &amp;s_float);

//allocating memory
  x-&gt;x_cmin = gsl_matrix_complex_alloc(8, 1);
  x-&gt;x_cov = gsl_matrix_complex_calloc(8, 8);
  x-&gt;x_eval = gsl_vector_alloc(8);
  x-&gt;x_evec = gsl_matrix_complex_alloc(8, 8);
  x-&gt;x_a = gsl_matrix_complex_alloc(8, 1);
  x-&gt;x_c2 = gsl_matrix_complex_alloc(1, 1);
  x-&gt;x_w = gsl_eigen_hermv_alloc(8);
  x-&gt;x_count = 0;

  return (x);
}

static void music_tilde_free(t_music_tilde *x)    //destructor for deallocating memory
{
  gsl_matrix_complex_free(x-&gt;x_cmin);
  gsl_matrix_complex_free(x-&gt;x_cov);
  gsl_vector_free(x-&gt;x_eval);
  gsl_matrix_complex_free(x-&gt;x_evec);
  gsl_matrix_complex_free(x-&gt;x_a);
  gsl_matrix_complex_free(x-&gt;x_c);
  gsl_matrix_complex_free(x-&gt;x_c2);
  gsl_eigen_hermv_free(x-&gt;x_w);
}

void music_tilde_setup(void)
{
  music_tilde_class = class_new(gensym("music~"), (t_newmethod)music_tilde_new, (t_method)music_tilde_free,
                 sizeof(t_music_tilde), 0, A_DEFFLOAT, 0);
  CLASS_MAINSIGNALIN(music_tilde_class, t_music_tilde, x_msi);
  class_addmethod(music_tilde_class, nullfn, gensym("signal"), 0);
  class_addmethod(music_tilde_class, (t_method)music_tilde_dsp, gensym("dsp"), 0);

  //class_sethelpsymbol(music_tilde_class, gensym("music/music~"));
}
</code></pre><h2 id="makefile">makefile</h2>
<p class="comments-section">我喜欢使用Makefile来组织上算法编译层次上的层级关系。要引用第三方库，这里的例子实现了下载和编译第三方库的例子如下：<div class="comments-icon"></div></p>
<pre><code>
FFTW = fftw-3.3.4

all: .deps fftw decode ops utils

.deps:
    sudo apt-get install libsamplerate-dev -y
    pip install -r requirements.txt

.PHONY: decode ops utils

fftw:
    mkdir -p third_party &amp;&amp; cd third_party &amp;&amp; \
    wget http://www.fftw.org/$(FFTW).tar.gz &amp;&amp; \
    tar -xzf $(FFTW).tar.gz &amp;&amp; \
    rm $(FFTW).tar.gz &amp;&amp; \
    cd $(FFTW) &amp;&amp; \
    mkdir -p build &amp;&amp; \
    ./configure --prefix=`pwd`/build \
        --enable-float --enable-shared &amp;&amp; \
    make clean &amp;&amp; \
    make -j 4 &amp;&amp; \
    make install

decode:
    $(MAKE) -C decoder

ops:
    $(MAKE) -C user_ops

utils:
    $(MAKE) -C utils

clean:
    $(MAKE) -C utils clean
    $(MAKE) -C user_ops clean

    $(MAKE) -C decoder clean
</code></pre>

<h1 id="附录1-高斯分布和em算法">附录1 高斯分布和EM算法</h1>
<p class="comments-section">在做ASR识别的声学模型时,用到多维高斯模型和EM算法,在做CGMM的波束形成时也用到了高斯分布和EM算法.<div class="comments-icon"></div></p>
<h2 id="马氏距离">马氏距离</h2>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/Flow Chart Demo.png" alt=""></p>
<blockquote>
<p class="comments-section">图1 最近最近临域边界法<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">如图二给出的两个列表,$$m_1(x_1,y_2),m_2(x_2,y_2)$$,$$x_1,y_1$$是第一个类别两个维度的中心距的坐标,对于一个新给的变量X,判断其所属的类别可以使用最近临域边界法,即求x分别到两个类别中心的距离,里哪个类别距离小,就可以将其归为哪个类别.可以更进一步,将距离通过指数函数$$e^{-||x-m||}$$转换成概率,增加权重因子$$c$$可得$$e^{- \frac{||x-m||}{c}}$$.
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/Screenshot from 2018-01-12 09:49:05.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图2 权重因子对概率函数的影响<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">对于多维情况,指数部分求距离则变成$${(\mathbf x -\mathbf m)^T\mathbf \Sigma^{-1}(\mathbf x -\mathbf m)}$$.这个距离被称为马氏距离.<div class="comments-icon"></div></p>
<h3 id="协方差矩阵的种类">协方差矩阵的种类</h3>
<p class="comments-section">球形,对角和全协方差矩阵
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/covariance.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图3 协方差种类<div class="comments-icon"></div></p>
</blockquote>
<h2 id="高斯分布">高斯分布</h2>
<p class="comments-section">高斯分布(Gaussian distribution)又称为正太分布(Normal distribution).<div class="comments-icon"></div></p>
<h2 id="一维高斯分布">一维高斯分布</h2>
<p class="comments-section">若随机变量$$x$$服从均值为$$\mu$$,标准方差为$$\sigma^2$$的高斯分布,则记为:  <div class="comments-icon"></div></p>
<p class="comments-section">$$X  \sim N(\mu, \sigma^2) \tag 1.0 $$<br>其概率密度函数记为:  <div class="comments-icon"></div></p>
<p class="comments-section">$$f(x;\mu, \sigma)= \frac{1}{\sigma\sqrt{2\pi}}\exp(-\frac{(x-\mu)^2}{2\sigma^2})$$<br><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/325px-Normal_distribution_pdf.png" alt=""><div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">图4 一维高斯分布的几种情况<div class="comments-icon"></div></p>
</blockquote>
<h2 id="二维高斯分布">二维高斯分布</h2>
<p class="comments-section">二维随机变量的高斯分布如下:  <div class="comments-icon"></div></p>
<p class="comments-section">$$f(x,y)=\frac{1}{2\pi \sigma_x \sigma_y \sqrt{1-\rho^2}} \exp (- \frac{1}{2(1-\rho^2)}[\frac{(x-\mu_x)^2}{\sigma_x^2} + \frac{(y-\mu_y)^2}{\sigma_y^2}- \frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x \sigma_y}] ) \tag{2}$$<br>其中$$\rho$$是$$x$$和$$y$$之间的相关系数, 在$$\sigma_x &gt; 0$$且$$\sigma_y &gt; 0$$的情况下,可以令:<div class="comments-icon"></div></p>
<p class="comments-section">$$
\mathbf{ \mu} = \begin{bmatrix}
x\ 
y
\end{bmatrix}, \mathbf \Sigma =\begin{bmatrix}
\sigma_x^2   &amp;&amp; \rho \sigma_x \sigma_y\ 
\rho \sigma_x\sigma_y &amp;&amp; \sigma_y^2 
\end{bmatrix}<div class="comments-icon"></div></p>
<p>$$</p>
<p class="comments-section">则有:  <div class="comments-icon"></div></p>
<p class="comments-section">$$f_x(x, y) = \frac{1}{\sqrt{(2\pi)^2|\mathbf \Sigma|}}\exp(-\frac{1}{2}(
\begin{bmatrix}
x -  \mu_x   \ y - \mu_y
\end{bmatrix}^T
\mathbf \Sigma^{-1}\begin{bmatrix}
x -  \mu_x   \ y - \mu_y
\end{bmatrix})\tag {3}$$<div class="comments-icon"></div></p>
<p class="comments-section">通过求解$$| \mathbf \Sigma|$$行列式的值,和二维矩阵求逆$$\mathbf \Sigma^{-1}$$计算公式是:<div class="comments-icon"></div></p>
<p class="comments-section">$$
\begin{bmatrix}
a   &amp;&amp; b\ 
c &amp;&amp; d 
\end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix}
d   &amp;&amp; -b\ 
-c &amp;&amp; a 
\end{bmatrix}=\frac{1}{\sigma_x^2\sigma_y^2 - \rho^2 \sigma_x^2\sigma_y^2}\begin{bmatrix}
\sigma_y^2   &amp;&amp; -\rho\sigma_x \sigma_y\ 
-\rho \sigma_x^2\sigma_y^2 &amp;&amp; \sigma_x^2 
\end{bmatrix} \tag{4}<div class="comments-icon"></div></p>
<p>$$</p>
<blockquote>
<p class="comments-section">对角阵
假设上述变量$$x$$和$$y$$是不相关的,那么其相关系数$$\rho=0$$,则协方差矩阵变为:<div class="comments-icon"></div></p>
<p class="comments-section">$$\mathbf \Sigma=\begin{bmatrix}
\sigma_x^2   &amp;&amp; 0\ 
0 &amp;&amp; \sigma_y^2 
\end{bmatrix}<div class="comments-icon"></div></p>
<p>$$</p>
</blockquote>
<p class="comments-section">在有些场景中,为了减少运算量,假设建模的对象(如语言发音模型)分类是不相关的,则由协方差矩阵蜕化为对角阵,kaldi中就有对角高斯混合模型协方差矩阵类型可选.<div class="comments-icon"></div></p>
<h3 id="多维高斯分布">多维高斯分布</h3>
<p class="comments-section">对于语音识别中的声学模型,每段语音分帧提取MFCC特征多半是40维度,或者NN方法中的fbank也多半选择了40维.将上节的二维拓展到多维分布后可得如下概率密度函数:  <div class="comments-icon"></div></p>
<p class="comments-section">$$f_{\mathbf X}(x_1,\cdots, x_k) = \frac{1}{\sqrt{(2\pi)^k|\mathbf \Sigma|}}\exp(-\frac{1}{2}(\mathbf x -\mathbf \mu)^T\mathbf \Sigma^{-1}(\mathbf x -\mathbf \mu)) \tag{5}$$<div class="comments-icon"></div></p>
<p class="comments-section">此外还有复高斯模型,其每一维变量都包括实部和虚部.<div class="comments-icon"></div></p>
<h2 id="em算法">EM算法</h2>
<h3 id="一维高斯分布的em算法">一维高斯分布的EM算法</h3>
<p class="comments-section">EM算法的初始化:<div class="comments-icon"></div></p>
<ul>
<li>设置随机变量集$$\mathbf X = {x<em>1, \cdots, x_N}$$的初始均值估计$$\hat \mu_1, \cdots, \hat \mu_k$$,如K=3(类别), N=100,即100个随机变量分类到三个类别中去, 则可以设置$$\hat \mu_1 = x</em>{45}, \hat \mu<em>2 = x</em>{32}, \hat \mu<em>2 = x</em>{10}$$.</li>
<li>计算初始协方差参数,$$\hat \sigma<em>1^2, \cdots, \hat \sigma_k^2 = \frac{1}{N}\sum</em>{i=1}^N(x<em>1 - \overline x )^2, \overline x = \frac{1}{N}\sum</em>{i=1}^Nx_i$$</li>
<li>设置随机变量的初始分布为均匀分布:
$$\hat \phi_1, ..., \hat \phi_K = \frac{1}{K}$$.</li>
</ul>
<p>E步骤:</p>
<ul>
<li>对$$\forall i, k$$计算
$$\hat \gamma<em>{ik} = \frac{\hat \phi_kN(x_i|\hat \mu_k, \hat \sigma_k)}{\sum</em>{j=1}^K \hat \phi<em>j N(x_i|\hat \mu_j, \hat \sigma_j)} \tag {6}$$
此处$$\hat \gamma</em>{ik}$$是$$x<em>i$$属于类别$$C_K$$的概率,即$$\hat \gamma</em>{ik} = p(C_k|x_i, \hat \phi, \hat \mu, \hat \sigma)$$.</li>
</ul>
<p class="comments-section">M步骤:<br>使用E步骤计算的$$\hat \gamma_{ik}$$对$$\forall k$$计算:<div class="comments-icon"></div></p>
<ul>
<li>$$\hat \phi<em>k =\sum \limits</em>{i=1}^N \frac{\hat \gamma_{ik}}{N}$$</li>
<li>$$\hat \mu<em>k= \frac{\sum</em>{i=1}^N \hat \gamma<em>{ik} x_i}{\sum</em>{i=1}^N \hat \gamma_{ik}}$$</li>
<li>$$\hat \sigma<em>k = \frac{\sum</em>{i=1}^N \hat \gamma<em>{ik}(x_i - \hat \mu_k)^2}{\sum</em>{i=1}^N \hat \gamma_{ik}}$$</li>
</ul>
<p class="comments-section">收敛结束条件:<div class="comments-icon"></div></p>
<ul>
<li>一种是按找EM总数算,一种是按照M步骤连续两次的目标值差异度算.</li>
</ul>
<p class="comments-section">聚类操作,是指并不知道参数的类别总数时将相近的随机变量合并成同一个类.<div class="comments-icon"></div></p>
<p class="comments-section"><a href="https://github.com/shichaog/em4gmm" target="_blank">高斯混合模型c代码下载链接</a><div class="comments-icon"></div></p>

<h1 id="线程编程">线程编程</h1>
<p class="comments-section">本篇所述线程是指POSIX标准的线程,<div class="comments-icon"></div></p>
<blockquote>
<p class="comments-section">pthread库不是Linux系统默认的库,链接程序时需要使用静态库libpthread.a,编译时可以加上-lpthread参数.<div class="comments-icon"></div></p>
</blockquote>
<p class="comments-section">在算法工程化这一章节中就提到了并发程序设计思想,并发程序设计的核心思想是利用处理多核特性,让数据处理并发,并发可以使用多进程实现也可以用使用多线程方式实现,在同一个进程中的多个线程可以访问所属进程的资源,如文件描述符,内存等;在多个线程里共享资源时就需要关注资源的一致性(信号,锁,同步)问题.<div class="comments-icon"></div></p>
<p class="comments-section">考虑时下一个比较常见的应用场景,麦克风阵列在做AEC(自动回声消除时),每一路麦克风数据处理极其相似,一路一路处理这种串行处理方式是一种方式,另外还有一种方法是设置多个线程,将每一路的语音数据分配到一个核上,并发执行,这样在四核四路麦克的情况下,实际的处理时间就是一路处理时间,这样系统的实时性就会很好.<div class="comments-icon"></div></p>
<p class="comments-section">当然在单核的情况下更需要多线程设计,举个例子,一个进程执行了定期获取麦克风寻向结果,然后根据获取到的结果进行点灯刷新操作,如果是多线程设计,那么在等待获取寻向结果线程时,这个线程是可以进入休眠的,将cpu让出来给其它线程使用,尽量充分利用cpu资源.<div class="comments-icon"></div></p>
<h2 id="线程创建">线程创建</h2>
<pre><code class="lang-c">#include &lt;phtread.h&gt;
int pthread_create(pthread_t *restrict tidp,
                const pthread_attr_t *restrict attr,
                void *(*start_rtn)(void*), void *restrict arg);
</code></pre>
<p class="comments-section">正确返回0,否则返回错误码.tidp是线程ID指针,attr是线程属性,新创建的线程从start_rtn开始执行,arg是传递给该函数的参数.
当线程被创建时,先运行新创建的线程还是创建线程的线程.<div class="comments-icon"></div></p>
<h2 id="线程退出">线程退出</h2>
<p class="comments-section">处在进程中的任何一个线程调用<code>exit</code>,<code>_Exit</code>,<code>_exit</code>将导致整个进程结束.线程可以使用三种方法退出:<div class="comments-icon"></div></p>
<ul>
<li>从线程入口函数<code>`start_rtn</code>退出,返回值是线程退出值.</li>
<li>线程可以被其它线程结束.</li>
<li>调用<code>pthread_exit</code></li>
</ul>
<pre><code class="lang-c"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;pthread.h&gt;</span></span>

<span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">pthread_exit</span><span class="hljs-params">(<span class="hljs-keyword">void</span> *rval_ptr)</span>
</span></code></pre>
<p class="comments-section">参数<code>rval_ptr</code>可以被其它线程使用<code>pthread_join</code>函数访问.<div class="comments-icon"></div></p>
<pre><code class="lang-c"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;phtread.h&gt;</span></span>
<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_join</span><span class="hljs-params">(pthread_t thread, <span class="hljs-keyword">void</span> **rval_ptr)</span>
</span></code></pre>
<p class="comments-section">调用<code>pthread_join</code>函数的线程会被阻塞,直到其参数thread指定的线程调用<code>pthread_exit</code>,从<code>start_rtn</code>返回或被取消.<div class="comments-icon"></div></p>
<h2 id="线程同步">线程同步</h2>
<h3 id="互斥锁">互斥锁</h3>
<p class="comments-section">保护数据一个时刻只能有一个线程访问.互斥锁的类型是<code>pthread_mutex_t</code>,如果是动态申请内存存储互斥锁,则需要在是否内存前先调用<code>pthread_mutex_destory</code>销毁互斥锁.<div class="comments-icon"></div></p>
<pre><code class="lang-c"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;pthread.h&gt;</span></span>
<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_mutex_init</span><span class="hljs-params">(pthread_mutex_t *<span class="hljs-keyword">restrict</span> mutex, 
           thre <span class="hljs-keyword">const</span> pthread_mutexattr_t *<span class="hljs-keyword">restrict</span> attr)</span></span>;

<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_mutex_destory</span><span class="hljs-params">(pthread_mutex_t *mutex)</span></span>;
</code></pre>
<p class="comments-section">正确返回0,错误返回错误码.创建默认熟悉的互斥锁时将<code>attr</code>设置成<code>NULL</code>.<div class="comments-icon"></div></p>
<p class="comments-section">调用<code>pthread_mutex_lock</code>锁住一个互斥锁,如果互斥锁已经被锁住,则调用线程被阻塞直到互斥锁被解锁.调用<code>pthread_mutex_unlock</code>解锁.<div class="comments-icon"></div></p>
<pre><code class="lang-c"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;pthread.h&gt;</span></span>

<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_mutex_lock</span><span class="hljs-params">(pthread_mutex_t * mutex)</span></span>;
<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_mutex_trylock</span><span class="hljs-params">(pthread_mutex_t * mutex)</span></span>;
<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_mutex_unlock</span><span class="hljs-params">(pthread_mutex_t *mutex)</span></span>;
</code></pre>
<p class="comments-section">如果线程不能被阻塞,则可以调用<code>pthread_mutex_trylock</code>有条件的给互斥锁上锁.如果锁可以上,则该函数上锁并返回0,如果锁已经被锁住,则该函数返回<code>EBUSY</code>而不会阻塞线程.<div class="comments-icon"></div></p>
<h3 id="读写锁">读写锁</h3>
<p class="comments-section">读写锁和互斥锁类似,但是提供比互斥锁更高维度的并发性.读写锁有三个状态:读情况上锁,写情况上锁和未上锁状态.当读写锁处于读锁定状态,所有其它尝试以读的方式上锁操作将被允许,任何尝试以写方式获得该锁时需要所有获得读锁的线程释放读锁.为了防止写者饥饿,通常在一个线程已经获得读锁,另一个线程处于写锁申请时,若再有一个线程尝试获取读锁,这种情况获取读锁的线程会被阻塞.
读写锁适合读的频次大于写频次的数据的保护,和互斥锁类似,读写的操作如下:<div class="comments-icon"></div></p>
<pre><code class="lang-c"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;pthread.h&gt;</span></span>

<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_rwlock_init</span><span class="hljs-params">(pthread_rwlock_t *<span class="hljs-keyword">restrict</span> rwlock,
 <span class="hljs-keyword">const</span> pthread_rwlockattr_t *<span class="hljs-keyword">restrict</span> attr)</span></span>;

 <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_rwlock_destory</span><span class="hljs-params">(pthread_rwlock_t *rwlock)</span></span>;

 <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_rwlock_rdlock</span><span class="hljs-params">(pthread_rwlock_t *rwlock)</span></span>;
 <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_rwlock_wrlock</span><span class="hljs-params">(pthread_rwlock_t *rwlock)</span></span>;
 <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_rwlock_unlock</span><span class="hljs-params">(pthread_rwlock_t *rwlock)</span></span>;

 <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_rwlock_tryrdlock</span><span class="hljs-params">(pthread_rwlock_t *rwlock)</span></span>;
 <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_rwlock_trywrlock</span><span class="hljs-params">(pthread_rwlock_t *rwlock)</span></span>;
</code></pre>
<h3 id="条件变量">条件变量</h3>
<p class="comments-section">条件变量是线程同步的另一个方法.条件变量本身由互斥锁保护,线程必须先锁定互斥锁才能改变条件变量的状态,其它线程只有在获得互斥锁之后才会知道锁的状态改变,条件变量(<code>pthread_cond_t</code>)在使用前必须先初始化,有两种初始化方法,静态方法给条件变量赋值<code>PTHREAD_COND_INITIALIZER</code>,如果条件变量是动态分配的,使用<code>pthread_cond_init</code>函数进行初始化.在释放条件变量所占用的内存前,使用<code>pthread_cond_destory</code>销毁条件变量.<div class="comments-icon"></div></p>
<pre><code class="lang-c"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;pthread.h&gt;</span></span>

<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_cond_init</span><span class="hljs-params">(pthread_cond_t *<span class="hljs-keyword">restrict</span> cond,
               pthread_condattr_t *<span class="hljs-keyword">restrict</span> attr)</span></span>;
<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_cond_destory</span><span class="hljs-params">(pthread_cond_t * cond)</span></span>;
</code></pre>
<p class="comments-section">正确返回值是0,错误情况返回错误码.<code>attr</code>设置成<code>NULL</code>表示使用默认属性.<div class="comments-icon"></div></p>
<pre><code class="lang-c"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;pthread.h&gt;</span></span>
<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_cond_wait</span><span class="hljs-params">(pthread_cond_t *<span class="hljs-keyword">restrict</span> cond, 
         pthread_mutex_t *<span class="hljs-keyword">restrict</span> mutex)</span></span>;

<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">pthread_cond_timewait</span><span class="hljs-params">(pthread_cond_t *<span class="hljs-keyword">restrict</span> cnd, pthread_mutex_t *<span class="hljs-keyword">restrict</span> mutex,
                          <span class="hljs-keyword">const</span> <span class="hljs-keyword">struct</span> timespec *<span class="hljs-keyword">restrict</span> timeout)</span></span>;
</code></pre>
<p class="comments-section">传递给<code>pthread_cond_wait</code>函数的<code>mutex</code>用于保护互斥锁,
线程首先锁定``mutex<code>,这里的互斥锁用于保证</code>pthread_cond_wait<code>能够并发,然后将调用线程置于等待该条件变量的链表上并解锁(以让其它线程可以被添加到等待条件变量的链表上),当</code>pthread_cond_wait<code>返回时,互斥锁将被再次锁定.</code>pthread_cond_timewait<code>函数在</code>pthread_cond_wait```函数基础上添加了超时功能,timeout设定了线程等待时间.<div class="comments-icon"></div></p>
<p class="comments-section">使用条件变量和互斥锁的实例<div class="comments-icon"></div></p>
<pre><code class="lang-c"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;pthread.h&gt;</span></span>

<span class="hljs-keyword">struct</span> msg{
   <span class="hljs-keyword">struct</span> msg *m_next;
   <span class="hljs-comment">/*... more stuff here...*/</span>
   };

<span class="hljs-keyword">struct</span> msg *workq;
<span class="hljs-keyword">pthread_cond_t</span> qready = PTHREAD_COND_INITIALIZER;
<span class="hljs-keyword">pthread_mutex_t</span> qlock = PTHREAD_MUTEX_INITIALIZER;

<span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">process_msg</span><span class="hljs-params">(<span class="hljs-keyword">void</span>)</span>
</span>{
    <span class="hljs-keyword">struct</span> msg *mp;
    <span class="hljs-keyword">for</span>(;;){
        pthread_mutex_lock(&amp;qlock);
        <span class="hljs-keyword">while</span>(workq == <span class="hljs-literal">NULL</span>);
            pthread_cond_wait(&amp;qready, &amp;qlock);
        mp = workq;
        workq = mp-&gt;m_next;
        pthread_mutex_unlock(&amp;qlock);
        <span class="hljs-comment">/*now process the message mp*/</span>
    }
}

<span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">enqueue_msg</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> msg *mp)</span>
</span>{
    pthread_mutex_lock(&amp;qlock);
    mp-&gt;m_next = workq;
    workq = mp;
    pthread_mutex_unlock(&amp;qlock);
    pthread_cond_signal(&amp;qready);
}
</code></pre>

<h1 id="定点化">定点化</h1>
<p class="comments-section">由于成本，功耗等音素，一些DSP和MCU上并没有硬浮点支持，采用软浮点较为耗时，更有甚至不支持浮点计算。
在这些处理器上要实现相关算法，浮点的计算转换到定点域里计算必不可少。<div class="comments-icon"></div></p>
<h2 id="q格式">Q格式</h2>
<p class="comments-section">Q格式是表示浮点数据的一种格式，如Q15表示有15个小数位，而Q1.14则表示有1个整数位和14个小数位。<div class="comments-icon"></div></p>
<p class="comments-section">Q格式的数据是名义上的定点数，它们的存储和计算均按照整数方式进行计算，这样的化就可以使用标准的整数ALU来实现浮点计算。而编程这必须确定整数和小数部分的位数以符合使用场景需要的动态范围和精度。<div class="comments-icon"></div></p>
<h2 id="浮点转成q格式">浮点转成Q格式</h2>
<p class="comments-section">从浮点转成Qm.n格式如下：
1.将浮点数乘以$$2^n$$
2.四舍五入到整数<div class="comments-icon"></div></p>
<h2 id="q格式转成浮点">Q格式转成浮点</h2>
<ol>
<li>乘以$$2^{-n}$$</li>
</ol>
<h1 id="有源噪声控制">有源噪声控制</h1>
<h1 id="arm在深度学习上的进展">ARM在深度学习上的进展</h1>
<h1 id="开发工具合集">开发工具合集</h1>
<p class="comments-section">一些常用辅助工具<div class="comments-icon"></div></p>
<h2 id="audacity音频分析工具">audacity音频分析工具</h2>
<p class="comments-section">audacity，在Ubuntu的软件center里可以直接安装，常用功能是open wav，import pcm， 时频域，频域是STFT，非常适合语音识别的短时稳态场景；
<img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/audacity_spetrom.png" alt=""><div class="comments-icon"></div></p>
<h2 id="codeblocks代码调试工具">codeblocks代码调试工具</h2>
<p class="comments-section">center里有，安装就好，比较好用。用于单步调试，省去的编写makefile文件。<div class="comments-icon"></div></p>
<h3 id="codeblock添加命令行参数方">codeblock添加命令行参数方</h3>
<pre><code>Project &gt; Set programs' arguments...
</code></pre><h3 id="codeblock添加宏定义">codeblock添加宏定义</h3>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/codeblocks_userdefined macro.png" alt=""></p>
<h3 id="codeblock输出导入文件方法">codeblock输出导入文件方法</h3>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/codeblocks_export_to_file.png" alt=""></p>
<h2 id="pycharm-python开发ide">Pycharm python开发IDE</h2>
<p class="comments-section">python编写调试IDE，非常好用<div class="comments-icon"></div></p>
<h3 id="添加tensorflow运行环境">添加tensorflow运行环境</h3>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/pycharm_add_tensorflow_environment.png" alt=""></p>
<h3 id="添加脚本输入参数">添加脚本输入参数</h3>
<p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/6KQF2.gif" alt=""></p>
<h1 id="matlab">MATLAB</h1>
<p class="comments-section">基本上时频域，窗函数，滤波器，自适应滤波器等信号处理的都有，；<div class="comments-icon"></div></p>
<h3 id="matlab-保存成c头文件">matlab 保存成c头文件</h3>
<pre><code>fid = fopen\('wave\_data.txt', 'wt'\);  
fprintf\(fid, '%g,', Y\);  
fclose\(fid\);
</code></pre><h2 id="ffmpeg-wav转pcm">ffmpeg wav转pcm</h2>
<pre><code>
ffmpeg -i bftest3-00.wav -f s16le -ar 16000 -ac 1 -acodec pcm\_f32le bftest3-01.raw
</code></pre><pre><code>重采样
ffmpeg -i sp02_babble_sn01.wav -ar 16000 sp02_babble_sn01_16k.wav
</code></pre><h1 id="python">python</h1>
<h2 id="shell字串获取和求值">shell字串获取和求值</h2>
<p class="comments-section">获取第二列<div class="comments-icon"></div></p>
<pre><code>cat log.txt | awk '{print $2}'
</code></pre><p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/shell_取一列.png" alt=""></p>
<pre><code>#!/bin/bash
cat log.txt | while read line   #filename 为需要读取的文件名,也可以放在命令行参数里。

do
    awk '{print $2}'
done
</code></pre><p class="comments-section">求第二列的最大值<div class="comments-icon"></div></p>
<pre><code>cat log.txt | awk 'BEGIN {max = 0} {if ($2&gt;max) max=$2 fi} END {print "Max=", max}'
</code></pre><p><img src="https://shichaog1.gitbooks.io/hand-book-of-asr-processing/content/assets/shell求第二列最大值.png" alt=""></p>

	</div>
</div>
</div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div>
<footer>
<br />
<p><a href="https://applenob.github.io">DanteLiujie's blog</a> &copy; DanteLiujie 2018</p>
</footer>

</div> <!-- /container -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="https://applenob.github.io/theme/bootstrap-collapse.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-99008972-1', 'auto');
  ga('send', 'pageview');

</script>
 
</body>
</html>